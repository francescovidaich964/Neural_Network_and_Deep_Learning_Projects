{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 02 - Function Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "train_data = np.genfromtxt('training_set.txt', delimiter=',')\n",
    "test_data = np.genfromtxt('test_set.txt', delimiter=',')\n",
    "\n",
    "# Split data in x and y\n",
    "x_train = train_data[:,0]\n",
    "y_train = train_data[:,1]\n",
    "x_test = test_data[:,0]\n",
    "y_test = test_data[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot train and test points \n",
    "plt.close('all')\n",
    "plt.plot(x_train, y_train, color='r', ls='', marker='.', label='Train data points')\n",
    "plt.plot(x_test, y_test, color='b', ls='', marker='.', label='Test data points')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activation functions, their derivatives to \n",
    "# work with single values and numpy vectors\n",
    "\n",
    "# --- sigmoid ---\n",
    "from scipy.special import expit\n",
    "expit_der = lambda x: expit(x) * (1 - expit(x))\n",
    "\n",
    "\n",
    "# --- ReLU ---\n",
    "def ReLU(x): \n",
    "    if type(x) is np.ndarray: \n",
    "        return np.array([ (x_i if x_i>0 else 0) for x_i in x])\n",
    "    else: \n",
    "        return (x if x>0 else 0) \n",
    "    \n",
    "def ReLU_der(x): \n",
    "    if type(x) is np.ndarray: \n",
    "        return np.array([ (1 if x_i>0 else 0) for x_i in x])\n",
    "    else: \n",
    "        return (1 if x>0 else 0) \n",
    "    \n",
    "\n",
    "# --- Leaky ReLU ---\n",
    "def leaky_ReLU(x, slope=0.05): \n",
    "    if type(x) is np.ndarray: \n",
    "        return np.array([ (x_i if x_i>0 else slope*x_i) for x_i in x])\n",
    "    else: \n",
    "        return (x if x>0 else slope*x_i) \n",
    "    \n",
    "def leaky_ReLU_der(x, slope=0.05): \n",
    "    if type(x) is np.ndarray: \n",
    "        return np.array([ (1 if x_i>0 else slope) for x_i in x])\n",
    "    else: \n",
    "        return (1 if x>0 else slope) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's define the class of the network \n",
    "The class below was modified to work with L1 regularization and to take as argument its Lambda parameter and the activation function to use. Furthermore, I added to the Network class a method for the training.\n",
    "\n",
    "After running the notebook I have noticed that the ReLU activation function could lead to a gradient explosion in certain situations. To avoid that, I added a Gradient Clipping structure that rescales the gradient if its norm is greater than a certain quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    \n",
    "    def __init__(self, Ni, Nh1, Nh2, No, act, L1_lambda):\n",
    "            \n",
    "        ### WEIGHT INITIALIZATION (Xavier)\n",
    "        # Initialize hidden weights and biases (layer 1)\n",
    "        Wh1 = (np.random.rand(Nh1, Ni) - 0.5) * np.sqrt(12 / (Nh1 + Ni))\n",
    "        Bh1 = np.zeros([Nh1, 1])\n",
    "        self.WBh1 = np.concatenate([Wh1, Bh1], 1) # Weight matrix including biases\n",
    "        # Initialize hidden weights and biases (layer 2)\n",
    "        Wh2 = (np.random.rand(Nh2, Nh1) - 0.5) * np.sqrt(12 / (Nh2 + Nh1))\n",
    "        Bh2 = np.zeros([Nh2, 1])\n",
    "        self.WBh2 = np.concatenate([Wh2, Bh2], 1) # Weight matrix including biases\n",
    "        # Initialize output weights and biases\n",
    "        Wo = (np.random.rand(No, Nh2) - 0.5) * np.sqrt(12 / (No + Nh2))\n",
    "        Bo = np.zeros([No, 1])\n",
    "        self.WBo = np.concatenate([Wo, Bo], 1) # Weight matrix including biases\n",
    "        \n",
    "        ### ACTIVATION FUNCTION (given as argument)\n",
    "        self.act = eval(act)\n",
    "        self.act_der = eval(act + '_der')\n",
    "        \n",
    "        ### Set lambda for L1 REGULARIZATION\n",
    "        self.L1_lambda = L1_lambda\n",
    "        \n",
    "        \n",
    "    def forward(self, x, additional_out=False):\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        x = np.array(x)\n",
    "        \n",
    "        ### Hidden layer 1\n",
    "        # Add bias term\n",
    "        X = np.append(x, 1)\n",
    "        # Forward pass (linear)\n",
    "        H1 = np.matmul(self.WBh1, X)\n",
    "        # Activation function\n",
    "        Z1 = self.act(H1)\n",
    "        \n",
    "        ### Hidden layer 2\n",
    "        # Add bias term\n",
    "        Z1 = np.append(Z1, 1)\n",
    "        # Forward pass (linear)\n",
    "        H2 = np.matmul(self.WBh2, Z1)\n",
    "        # Activation function\n",
    "        Z2 = self.act(H2)\n",
    "        \n",
    "        ### Output layer\n",
    "        # Add bias term\n",
    "        Z2 = np.append(Z2, 1)\n",
    "        # Forward pass (linear)\n",
    "        Y = np.matmul(self.WBo, Z2)\n",
    "        # NO activation function\n",
    "        \n",
    "        if additional_out:\n",
    "            return Y.squeeze(), Z2\n",
    "        \n",
    "        return Y.squeeze()\n",
    "        \n",
    "                                    # If not specified, do not use Gradient Clipping\n",
    "    def update(self, x, label, lr, clip_norm = np.inf):\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        X = np.array(x)\n",
    "        \n",
    "        ### Hidden layer 1\n",
    "        # Add bias term\n",
    "        X = np.append(X, 1)\n",
    "        # Forward pass (linear)\n",
    "        H1 = np.matmul(self.WBh1, X)\n",
    "        # Activation function\n",
    "        Z1 = self.act(H1)\n",
    "        \n",
    "        ### Hidden layer 2\n",
    "        # Add bias term\n",
    "        Z1 = np.append(Z1, 1)\n",
    "        # Forward pass (linear)\n",
    "        H2 = np.matmul(self.WBh2, Z1)\n",
    "        # Activation function\n",
    "        Z2 = self.act(H2)\n",
    "        \n",
    "        ### Output layer\n",
    "        # Add bias term\n",
    "        Z2 = np.append(Z2, 1)\n",
    "        # Forward pass (linear)\n",
    "        Y = np.matmul(self.WBo, Z2)\n",
    "        # NO activation function\n",
    "        \n",
    "        # Evaluate the derivative terms\n",
    "        D1 = Y - label    # derivate of the loss wrt Y\n",
    "        D2 = Z2\n",
    "        D3 = self.WBo[:,:-1]\n",
    "        D4 = self.act_der(H2)\n",
    "        D5 = Z1\n",
    "        D6 = self.WBh2[:,:-1]\n",
    "        D7 = self.act_der(H1)\n",
    "        D8 = X\n",
    "        \n",
    "        # Layer Error\n",
    "        Eo = D1\n",
    "        Eh2 = np.matmul(Eo, D3) * D4\n",
    "        Eh1 = np.matmul(Eh2, D6) * D7\n",
    "        \n",
    "        # Derivative for weight matrices (gradients)\n",
    "        dWBo = np.matmul(Eo.reshape(-1,1), D2.reshape(1,-1))\n",
    "        dWBh2 = np.matmul(Eh2.reshape(-1,1), D5.reshape(1,-1))\n",
    "        dWBh1 = np.matmul(Eh1.reshape(-1,1), D8.reshape(1,-1))\n",
    "        \n",
    "        # GRADIENT CLIPPING\n",
    "        # Compute grad norm; if norm > 'clip_norm', rescale grad to have norm = 'clip_norm'\n",
    "        grad_norm = np.linalg.norm(np.concatenate([dWBo.flatten(),dWBh2.flatten(),dWBh1.flatten()]))\n",
    "        if grad_norm >= clip_norm:\n",
    "            dWBo  = dWBo/grad_norm * clip_norm\n",
    "            dWBh2 = dWBh2/grad_norm * clip_norm\n",
    "            dWBh1 = dWBh1/grad_norm * clip_norm\n",
    "        \n",
    "        # Update the weights (considering L1 REGULARIZATION)\n",
    "        self.WBh1 -= lr * (dWBh1 + self.L1_lambda*np.sign(self.WBh1))\n",
    "        self.WBh2 -= lr * (dWBh2 + self.L1_lambda*np.sign(self.WBh2))\n",
    "        self.WBo -= lr * (dWBo + self.L1_lambda*np.sign(self.WBo))\n",
    "        \n",
    "        # Evaluate loss function\n",
    "        loss = (Y - label)**2/2 + self.L1_lambda * ( np.abs(self.WBh1).sum() + \n",
    "                                                     np.abs(self.WBh2).sum() + \n",
    "                                                     np.abs(self.WBo).sum() )\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def plot_weights(self):\n",
    "    \n",
    "        fig, axs = plt.subplots(3, 1, figsize=(12,6))\n",
    "        axs[0].hist(self.WBh1.flatten(), 20)\n",
    "        axs[1].hist(self.WBh2.flatten(), 50)\n",
    "        axs[2].hist(self.WBo.flatten(), 20)\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    def train(self, x_train, y_train, x_test, y_test, num_epochs, \n",
    "              lr, en_decay=False, lr_final=0, return_log=False, out_log=False):\n",
    "\n",
    "        # if we want to keep track of the errors, initialize arrays to contain them\n",
    "        if out_log or return_log: \n",
    "            train_loss_log = []\n",
    "            test_loss_log = []\n",
    "        \n",
    "        if en_decay:\n",
    "            lr_decay = (lr_final / lr)**(1 / num_epochs)\n",
    "        \n",
    "        for num_ep in range(num_epochs):\n",
    "            \n",
    "            # IF 'en_decay' is true, compute new learning rate for each epoch\n",
    "            if en_decay:\n",
    "                lr *= lr_decay\n",
    "                \n",
    "            # Train single epoch (sample by sample, no batch for now)\n",
    "            #train_loss_vec = [self.update(x, y, lr) for x, y in zip(x_train, y_train)]\n",
    "            train_loss_vec = [self.update(x, y, lr, clip_norm=10) for x, y in zip(x_train, y_train)]\n",
    "            avg_train_loss = np.mean(train_loss_vec)\n",
    "            \n",
    "            # Test network (The test score is given only by the MSE, we don't add the reg term)\n",
    "            y_test_est = np.array([self.forward(x) for x in x_test])\n",
    "            avg_test_loss = np.mean((y_test_est - y_test)**2/2) # + self.L1_lambda * ( np.abs(self.WBh1).sum() + \n",
    "                                                               #                     np.abs(self.WBh2).sum() + \n",
    "                                                               #                     np.abs(self.WBo).sum() ))\n",
    "            \n",
    "            # if we want to keep track of errors, add them the their array\n",
    "            if return_log or out_log:\n",
    "                train_loss_log.append(avg_train_loss)\n",
    "                test_loss_log.append(avg_test_loss)\n",
    "                # IF 'out_log' is true, save and print train/test errors for each epoch\n",
    "                if out_log:\n",
    "                    print('Epoch %d - lr: %.5f - Train loss: %.5f - Test loss: %.5f' % (num_ep + 1, lr, avg_train_loss, avg_test_loss))\n",
    "\n",
    "        # IF 'return_log' is true, return arrays with train/test loss values\n",
    "        # ELSE return only the final values of the trained network\n",
    "        if return_log:\n",
    "            return train_loss_log, test_loss_log\n",
    "        else:\n",
    "            return avg_train_loss, avg_test_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the network class\n",
    "Let's perform a simple training of a generic NN to check if the class is working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - lr: 0.00998 - Train loss: 16.24111 - Test loss: 9.10354\n",
      "Epoch 2 - lr: 0.00995 - Train loss: 11.06866 - Test loss: 9.00544\n",
      "Epoch 3 - lr: 0.00993 - Train loss: 8.05419 - Test loss: 5.17317\n",
      "Epoch 4 - lr: 0.00991 - Train loss: 5.66599 - Test loss: 5.00482\n",
      "Epoch 5 - lr: 0.00989 - Train loss: 4.37686 - Test loss: 3.77914\n",
      "Epoch 6 - lr: 0.00986 - Train loss: 3.50995 - Test loss: 3.11188\n",
      "Epoch 7 - lr: 0.00984 - Train loss: 3.00625 - Test loss: 3.87253\n",
      "Epoch 8 - lr: 0.00982 - Train loss: 2.75736 - Test loss: 3.21834\n",
      "Epoch 9 - lr: 0.00979 - Train loss: 2.63992 - Test loss: 2.07359\n",
      "Epoch 10 - lr: 0.00977 - Train loss: 2.35714 - Test loss: 2.12813\n",
      "Epoch 11 - lr: 0.00975 - Train loss: 2.23055 - Test loss: 3.09174\n",
      "Epoch 12 - lr: 0.00973 - Train loss: 2.27914 - Test loss: 2.00178\n",
      "Epoch 13 - lr: 0.00971 - Train loss: 2.07231 - Test loss: 1.94310\n",
      "Epoch 14 - lr: 0.00968 - Train loss: 2.04394 - Test loss: 1.95127\n",
      "Epoch 15 - lr: 0.00966 - Train loss: 1.99217 - Test loss: 2.65293\n",
      "Epoch 16 - lr: 0.00964 - Train loss: 2.02940 - Test loss: 1.95373\n",
      "Epoch 17 - lr: 0.00962 - Train loss: 2.00241 - Test loss: 2.26531\n",
      "Epoch 18 - lr: 0.00959 - Train loss: 2.02508 - Test loss: 2.25085\n",
      "Epoch 19 - lr: 0.00957 - Train loss: 2.02301 - Test loss: 2.85804\n",
      "Epoch 20 - lr: 0.00955 - Train loss: 2.11729 - Test loss: 2.11403\n",
      "Epoch 21 - lr: 0.00953 - Train loss: 2.09676 - Test loss: 2.47533\n",
      "Epoch 22 - lr: 0.00951 - Train loss: 2.08455 - Test loss: 2.50898\n",
      "Epoch 23 - lr: 0.00948 - Train loss: 2.07330 - Test loss: 2.18628\n",
      "Epoch 24 - lr: 0.00946 - Train loss: 2.10524 - Test loss: 1.86036\n",
      "Epoch 25 - lr: 0.00944 - Train loss: 1.99602 - Test loss: 2.91697\n",
      "Epoch 26 - lr: 0.00942 - Train loss: 2.06988 - Test loss: 2.88762\n",
      "Epoch 27 - lr: 0.00940 - Train loss: 2.08581 - Test loss: 2.47758\n",
      "Epoch 28 - lr: 0.00938 - Train loss: 2.06936 - Test loss: 2.64014\n",
      "Epoch 29 - lr: 0.00935 - Train loss: 2.06337 - Test loss: 2.21026\n",
      "Epoch 30 - lr: 0.00933 - Train loss: 2.06823 - Test loss: 2.11139\n",
      "Epoch 31 - lr: 0.00931 - Train loss: 2.02148 - Test loss: 2.84399\n",
      "Epoch 32 - lr: 0.00929 - Train loss: 2.05626 - Test loss: 2.38632\n",
      "Epoch 33 - lr: 0.00927 - Train loss: 2.04408 - Test loss: 2.29940\n",
      "Epoch 34 - lr: 0.00925 - Train loss: 2.10979 - Test loss: 1.75913\n",
      "Epoch 35 - lr: 0.00923 - Train loss: 1.97130 - Test loss: 1.80526\n",
      "Epoch 36 - lr: 0.00920 - Train loss: 1.91741 - Test loss: 2.73619\n",
      "Epoch 37 - lr: 0.00918 - Train loss: 1.98351 - Test loss: 1.87564\n",
      "Epoch 38 - lr: 0.00916 - Train loss: 1.93328 - Test loss: 2.78481\n",
      "Epoch 39 - lr: 0.00914 - Train loss: 2.01074 - Test loss: 2.72826\n",
      "Epoch 40 - lr: 0.00912 - Train loss: 1.99299 - Test loss: 2.13769\n",
      "Epoch 41 - lr: 0.00910 - Train loss: 1.98082 - Test loss: 2.75466\n",
      "Epoch 42 - lr: 0.00908 - Train loss: 2.02409 - Test loss: 1.77601\n",
      "Epoch 43 - lr: 0.00906 - Train loss: 2.01763 - Test loss: 1.75772\n",
      "Epoch 44 - lr: 0.00904 - Train loss: 1.91771 - Test loss: 1.77470\n",
      "Epoch 45 - lr: 0.00902 - Train loss: 1.88567 - Test loss: 1.80821\n",
      "Epoch 46 - lr: 0.00899 - Train loss: 1.88579 - Test loss: 1.79115\n",
      "Epoch 47 - lr: 0.00897 - Train loss: 1.85740 - Test loss: 2.73235\n",
      "Epoch 48 - lr: 0.00895 - Train loss: 1.90741 - Test loss: 2.69142\n",
      "Epoch 49 - lr: 0.00893 - Train loss: 1.96045 - Test loss: 2.09178\n",
      "Epoch 50 - lr: 0.00891 - Train loss: 1.93133 - Test loss: 2.30643\n",
      "Epoch 51 - lr: 0.00889 - Train loss: 1.96325 - Test loss: 2.69292\n",
      "Epoch 52 - lr: 0.00887 - Train loss: 1.97860 - Test loss: 1.83921\n",
      "Epoch 53 - lr: 0.00885 - Train loss: 1.93868 - Test loss: 2.50550\n",
      "Epoch 54 - lr: 0.00883 - Train loss: 1.96102 - Test loss: 1.80517\n",
      "Epoch 55 - lr: 0.00881 - Train loss: 1.95774 - Test loss: 1.80636\n",
      "Epoch 56 - lr: 0.00879 - Train loss: 1.89371 - Test loss: 1.75262\n",
      "Epoch 57 - lr: 0.00877 - Train loss: 1.87727 - Test loss: 1.76328\n",
      "Epoch 58 - lr: 0.00875 - Train loss: 1.83946 - Test loss: 2.17864\n",
      "Epoch 59 - lr: 0.00873 - Train loss: 1.89336 - Test loss: 2.16953\n",
      "Epoch 60 - lr: 0.00871 - Train loss: 1.90193 - Test loss: 2.61335\n",
      "Epoch 61 - lr: 0.00869 - Train loss: 1.92789 - Test loss: 2.10835\n",
      "Epoch 62 - lr: 0.00867 - Train loss: 1.91164 - Test loss: 2.16225\n",
      "Epoch 63 - lr: 0.00865 - Train loss: 1.92471 - Test loss: 1.84326\n",
      "Epoch 64 - lr: 0.00863 - Train loss: 1.92409 - Test loss: 1.95746\n",
      "Epoch 65 - lr: 0.00861 - Train loss: 1.95261 - Test loss: 1.77206\n",
      "Epoch 66 - lr: 0.00859 - Train loss: 1.93068 - Test loss: 1.77453\n",
      "Epoch 67 - lr: 0.00857 - Train loss: 1.86720 - Test loss: 2.51970\n",
      "Epoch 68 - lr: 0.00855 - Train loss: 1.92208 - Test loss: 2.45794\n",
      "Epoch 69 - lr: 0.00853 - Train loss: 2.00086 - Test loss: 1.69228\n",
      "Epoch 70 - lr: 0.00851 - Train loss: 1.92840 - Test loss: 1.76452\n",
      "Epoch 71 - lr: 0.00849 - Train loss: 1.90077 - Test loss: 1.93171\n",
      "Epoch 72 - lr: 0.00847 - Train loss: 1.89694 - Test loss: 2.12189\n",
      "Epoch 73 - lr: 0.00845 - Train loss: 1.94457 - Test loss: 1.72202\n",
      "Epoch 74 - lr: 0.00843 - Train loss: 1.88011 - Test loss: 2.26474\n",
      "Epoch 75 - lr: 0.00841 - Train loss: 1.89480 - Test loss: 1.91864\n",
      "Epoch 76 - lr: 0.00839 - Train loss: 1.87560 - Test loss: 2.19856\n",
      "Epoch 77 - lr: 0.00838 - Train loss: 1.91818 - Test loss: 2.08463\n",
      "Epoch 78 - lr: 0.00836 - Train loss: 1.93322 - Test loss: 1.86731\n",
      "Epoch 79 - lr: 0.00834 - Train loss: 1.89340 - Test loss: 2.45838\n",
      "Epoch 80 - lr: 0.00832 - Train loss: 1.91647 - Test loss: 2.35675\n",
      "Epoch 81 - lr: 0.00830 - Train loss: 1.89737 - Test loss: 2.09505\n",
      "Epoch 82 - lr: 0.00828 - Train loss: 1.91242 - Test loss: 1.81215\n",
      "Epoch 83 - lr: 0.00826 - Train loss: 1.89706 - Test loss: 2.32330\n",
      "Epoch 84 - lr: 0.00824 - Train loss: 1.93630 - Test loss: 1.70622\n",
      "Epoch 85 - lr: 0.00822 - Train loss: 1.92456 - Test loss: 1.71891\n",
      "Epoch 86 - lr: 0.00820 - Train loss: 1.87406 - Test loss: 1.69077\n",
      "Epoch 87 - lr: 0.00818 - Train loss: 1.89230 - Test loss: 1.74245\n",
      "Epoch 88 - lr: 0.00817 - Train loss: 1.85080 - Test loss: 2.38365\n",
      "Epoch 89 - lr: 0.00815 - Train loss: 1.94855 - Test loss: 1.69716\n",
      "Epoch 90 - lr: 0.00813 - Train loss: 1.86284 - Test loss: 1.70642\n",
      "Epoch 91 - lr: 0.00811 - Train loss: 1.84976 - Test loss: 2.38196\n",
      "Epoch 92 - lr: 0.00809 - Train loss: 1.90070 - Test loss: 2.37083\n",
      "Epoch 93 - lr: 0.00807 - Train loss: 1.90193 - Test loss: 1.85608\n",
      "Epoch 94 - lr: 0.00805 - Train loss: 1.87848 - Test loss: 2.35010\n",
      "Epoch 95 - lr: 0.00804 - Train loss: 1.87050 - Test loss: 2.04825\n",
      "Epoch 96 - lr: 0.00802 - Train loss: 1.86400 - Test loss: 1.96805\n",
      "Epoch 97 - lr: 0.00800 - Train loss: 1.88185 - Test loss: 1.77290\n",
      "Epoch 98 - lr: 0.00798 - Train loss: 1.87094 - Test loss: 2.27081\n",
      "Epoch 99 - lr: 0.00796 - Train loss: 1.87348 - Test loss: 1.87927\n",
      "Epoch 100 - lr: 0.00794 - Train loss: 1.86741 - Test loss: 1.91661\n",
      "Epoch 101 - lr: 0.00793 - Train loss: 1.86112 - Test loss: 2.11702\n",
      "Epoch 102 - lr: 0.00791 - Train loss: 1.86626 - Test loss: 1.84342\n",
      "Epoch 103 - lr: 0.00789 - Train loss: 1.85634 - Test loss: 2.01763\n",
      "Epoch 104 - lr: 0.00787 - Train loss: 1.88429 - Test loss: 1.69716\n",
      "Epoch 105 - lr: 0.00785 - Train loss: 1.86916 - Test loss: 2.11801\n",
      "Epoch 106 - lr: 0.00783 - Train loss: 1.86716 - Test loss: 1.87643\n",
      "Epoch 107 - lr: 0.00782 - Train loss: 1.84880 - Test loss: 2.12716\n",
      "Epoch 108 - lr: 0.00780 - Train loss: 1.84365 - Test loss: 1.94034\n",
      "Epoch 109 - lr: 0.00778 - Train loss: 1.83959 - Test loss: 1.88810\n",
      "Epoch 110 - lr: 0.00776 - Train loss: 1.85655 - Test loss: 1.74007\n",
      "Epoch 111 - lr: 0.00774 - Train loss: 1.84128 - Test loss: 2.27552\n",
      "Epoch 112 - lr: 0.00773 - Train loss: 1.82692 - Test loss: 2.14554\n",
      "Epoch 113 - lr: 0.00771 - Train loss: 1.79681 - Test loss: 2.01093\n",
      "Epoch 114 - lr: 0.00769 - Train loss: 1.78932 - Test loss: 1.92604\n",
      "Epoch 115 - lr: 0.00767 - Train loss: 1.79420 - Test loss: 2.25799\n",
      "Epoch 116 - lr: 0.00766 - Train loss: 1.78666 - Test loss: 1.69584\n",
      "Epoch 117 - lr: 0.00764 - Train loss: 1.80198 - Test loss: 2.01589\n",
      "Epoch 118 - lr: 0.00762 - Train loss: 1.79943 - Test loss: 2.18374\n",
      "Epoch 119 - lr: 0.00760 - Train loss: 1.78341 - Test loss: 1.73698\n",
      "Epoch 120 - lr: 0.00759 - Train loss: 1.79458 - Test loss: 2.08778\n",
      "Epoch 121 - lr: 0.00757 - Train loss: 1.78220 - Test loss: 2.05995\n",
      "Epoch 122 - lr: 0.00755 - Train loss: 1.77845 - Test loss: 1.69963\n",
      "Epoch 123 - lr: 0.00753 - Train loss: 1.79728 - Test loss: 1.94045\n",
      "Epoch 124 - lr: 0.00752 - Train loss: 1.79280 - Test loss: 2.17238\n",
      "Epoch 125 - lr: 0.00750 - Train loss: 1.77696 - Test loss: 1.70955\n",
      "Epoch 126 - lr: 0.00748 - Train loss: 1.78791 - Test loss: 1.99757\n",
      "Epoch 127 - lr: 0.00746 - Train loss: 1.78245 - Test loss: 2.12579\n",
      "Epoch 128 - lr: 0.00745 - Train loss: 1.76549 - Test loss: 1.74780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129 - lr: 0.00743 - Train loss: 1.77335 - Test loss: 2.20931\n",
      "Epoch 130 - lr: 0.00741 - Train loss: 1.76251 - Test loss: 1.72469\n",
      "Epoch 131 - lr: 0.00740 - Train loss: 1.76770 - Test loss: 2.18458\n",
      "Epoch 132 - lr: 0.00738 - Train loss: 1.75731 - Test loss: 1.74817\n",
      "Epoch 133 - lr: 0.00736 - Train loss: 1.76196 - Test loss: 2.11264\n",
      "Epoch 134 - lr: 0.00735 - Train loss: 1.75200 - Test loss: 1.77239\n",
      "Epoch 135 - lr: 0.00733 - Train loss: 1.75731 - Test loss: 2.09840\n",
      "Epoch 136 - lr: 0.00731 - Train loss: 1.74739 - Test loss: 1.78687\n",
      "Epoch 137 - lr: 0.00729 - Train loss: 1.74921 - Test loss: 2.16955\n",
      "Epoch 138 - lr: 0.00728 - Train loss: 1.74608 - Test loss: 2.12981\n",
      "Epoch 139 - lr: 0.00726 - Train loss: 1.75833 - Test loss: 1.68573\n",
      "Epoch 140 - lr: 0.00724 - Train loss: 1.73550 - Test loss: 2.08876\n",
      "Epoch 141 - lr: 0.00723 - Train loss: 1.74050 - Test loss: 2.07617\n",
      "Epoch 142 - lr: 0.00721 - Train loss: 1.74512 - Test loss: 1.76822\n",
      "Epoch 143 - lr: 0.00719 - Train loss: 1.72693 - Test loss: 1.80360\n",
      "Epoch 144 - lr: 0.00718 - Train loss: 1.73567 - Test loss: 2.09703\n",
      "Epoch 145 - lr: 0.00716 - Train loss: 1.73464 - Test loss: 2.12764\n",
      "Epoch 146 - lr: 0.00714 - Train loss: 1.73604 - Test loss: 1.76621\n",
      "Epoch 147 - lr: 0.00713 - Train loss: 1.71855 - Test loss: 2.08812\n",
      "Epoch 148 - lr: 0.00711 - Train loss: 1.74085 - Test loss: 1.72160\n",
      "Epoch 149 - lr: 0.00710 - Train loss: 1.71835 - Test loss: 1.94544\n",
      "Epoch 150 - lr: 0.00708 - Train loss: 1.72389 - Test loss: 2.09902\n",
      "Epoch 151 - lr: 0.00706 - Train loss: 1.73941 - Test loss: 1.69695\n",
      "Epoch 152 - lr: 0.00705 - Train loss: 1.71581 - Test loss: 2.02131\n",
      "Epoch 153 - lr: 0.00703 - Train loss: 1.72146 - Test loss: 2.06722\n",
      "Epoch 154 - lr: 0.00701 - Train loss: 1.73264 - Test loss: 1.68930\n",
      "Epoch 155 - lr: 0.00700 - Train loss: 1.70969 - Test loss: 1.98961\n",
      "Epoch 156 - lr: 0.00698 - Train loss: 1.71672 - Test loss: 2.05506\n",
      "Epoch 157 - lr: 0.00697 - Train loss: 1.72004 - Test loss: 2.06879\n",
      "Epoch 158 - lr: 0.00695 - Train loss: 1.71403 - Test loss: 1.78777\n",
      "Epoch 159 - lr: 0.00693 - Train loss: 1.70489 - Test loss: 1.94786\n",
      "Epoch 160 - lr: 0.00692 - Train loss: 1.71815 - Test loss: 1.73227\n",
      "Epoch 161 - lr: 0.00690 - Train loss: 1.70369 - Test loss: 2.04008\n",
      "Epoch 162 - lr: 0.00689 - Train loss: 1.72080 - Test loss: 1.71175\n",
      "Epoch 163 - lr: 0.00687 - Train loss: 1.70037 - Test loss: 2.06878\n",
      "Epoch 164 - lr: 0.00685 - Train loss: 1.71374 - Test loss: 2.04694\n",
      "Epoch 165 - lr: 0.00684 - Train loss: 1.70803 - Test loss: 1.74841\n",
      "Epoch 166 - lr: 0.00682 - Train loss: 1.69559 - Test loss: 1.97212\n",
      "Epoch 167 - lr: 0.00681 - Train loss: 1.71679 - Test loss: 1.65957\n",
      "Epoch 168 - lr: 0.00679 - Train loss: 1.69427 - Test loss: 2.03993\n",
      "Epoch 169 - lr: 0.00678 - Train loss: 1.70839 - Test loss: 2.04637\n",
      "Epoch 170 - lr: 0.00676 - Train loss: 1.70131 - Test loss: 1.74798\n",
      "Epoch 171 - lr: 0.00675 - Train loss: 1.69041 - Test loss: 1.88655\n",
      "Epoch 172 - lr: 0.00673 - Train loss: 1.69981 - Test loss: 1.74390\n",
      "Epoch 173 - lr: 0.00671 - Train loss: 1.69944 - Test loss: 1.99017\n",
      "Epoch 174 - lr: 0.00670 - Train loss: 1.70141 - Test loss: 1.76044\n",
      "Epoch 175 - lr: 0.00668 - Train loss: 1.68733 - Test loss: 1.95453\n",
      "Epoch 176 - lr: 0.00667 - Train loss: 1.69659 - Test loss: 1.89979\n",
      "Epoch 177 - lr: 0.00665 - Train loss: 1.69827 - Test loss: 1.69412\n",
      "Epoch 178 - lr: 0.00664 - Train loss: 1.68323 - Test loss: 2.02430\n",
      "Epoch 179 - lr: 0.00662 - Train loss: 1.69091 - Test loss: 1.73010\n",
      "Epoch 180 - lr: 0.00661 - Train loss: 1.68757 - Test loss: 1.95670\n",
      "Epoch 181 - lr: 0.00659 - Train loss: 1.69722 - Test loss: 1.67759\n",
      "Epoch 182 - lr: 0.00658 - Train loss: 1.67861 - Test loss: 2.00943\n",
      "Epoch 183 - lr: 0.00656 - Train loss: 1.69337 - Test loss: 1.77834\n",
      "Epoch 184 - lr: 0.00655 - Train loss: 1.67980 - Test loss: 1.80470\n",
      "Epoch 185 - lr: 0.00653 - Train loss: 1.68068 - Test loss: 1.77049\n",
      "Epoch 186 - lr: 0.00652 - Train loss: 1.68326 - Test loss: 1.97129\n",
      "Epoch 187 - lr: 0.00650 - Train loss: 1.69001 - Test loss: 1.66623\n",
      "Epoch 188 - lr: 0.00649 - Train loss: 1.67076 - Test loss: 1.92349\n",
      "Epoch 189 - lr: 0.00647 - Train loss: 1.68344 - Test loss: 1.81261\n",
      "Epoch 190 - lr: 0.00646 - Train loss: 1.67716 - Test loss: 1.73512\n",
      "Epoch 191 - lr: 0.00644 - Train loss: 1.67051 - Test loss: 1.81386\n",
      "Epoch 192 - lr: 0.00643 - Train loss: 1.67679 - Test loss: 1.89615\n",
      "Epoch 193 - lr: 0.00641 - Train loss: 1.68184 - Test loss: 1.66272\n",
      "Epoch 194 - lr: 0.00640 - Train loss: 1.66469 - Test loss: 1.88350\n",
      "Epoch 195 - lr: 0.00638 - Train loss: 1.67632 - Test loss: 1.80948\n",
      "Epoch 196 - lr: 0.00637 - Train loss: 1.67132 - Test loss: 1.71929\n",
      "Epoch 197 - lr: 0.00635 - Train loss: 1.67079 - Test loss: 1.94341\n",
      "Epoch 198 - lr: 0.00634 - Train loss: 1.67576 - Test loss: 1.66647\n",
      "Epoch 199 - lr: 0.00632 - Train loss: 1.66777 - Test loss: 1.95494\n",
      "Epoch 200 - lr: 0.00631 - Train loss: 1.67235 - Test loss: 1.67494\n",
      "Epoch 201 - lr: 0.00630 - Train loss: 1.65903 - Test loss: 1.77645\n",
      "Epoch 202 - lr: 0.00628 - Train loss: 1.66684 - Test loss: 1.83377\n",
      "Epoch 203 - lr: 0.00627 - Train loss: 1.66763 - Test loss: 1.68412\n",
      "Epoch 204 - lr: 0.00625 - Train loss: 1.66379 - Test loss: 1.90205\n",
      "Epoch 205 - lr: 0.00624 - Train loss: 1.67120 - Test loss: 1.64878\n",
      "Epoch 206 - lr: 0.00622 - Train loss: 1.66196 - Test loss: 1.91617\n",
      "Epoch 207 - lr: 0.00621 - Train loss: 1.66888 - Test loss: 1.65261\n",
      "Epoch 208 - lr: 0.00619 - Train loss: 1.65973 - Test loss: 1.88513\n",
      "Epoch 209 - lr: 0.00618 - Train loss: 1.66664 - Test loss: 1.65233\n",
      "Epoch 210 - lr: 0.00617 - Train loss: 1.65825 - Test loss: 1.87212\n",
      "Epoch 211 - lr: 0.00615 - Train loss: 1.66403 - Test loss: 1.65711\n",
      "Epoch 212 - lr: 0.00614 - Train loss: 1.65660 - Test loss: 1.85103\n",
      "Epoch 213 - lr: 0.00612 - Train loss: 1.66100 - Test loss: 1.66367\n",
      "Epoch 214 - lr: 0.00611 - Train loss: 1.65506 - Test loss: 1.83042\n",
      "Epoch 215 - lr: 0.00610 - Train loss: 1.65727 - Test loss: 1.67968\n",
      "Epoch 216 - lr: 0.00608 - Train loss: 1.65397 - Test loss: 1.80644\n",
      "Epoch 217 - lr: 0.00607 - Train loss: 1.65443 - Test loss: 1.68543\n",
      "Epoch 218 - lr: 0.00605 - Train loss: 1.65266 - Test loss: 1.79013\n",
      "Epoch 219 - lr: 0.00604 - Train loss: 1.65207 - Test loss: 1.68794\n",
      "Epoch 220 - lr: 0.00603 - Train loss: 1.65130 - Test loss: 1.77717\n",
      "Epoch 221 - lr: 0.00601 - Train loss: 1.64991 - Test loss: 1.68913\n",
      "Epoch 222 - lr: 0.00600 - Train loss: 1.64994 - Test loss: 1.76604\n",
      "Epoch 223 - lr: 0.00598 - Train loss: 1.64786 - Test loss: 1.68961\n",
      "Epoch 224 - lr: 0.00597 - Train loss: 1.64857 - Test loss: 1.75616\n",
      "Epoch 225 - lr: 0.00596 - Train loss: 1.64588 - Test loss: 1.68960\n",
      "Epoch 226 - lr: 0.00594 - Train loss: 1.64718 - Test loss: 1.74725\n",
      "Epoch 227 - lr: 0.00593 - Train loss: 1.64395 - Test loss: 1.68921\n",
      "Epoch 228 - lr: 0.00592 - Train loss: 1.64577 - Test loss: 1.73914\n",
      "Epoch 229 - lr: 0.00590 - Train loss: 1.64208 - Test loss: 1.68853\n",
      "Epoch 230 - lr: 0.00589 - Train loss: 1.64436 - Test loss: 1.73171\n",
      "Epoch 231 - lr: 0.00587 - Train loss: 1.64024 - Test loss: 1.68761\n",
      "Epoch 232 - lr: 0.00586 - Train loss: 1.64294 - Test loss: 1.72486\n",
      "Epoch 233 - lr: 0.00585 - Train loss: 1.63843 - Test loss: 1.68650\n",
      "Epoch 234 - lr: 0.00583 - Train loss: 1.64150 - Test loss: 1.71850\n",
      "Epoch 235 - lr: 0.00582 - Train loss: 1.63665 - Test loss: 1.68524\n",
      "Epoch 236 - lr: 0.00581 - Train loss: 1.64006 - Test loss: 1.71257\n",
      "Epoch 237 - lr: 0.00579 - Train loss: 1.63489 - Test loss: 1.68384\n",
      "Epoch 238 - lr: 0.00578 - Train loss: 1.63861 - Test loss: 1.70702\n",
      "Epoch 239 - lr: 0.00577 - Train loss: 1.63315 - Test loss: 1.68235\n",
      "Epoch 240 - lr: 0.00575 - Train loss: 1.63715 - Test loss: 1.70181\n",
      "Epoch 241 - lr: 0.00574 - Train loss: 1.63771 - Test loss: 1.70731\n",
      "Epoch 242 - lr: 0.00573 - Train loss: 1.63147 - Test loss: 1.68027\n",
      "Epoch 243 - lr: 0.00571 - Train loss: 1.63510 - Test loss: 1.69974\n",
      "Epoch 244 - lr: 0.00570 - Train loss: 1.62957 - Test loss: 1.67680\n",
      "Epoch 245 - lr: 0.00569 - Train loss: 1.63351 - Test loss: 1.69441\n",
      "Epoch 246 - lr: 0.00568 - Train loss: 1.62785 - Test loss: 1.67490\n",
      "Epoch 247 - lr: 0.00566 - Train loss: 1.63200 - Test loss: 1.68987\n",
      "Epoch 248 - lr: 0.00565 - Train loss: 1.62617 - Test loss: 1.67314\n",
      "Epoch 249 - lr: 0.00564 - Train loss: 1.63048 - Test loss: 1.68569\n",
      "Epoch 250 - lr: 0.00562 - Train loss: 1.62451 - Test loss: 1.67140\n",
      "Epoch 251 - lr: 0.00561 - Train loss: 1.62897 - Test loss: 1.68173\n",
      "Epoch 252 - lr: 0.00560 - Train loss: 1.62285 - Test loss: 1.66966\n",
      "Epoch 253 - lr: 0.00558 - Train loss: 1.62744 - Test loss: 1.67797\n",
      "Epoch 254 - lr: 0.00557 - Train loss: 1.62121 - Test loss: 1.66793\n",
      "Epoch 255 - lr: 0.00556 - Train loss: 1.62592 - Test loss: 1.67439\n",
      "Epoch 256 - lr: 0.00555 - Train loss: 1.62635 - Test loss: 1.67866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257 - lr: 0.00553 - Train loss: 1.61956 - Test loss: 1.66640\n",
      "Epoch 258 - lr: 0.00552 - Train loss: 1.62390 - Test loss: 1.66971\n",
      "Epoch 259 - lr: 0.00551 - Train loss: 1.61738 - Test loss: 1.66295\n",
      "Epoch 260 - lr: 0.00550 - Train loss: 1.62214 - Test loss: 1.66575\n",
      "Epoch 261 - lr: 0.00548 - Train loss: 1.61562 - Test loss: 1.66109\n",
      "Epoch 262 - lr: 0.00547 - Train loss: 1.62055 - Test loss: 1.66264\n",
      "Epoch 263 - lr: 0.00546 - Train loss: 1.62085 - Test loss: 1.66742\n",
      "Epoch 264 - lr: 0.00545 - Train loss: 1.61400 - Test loss: 1.66006\n",
      "Epoch 265 - lr: 0.00543 - Train loss: 1.61853 - Test loss: 1.65899\n",
      "Epoch 266 - lr: 0.00542 - Train loss: 1.61176 - Test loss: 1.65678\n",
      "Epoch 267 - lr: 0.00541 - Train loss: 1.61672 - Test loss: 1.65547\n",
      "Epoch 268 - lr: 0.00540 - Train loss: 1.61693 - Test loss: 1.66022\n",
      "Epoch 269 - lr: 0.00538 - Train loss: 1.61005 - Test loss: 1.65574\n",
      "Epoch 270 - lr: 0.00537 - Train loss: 1.61466 - Test loss: 1.65227\n",
      "Epoch 271 - lr: 0.00536 - Train loss: 1.60777 - Test loss: 1.65258\n",
      "Epoch 272 - lr: 0.00535 - Train loss: 1.61282 - Test loss: 1.64902\n",
      "Epoch 273 - lr: 0.00533 - Train loss: 1.61297 - Test loss: 1.65368\n",
      "Epoch 274 - lr: 0.00532 - Train loss: 1.60606 - Test loss: 1.65179\n",
      "Epoch 275 - lr: 0.00531 - Train loss: 1.61076 - Test loss: 1.64631\n",
      "Epoch 276 - lr: 0.00530 - Train loss: 1.61075 - Test loss: 1.65036\n",
      "Epoch 277 - lr: 0.00528 - Train loss: 1.60379 - Test loss: 1.64948\n",
      "Epoch 278 - lr: 0.00527 - Train loss: 1.60845 - Test loss: 1.64301\n",
      "Epoch 279 - lr: 0.00526 - Train loss: 1.60062 - Test loss: 1.65143\n",
      "Epoch 280 - lr: 0.00525 - Train loss: 1.60652 - Test loss: 1.63855\n",
      "Epoch 281 - lr: 0.00524 - Train loss: 1.60645 - Test loss: 1.64395\n",
      "Epoch 282 - lr: 0.00522 - Train loss: 1.59847 - Test loss: 1.65288\n",
      "Epoch 283 - lr: 0.00521 - Train loss: 1.60441 - Test loss: 1.63580\n",
      "Epoch 284 - lr: 0.00520 - Train loss: 1.60413 - Test loss: 1.64098\n",
      "Epoch 285 - lr: 0.00519 - Train loss: 1.59551 - Test loss: 1.65486\n",
      "Epoch 286 - lr: 0.00518 - Train loss: 1.60107 - Test loss: 1.63825\n",
      "Epoch 287 - lr: 0.00516 - Train loss: 1.60158 - Test loss: 1.63608\n",
      "Epoch 288 - lr: 0.00515 - Train loss: 1.59210 - Test loss: 1.65696\n",
      "Epoch 289 - lr: 0.00514 - Train loss: 1.59740 - Test loss: 1.64142\n",
      "Epoch 290 - lr: 0.00513 - Train loss: 1.59765 - Test loss: 1.63892\n",
      "Epoch 291 - lr: 0.00512 - Train loss: 1.58806 - Test loss: 1.66037\n",
      "Epoch 292 - lr: 0.00511 - Train loss: 1.59310 - Test loss: 1.64513\n",
      "Epoch 293 - lr: 0.00509 - Train loss: 1.59314 - Test loss: 1.64282\n",
      "Epoch 294 - lr: 0.00508 - Train loss: 1.58366 - Test loss: 1.66388\n",
      "Epoch 295 - lr: 0.00507 - Train loss: 1.58861 - Test loss: 1.64870\n",
      "Epoch 296 - lr: 0.00506 - Train loss: 1.58786 - Test loss: 1.63898\n",
      "Epoch 297 - lr: 0.00505 - Train loss: 1.58017 - Test loss: 1.66501\n",
      "Epoch 298 - lr: 0.00504 - Train loss: 1.58443 - Test loss: 1.64326\n",
      "Epoch 299 - lr: 0.00502 - Train loss: 1.57647 - Test loss: 1.66763\n",
      "Epoch 300 - lr: 0.00501 - Train loss: 1.58161 - Test loss: 1.65303\n",
      "Epoch 301 - lr: 0.00500 - Train loss: 1.58089 - Test loss: 1.64339\n",
      "Epoch 302 - lr: 0.00499 - Train loss: 1.57341 - Test loss: 1.66853\n",
      "Epoch 303 - lr: 0.00498 - Train loss: 1.57777 - Test loss: 1.64680\n",
      "Epoch 304 - lr: 0.00497 - Train loss: 1.57822 - Test loss: 1.64319\n",
      "Epoch 305 - lr: 0.00495 - Train loss: 1.57094 - Test loss: 1.66878\n",
      "Epoch 306 - lr: 0.00494 - Train loss: 1.57500 - Test loss: 1.64751\n",
      "Epoch 307 - lr: 0.00493 - Train loss: 1.56723 - Test loss: 1.67066\n",
      "Epoch 308 - lr: 0.00492 - Train loss: 1.57167 - Test loss: 1.64883\n",
      "Epoch 309 - lr: 0.00491 - Train loss: 1.57207 - Test loss: 1.64521\n",
      "Epoch 310 - lr: 0.00490 - Train loss: 1.56509 - Test loss: 1.66987\n",
      "Epoch 311 - lr: 0.00489 - Train loss: 1.56929 - Test loss: 1.64859\n",
      "Epoch 312 - lr: 0.00488 - Train loss: 1.56947 - Test loss: 1.64527\n",
      "Epoch 313 - lr: 0.00486 - Train loss: 1.56250 - Test loss: 1.66952\n",
      "Epoch 314 - lr: 0.00485 - Train loss: 1.56656 - Test loss: 1.64845\n",
      "Epoch 315 - lr: 0.00484 - Train loss: 1.55906 - Test loss: 1.67033\n",
      "Epoch 316 - lr: 0.00483 - Train loss: 1.56354 - Test loss: 1.64872\n",
      "Epoch 317 - lr: 0.00482 - Train loss: 1.56382 - Test loss: 1.64527\n",
      "Epoch 318 - lr: 0.00481 - Train loss: 1.55719 - Test loss: 1.66864\n",
      "Epoch 319 - lr: 0.00480 - Train loss: 1.56149 - Test loss: 1.64752\n",
      "Epoch 320 - lr: 0.00479 - Train loss: 1.56158 - Test loss: 1.64434\n",
      "Epoch 321 - lr: 0.00478 - Train loss: 1.55495 - Test loss: 1.66733\n",
      "Epoch 322 - lr: 0.00476 - Train loss: 1.55914 - Test loss: 1.64642\n",
      "Epoch 323 - lr: 0.00475 - Train loss: 1.55912 - Test loss: 1.64338\n",
      "Epoch 324 - lr: 0.00474 - Train loss: 1.55207 - Test loss: 1.65817\n",
      "Epoch 325 - lr: 0.00473 - Train loss: 1.55727 - Test loss: 1.64412\n",
      "Epoch 326 - lr: 0.00472 - Train loss: 1.54979 - Test loss: 1.65743\n",
      "Epoch 327 - lr: 0.00471 - Train loss: 1.55515 - Test loss: 1.64328\n",
      "Epoch 328 - lr: 0.00470 - Train loss: 1.55532 - Test loss: 1.64109\n",
      "Epoch 329 - lr: 0.00469 - Train loss: 1.54838 - Test loss: 1.65546\n",
      "Epoch 330 - lr: 0.00468 - Train loss: 1.55346 - Test loss: 1.64164\n",
      "Epoch 331 - lr: 0.00467 - Train loss: 1.54606 - Test loss: 1.65450\n",
      "Epoch 332 - lr: 0.00466 - Train loss: 1.55136 - Test loss: 1.64058\n",
      "Epoch 333 - lr: 0.00465 - Train loss: 1.55145 - Test loss: 1.63851\n",
      "Epoch 334 - lr: 0.00463 - Train loss: 1.54463 - Test loss: 1.65239\n",
      "Epoch 335 - lr: 0.00462 - Train loss: 1.54970 - Test loss: 1.63876\n",
      "Epoch 336 - lr: 0.00461 - Train loss: 1.54962 - Test loss: 1.63683\n",
      "Epoch 337 - lr: 0.00460 - Train loss: 1.54279 - Test loss: 1.65049\n",
      "Epoch 338 - lr: 0.00459 - Train loss: 1.54771 - Test loss: 1.63699\n",
      "Epoch 339 - lr: 0.00458 - Train loss: 1.54043 - Test loss: 1.64919\n",
      "Epoch 340 - lr: 0.00457 - Train loss: 1.54560 - Test loss: 1.63559\n",
      "Epoch 341 - lr: 0.00456 - Train loss: 1.54561 - Test loss: 1.63369\n",
      "Epoch 342 - lr: 0.00455 - Train loss: 1.53898 - Test loss: 1.64688\n",
      "Epoch 343 - lr: 0.00454 - Train loss: 1.54398 - Test loss: 1.63352\n",
      "Epoch 344 - lr: 0.00453 - Train loss: 1.54385 - Test loss: 1.63172\n",
      "Epoch 345 - lr: 0.00452 - Train loss: 1.53721 - Test loss: 1.64472\n",
      "Epoch 346 - lr: 0.00451 - Train loss: 1.54209 - Test loss: 1.63146\n",
      "Epoch 347 - lr: 0.00450 - Train loss: 1.54191 - Test loss: 1.62972\n",
      "Epoch 348 - lr: 0.00449 - Train loss: 1.53532 - Test loss: 1.64250\n",
      "Epoch 349 - lr: 0.00448 - Train loss: 1.54013 - Test loss: 1.62934\n",
      "Epoch 350 - lr: 0.00447 - Train loss: 1.53992 - Test loss: 1.62764\n",
      "Epoch 351 - lr: 0.00446 - Train loss: 1.53339 - Test loss: 1.64021\n",
      "Epoch 352 - lr: 0.00445 - Train loss: 1.53817 - Test loss: 1.62713\n",
      "Epoch 353 - lr: 0.00444 - Train loss: 1.53795 - Test loss: 1.62548\n",
      "Epoch 354 - lr: 0.00443 - Train loss: 1.53149 - Test loss: 1.63785\n",
      "Epoch 355 - lr: 0.00442 - Train loss: 1.53624 - Test loss: 1.62486\n",
      "Epoch 356 - lr: 0.00441 - Train loss: 1.53602 - Test loss: 1.62324\n",
      "Epoch 357 - lr: 0.00440 - Train loss: 1.53625 - Test loss: 1.62275\n",
      "Epoch 358 - lr: 0.00439 - Train loss: 1.52971 - Test loss: 1.63499\n",
      "Epoch 359 - lr: 0.00438 - Train loss: 1.53419 - Test loss: 1.62208\n",
      "Epoch 360 - lr: 0.00437 - Train loss: 1.53383 - Test loss: 1.62051\n",
      "Epoch 361 - lr: 0.00436 - Train loss: 1.52744 - Test loss: 1.63248\n",
      "Epoch 362 - lr: 0.00435 - Train loss: 1.53204 - Test loss: 1.61965\n",
      "Epoch 363 - lr: 0.00434 - Train loss: 1.53175 - Test loss: 1.61811\n",
      "Epoch 364 - lr: 0.00433 - Train loss: 1.53195 - Test loss: 1.61764\n",
      "Epoch 365 - lr: 0.00432 - Train loss: 1.52555 - Test loss: 1.62947\n",
      "Epoch 366 - lr: 0.00431 - Train loss: 1.52995 - Test loss: 1.61672\n",
      "Epoch 367 - lr: 0.00430 - Train loss: 1.52957 - Test loss: 1.61521\n",
      "Epoch 368 - lr: 0.00429 - Train loss: 1.52969 - Test loss: 1.61474\n",
      "Epoch 369 - lr: 0.00428 - Train loss: 1.52334 - Test loss: 1.62637\n",
      "Epoch 370 - lr: 0.00427 - Train loss: 1.52767 - Test loss: 1.61369\n",
      "Epoch 371 - lr: 0.00426 - Train loss: 1.52726 - Test loss: 1.61220\n",
      "Epoch 372 - lr: 0.00425 - Train loss: 1.52737 - Test loss: 1.61173\n",
      "Epoch 373 - lr: 0.00424 - Train loss: 1.52109 - Test loss: 1.62317\n",
      "Epoch 374 - lr: 0.00423 - Train loss: 1.52538 - Test loss: 1.61058\n",
      "Epoch 375 - lr: 0.00422 - Train loss: 1.52496 - Test loss: 1.60911\n",
      "Epoch 376 - lr: 0.00421 - Train loss: 1.52506 - Test loss: 1.60865\n",
      "Epoch 377 - lr: 0.00420 - Train loss: 1.51886 - Test loss: 1.61992\n",
      "Epoch 378 - lr: 0.00419 - Train loss: 1.52312 - Test loss: 1.60741\n",
      "Epoch 379 - lr: 0.00418 - Train loss: 1.52270 - Test loss: 1.60597\n",
      "Epoch 380 - lr: 0.00417 - Train loss: 1.52280 - Test loss: 1.60552\n",
      "Epoch 381 - lr: 0.00416 - Train loss: 1.52277 - Test loss: 1.60510\n",
      "Epoch 382 - lr: 0.00415 - Train loss: 1.51654 - Test loss: 1.61615\n",
      "Epoch 383 - lr: 0.00414 - Train loss: 1.52063 - Test loss: 1.60370\n",
      "Epoch 384 - lr: 0.00413 - Train loss: 1.52013 - Test loss: 1.60226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385 - lr: 0.00412 - Train loss: 1.52018 - Test loss: 1.60180\n",
      "Epoch 386 - lr: 0.00411 - Train loss: 1.52011 - Test loss: 1.60136\n",
      "Epoch 387 - lr: 0.00410 - Train loss: 1.51396 - Test loss: 1.61222\n",
      "Epoch 388 - lr: 0.00409 - Train loss: 1.51799 - Test loss: 1.59986\n",
      "Epoch 389 - lr: 0.00408 - Train loss: 1.51747 - Test loss: 1.59843\n",
      "Epoch 390 - lr: 0.00407 - Train loss: 1.51751 - Test loss: 1.59797\n",
      "Epoch 391 - lr: 0.00406 - Train loss: 1.51744 - Test loss: 1.59753\n",
      "Epoch 392 - lr: 0.00406 - Train loss: 1.51724 - Test loss: 1.59705\n",
      "Epoch 393 - lr: 0.00405 - Train loss: 1.51114 - Test loss: 1.60770\n",
      "Epoch 394 - lr: 0.00404 - Train loss: 1.51503 - Test loss: 1.59540\n",
      "Epoch 395 - lr: 0.00403 - Train loss: 1.51446 - Test loss: 1.59397\n",
      "Epoch 396 - lr: 0.00402 - Train loss: 1.51446 - Test loss: 1.59349\n",
      "Epoch 397 - lr: 0.00401 - Train loss: 1.51437 - Test loss: 1.59304\n",
      "Epoch 398 - lr: 0.00400 - Train loss: 1.51416 - Test loss: 1.59255\n",
      "Epoch 399 - lr: 0.00399 - Train loss: 1.50816 - Test loss: 1.60302\n",
      "Epoch 400 - lr: 0.00398 - Train loss: 1.51200 - Test loss: 1.59082\n",
      "Epoch 401 - lr: 0.00397 - Train loss: 1.51142 - Test loss: 1.58940\n",
      "Epoch 402 - lr: 0.00396 - Train loss: 1.51142 - Test loss: 1.58893\n",
      "Epoch 403 - lr: 0.00395 - Train loss: 1.51133 - Test loss: 1.58848\n",
      "Epoch 404 - lr: 0.00394 - Train loss: 1.51113 - Test loss: 1.58798\n",
      "Epoch 405 - lr: 0.00394 - Train loss: 1.51084 - Test loss: 1.58745\n",
      "Epoch 406 - lr: 0.00393 - Train loss: 1.51051 - Test loss: 1.58688\n",
      "Epoch 407 - lr: 0.00392 - Train loss: 1.50457 - Test loss: 1.59712\n",
      "Epoch 408 - lr: 0.00391 - Train loss: 1.50826 - Test loss: 1.58501\n",
      "Epoch 409 - lr: 0.00390 - Train loss: 1.50762 - Test loss: 1.58356\n",
      "Epoch 410 - lr: 0.00389 - Train loss: 1.50758 - Test loss: 1.58307\n",
      "Epoch 411 - lr: 0.00388 - Train loss: 1.50747 - Test loss: 1.58261\n",
      "Epoch 412 - lr: 0.00387 - Train loss: 1.50726 - Test loss: 1.58210\n",
      "Epoch 413 - lr: 0.00386 - Train loss: 1.50698 - Test loss: 1.58155\n",
      "Epoch 414 - lr: 0.00385 - Train loss: 1.50664 - Test loss: 1.58098\n",
      "Epoch 415 - lr: 0.00385 - Train loss: 1.50627 - Test loss: 1.58038\n",
      "Epoch 416 - lr: 0.00384 - Train loss: 1.50047 - Test loss: 1.59041\n",
      "Epoch 417 - lr: 0.00383 - Train loss: 1.50407 - Test loss: 1.57844\n",
      "Epoch 418 - lr: 0.00382 - Train loss: 1.50341 - Test loss: 1.57698\n",
      "Epoch 419 - lr: 0.00381 - Train loss: 1.50336 - Test loss: 1.57649\n",
      "Epoch 420 - lr: 0.00380 - Train loss: 1.50324 - Test loss: 1.57602\n",
      "Epoch 421 - lr: 0.00379 - Train loss: 1.50303 - Test loss: 1.57550\n",
      "Epoch 422 - lr: 0.00378 - Train loss: 1.50276 - Test loss: 1.57495\n",
      "Epoch 423 - lr: 0.00378 - Train loss: 1.50243 - Test loss: 1.57438\n",
      "Epoch 424 - lr: 0.00377 - Train loss: 1.50208 - Test loss: 1.57378\n",
      "Epoch 425 - lr: 0.00376 - Train loss: 1.50169 - Test loss: 1.57317\n",
      "Epoch 426 - lr: 0.00375 - Train loss: 1.50130 - Test loss: 1.57254\n",
      "Epoch 427 - lr: 0.00374 - Train loss: 1.50089 - Test loss: 1.57191\n",
      "Epoch 428 - lr: 0.00373 - Train loss: 1.50047 - Test loss: 1.57126\n",
      "Epoch 429 - lr: 0.00372 - Train loss: 1.50004 - Test loss: 1.57062\n",
      "Epoch 430 - lr: 0.00372 - Train loss: 1.49962 - Test loss: 1.56997\n",
      "Epoch 431 - lr: 0.00371 - Train loss: 1.49919 - Test loss: 1.56931\n",
      "Epoch 432 - lr: 0.00370 - Train loss: 1.49362 - Test loss: 1.57905\n",
      "Epoch 433 - lr: 0.00369 - Train loss: 1.49704 - Test loss: 1.56730\n",
      "Epoch 434 - lr: 0.00368 - Train loss: 1.49634 - Test loss: 1.56580\n",
      "Epoch 435 - lr: 0.00367 - Train loss: 1.49626 - Test loss: 1.56528\n",
      "Epoch 436 - lr: 0.00366 - Train loss: 1.49613 - Test loss: 1.56480\n",
      "Epoch 437 - lr: 0.00366 - Train loss: 1.49592 - Test loss: 1.56428\n",
      "Epoch 438 - lr: 0.00365 - Train loss: 1.49566 - Test loss: 1.56372\n",
      "Epoch 439 - lr: 0.00364 - Train loss: 1.49535 - Test loss: 1.56314\n",
      "Epoch 440 - lr: 0.00363 - Train loss: 1.49501 - Test loss: 1.56254\n",
      "Epoch 441 - lr: 0.00362 - Train loss: 1.49465 - Test loss: 1.56193\n",
      "Epoch 442 - lr: 0.00361 - Train loss: 1.49427 - Test loss: 1.56131\n",
      "Epoch 443 - lr: 0.00361 - Train loss: 1.49389 - Test loss: 1.56068\n",
      "Epoch 444 - lr: 0.00360 - Train loss: 1.49349 - Test loss: 1.56004\n",
      "Epoch 445 - lr: 0.00359 - Train loss: 1.49309 - Test loss: 1.55941\n",
      "Epoch 446 - lr: 0.00358 - Train loss: 1.49269 - Test loss: 1.55876\n",
      "Epoch 447 - lr: 0.00357 - Train loss: 1.49229 - Test loss: 1.55812\n",
      "Epoch 448 - lr: 0.00356 - Train loss: 1.49188 - Test loss: 1.55748\n",
      "Epoch 449 - lr: 0.00356 - Train loss: 1.49148 - Test loss: 1.55683\n",
      "Epoch 450 - lr: 0.00355 - Train loss: 1.49107 - Test loss: 1.55618\n",
      "Epoch 451 - lr: 0.00354 - Train loss: 1.49067 - Test loss: 1.55554\n",
      "Epoch 452 - lr: 0.00353 - Train loss: 1.49026 - Test loss: 1.55489\n",
      "Epoch 453 - lr: 0.00352 - Train loss: 1.48986 - Test loss: 1.55424\n",
      "Epoch 454 - lr: 0.00352 - Train loss: 1.48945 - Test loss: 1.55359\n",
      "Epoch 455 - lr: 0.00351 - Train loss: 1.48905 - Test loss: 1.55294\n",
      "Epoch 456 - lr: 0.00350 - Train loss: 1.48865 - Test loss: 1.55230\n",
      "Epoch 457 - lr: 0.00349 - Train loss: 1.48825 - Test loss: 1.55165\n",
      "Epoch 458 - lr: 0.00348 - Train loss: 1.48785 - Test loss: 1.55100\n",
      "Epoch 459 - lr: 0.00348 - Train loss: 1.48745 - Test loss: 1.55036\n",
      "Epoch 460 - lr: 0.00347 - Train loss: 1.48705 - Test loss: 1.54971\n",
      "Epoch 461 - lr: 0.00346 - Train loss: 1.48666 - Test loss: 1.54907\n",
      "Epoch 462 - lr: 0.00345 - Train loss: 1.48626 - Test loss: 1.54843\n",
      "Epoch 463 - lr: 0.00344 - Train loss: 1.48587 - Test loss: 1.54778\n",
      "Epoch 464 - lr: 0.00344 - Train loss: 1.48548 - Test loss: 1.54714\n",
      "Epoch 465 - lr: 0.00343 - Train loss: 1.48509 - Test loss: 1.54650\n",
      "Epoch 466 - lr: 0.00342 - Train loss: 1.48470 - Test loss: 1.54586\n",
      "Epoch 467 - lr: 0.00341 - Train loss: 1.48431 - Test loss: 1.54523\n",
      "Epoch 468 - lr: 0.00340 - Train loss: 1.48392 - Test loss: 1.54459\n",
      "Epoch 469 - lr: 0.00340 - Train loss: 1.48353 - Test loss: 1.54396\n",
      "Epoch 470 - lr: 0.00339 - Train loss: 1.48315 - Test loss: 1.54332\n",
      "Epoch 471 - lr: 0.00338 - Train loss: 1.48277 - Test loss: 1.54269\n",
      "Epoch 472 - lr: 0.00337 - Train loss: 1.48238 - Test loss: 1.54206\n",
      "Epoch 473 - lr: 0.00337 - Train loss: 1.48200 - Test loss: 1.54143\n",
      "Epoch 474 - lr: 0.00336 - Train loss: 1.48162 - Test loss: 1.54081\n",
      "Epoch 475 - lr: 0.00335 - Train loss: 1.48124 - Test loss: 1.54018\n",
      "Epoch 476 - lr: 0.00334 - Train loss: 1.48086 - Test loss: 1.53956\n",
      "Epoch 477 - lr: 0.00333 - Train loss: 1.48049 - Test loss: 1.53894\n",
      "Epoch 478 - lr: 0.00333 - Train loss: 1.48010 - Test loss: 1.53831\n",
      "Epoch 479 - lr: 0.00332 - Train loss: 1.47971 - Test loss: 1.53769\n",
      "Epoch 480 - lr: 0.00331 - Train loss: 1.47933 - Test loss: 1.53706\n",
      "Epoch 481 - lr: 0.00330 - Train loss: 1.47894 - Test loss: 1.53644\n",
      "Epoch 482 - lr: 0.00330 - Train loss: 1.47856 - Test loss: 1.53581\n",
      "Epoch 483 - lr: 0.00329 - Train loss: 1.47817 - Test loss: 1.53519\n",
      "Epoch 484 - lr: 0.00328 - Train loss: 1.47779 - Test loss: 1.53458\n",
      "Epoch 485 - lr: 0.00327 - Train loss: 1.47741 - Test loss: 1.53396\n",
      "Epoch 486 - lr: 0.00327 - Train loss: 1.47703 - Test loss: 1.53335\n",
      "Epoch 487 - lr: 0.00326 - Train loss: 1.47665 - Test loss: 1.53273\n",
      "Epoch 488 - lr: 0.00325 - Train loss: 1.47627 - Test loss: 1.53212\n",
      "Epoch 489 - lr: 0.00324 - Train loss: 1.47589 - Test loss: 1.53151\n",
      "Epoch 490 - lr: 0.00324 - Train loss: 1.47551 - Test loss: 1.53091\n",
      "Epoch 491 - lr: 0.00323 - Train loss: 1.47514 - Test loss: 1.53030\n",
      "Epoch 492 - lr: 0.00322 - Train loss: 1.47476 - Test loss: 1.52970\n",
      "Epoch 493 - lr: 0.00321 - Train loss: 1.47439 - Test loss: 1.52910\n",
      "Epoch 494 - lr: 0.00321 - Train loss: 1.47402 - Test loss: 1.52850\n",
      "Epoch 495 - lr: 0.00320 - Train loss: 1.47365 - Test loss: 1.52791\n",
      "Epoch 496 - lr: 0.00319 - Train loss: 1.47328 - Test loss: 1.52731\n",
      "Epoch 497 - lr: 0.00318 - Train loss: 1.47291 - Test loss: 1.52672\n",
      "Epoch 498 - lr: 0.00318 - Train loss: 1.47254 - Test loss: 1.52613\n",
      "Epoch 499 - lr: 0.00317 - Train loss: 1.47218 - Test loss: 1.52554\n",
      "Epoch 500 - lr: 0.00316 - Train loss: 1.47181 - Test loss: 1.52496\n",
      "Epoch 501 - lr: 0.00316 - Train loss: 1.47145 - Test loss: 1.52437\n",
      "Epoch 502 - lr: 0.00315 - Train loss: 1.47108 - Test loss: 1.52379\n",
      "Epoch 503 - lr: 0.00314 - Train loss: 1.47072 - Test loss: 1.52322\n",
      "Epoch 504 - lr: 0.00313 - Train loss: 1.47036 - Test loss: 1.52264\n",
      "Epoch 505 - lr: 0.00313 - Train loss: 1.47000 - Test loss: 1.52207\n",
      "Epoch 506 - lr: 0.00312 - Train loss: 1.46964 - Test loss: 1.52149\n",
      "Epoch 507 - lr: 0.00311 - Train loss: 1.46928 - Test loss: 1.52092\n",
      "Epoch 508 - lr: 0.00310 - Train loss: 1.46893 - Test loss: 1.52036\n",
      "Epoch 509 - lr: 0.00310 - Train loss: 1.46857 - Test loss: 1.51979\n",
      "Epoch 510 - lr: 0.00309 - Train loss: 1.46822 - Test loss: 1.51923\n",
      "Epoch 511 - lr: 0.00308 - Train loss: 1.46786 - Test loss: 1.51867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 512 - lr: 0.00308 - Train loss: 1.46751 - Test loss: 1.51811\n",
      "Epoch 513 - lr: 0.00307 - Train loss: 1.46716 - Test loss: 1.51755\n",
      "Epoch 514 - lr: 0.00306 - Train loss: 1.46681 - Test loss: 1.51700\n",
      "Epoch 515 - lr: 0.00305 - Train loss: 1.46646 - Test loss: 1.51645\n",
      "Epoch 516 - lr: 0.00305 - Train loss: 1.46612 - Test loss: 1.51597\n",
      "Epoch 517 - lr: 0.00304 - Train loss: 1.46577 - Test loss: 1.51553\n",
      "Epoch 518 - lr: 0.00303 - Train loss: 1.46542 - Test loss: 1.51509\n",
      "Epoch 519 - lr: 0.00303 - Train loss: 1.46507 - Test loss: 1.51465\n",
      "Epoch 520 - lr: 0.00302 - Train loss: 1.46472 - Test loss: 1.51421\n",
      "Epoch 521 - lr: 0.00301 - Train loss: 1.46437 - Test loss: 1.51376\n",
      "Epoch 522 - lr: 0.00301 - Train loss: 1.46401 - Test loss: 1.51331\n",
      "Epoch 523 - lr: 0.00300 - Train loss: 1.46366 - Test loss: 1.51286\n",
      "Epoch 524 - lr: 0.00299 - Train loss: 1.46331 - Test loss: 1.51241\n",
      "Epoch 525 - lr: 0.00299 - Train loss: 1.46296 - Test loss: 1.51195\n",
      "Epoch 526 - lr: 0.00298 - Train loss: 1.46261 - Test loss: 1.51150\n",
      "Epoch 527 - lr: 0.00297 - Train loss: 1.46226 - Test loss: 1.51104\n",
      "Epoch 528 - lr: 0.00296 - Train loss: 1.46191 - Test loss: 1.51059\n",
      "Epoch 529 - lr: 0.00296 - Train loss: 1.46157 - Test loss: 1.51013\n",
      "Epoch 530 - lr: 0.00295 - Train loss: 1.46122 - Test loss: 1.50967\n",
      "Epoch 531 - lr: 0.00294 - Train loss: 1.46088 - Test loss: 1.50921\n",
      "Epoch 532 - lr: 0.00294 - Train loss: 1.46054 - Test loss: 1.50876\n",
      "Epoch 533 - lr: 0.00293 - Train loss: 1.46019 - Test loss: 1.50830\n",
      "Epoch 534 - lr: 0.00292 - Train loss: 1.45986 - Test loss: 1.50784\n",
      "Epoch 535 - lr: 0.00292 - Train loss: 1.45952 - Test loss: 1.50738\n",
      "Epoch 536 - lr: 0.00291 - Train loss: 1.45918 - Test loss: 1.50692\n",
      "Epoch 537 - lr: 0.00290 - Train loss: 1.45885 - Test loss: 1.50645\n",
      "Epoch 538 - lr: 0.00290 - Train loss: 1.45851 - Test loss: 1.50599\n",
      "Epoch 539 - lr: 0.00289 - Train loss: 1.45818 - Test loss: 1.50553\n",
      "Epoch 540 - lr: 0.00288 - Train loss: 1.45785 - Test loss: 1.50507\n",
      "Epoch 541 - lr: 0.00288 - Train loss: 1.45752 - Test loss: 1.50461\n",
      "Epoch 542 - lr: 0.00287 - Train loss: 1.45720 - Test loss: 1.50415\n",
      "Epoch 543 - lr: 0.00286 - Train loss: 1.45687 - Test loss: 1.50368\n",
      "Epoch 544 - lr: 0.00286 - Train loss: 1.45654 - Test loss: 1.50322\n",
      "Epoch 545 - lr: 0.00285 - Train loss: 1.45622 - Test loss: 1.50276\n",
      "Epoch 546 - lr: 0.00284 - Train loss: 1.45590 - Test loss: 1.50229\n",
      "Epoch 547 - lr: 0.00284 - Train loss: 1.45558 - Test loss: 1.50183\n",
      "Epoch 548 - lr: 0.00283 - Train loss: 1.45526 - Test loss: 1.50137\n",
      "Epoch 549 - lr: 0.00282 - Train loss: 1.45494 - Test loss: 1.50090\n",
      "Epoch 550 - lr: 0.00282 - Train loss: 1.45462 - Test loss: 1.50044\n",
      "Epoch 551 - lr: 0.00281 - Train loss: 1.45431 - Test loss: 1.49997\n",
      "Epoch 552 - lr: 0.00281 - Train loss: 1.45399 - Test loss: 1.49951\n",
      "Epoch 553 - lr: 0.00280 - Train loss: 1.45368 - Test loss: 1.49905\n",
      "Epoch 554 - lr: 0.00279 - Train loss: 1.45337 - Test loss: 1.49858\n",
      "Epoch 555 - lr: 0.00279 - Train loss: 1.45306 - Test loss: 1.49812\n",
      "Epoch 556 - lr: 0.00278 - Train loss: 1.45275 - Test loss: 1.49765\n",
      "Epoch 557 - lr: 0.00277 - Train loss: 1.45244 - Test loss: 1.49719\n",
      "Epoch 558 - lr: 0.00277 - Train loss: 1.45214 - Test loss: 1.49673\n",
      "Epoch 559 - lr: 0.00276 - Train loss: 1.45183 - Test loss: 1.49626\n",
      "Epoch 560 - lr: 0.00275 - Train loss: 1.45153 - Test loss: 1.49580\n",
      "Epoch 561 - lr: 0.00275 - Train loss: 1.45123 - Test loss: 1.49534\n",
      "Epoch 562 - lr: 0.00274 - Train loss: 1.45093 - Test loss: 1.49487\n",
      "Epoch 563 - lr: 0.00274 - Train loss: 1.45063 - Test loss: 1.49441\n",
      "Epoch 564 - lr: 0.00273 - Train loss: 1.45033 - Test loss: 1.49394\n",
      "Epoch 565 - lr: 0.00272 - Train loss: 1.45003 - Test loss: 1.49348\n",
      "Epoch 566 - lr: 0.00272 - Train loss: 1.44973 - Test loss: 1.49302\n",
      "Epoch 567 - lr: 0.00271 - Train loss: 1.44944 - Test loss: 1.49256\n",
      "Epoch 568 - lr: 0.00270 - Train loss: 1.44914 - Test loss: 1.49209\n",
      "Epoch 569 - lr: 0.00270 - Train loss: 1.44885 - Test loss: 1.49163\n",
      "Epoch 570 - lr: 0.00269 - Train loss: 1.44856 - Test loss: 1.49117\n",
      "Epoch 571 - lr: 0.00269 - Train loss: 1.44827 - Test loss: 1.49071\n",
      "Epoch 572 - lr: 0.00268 - Train loss: 1.44798 - Test loss: 1.49025\n",
      "Epoch 573 - lr: 0.00267 - Train loss: 1.44769 - Test loss: 1.48979\n",
      "Epoch 574 - lr: 0.00267 - Train loss: 1.44740 - Test loss: 1.48932\n",
      "Epoch 575 - lr: 0.00266 - Train loss: 1.44712 - Test loss: 1.48886\n",
      "Epoch 576 - lr: 0.00265 - Train loss: 1.44683 - Test loss: 1.48840\n",
      "Epoch 577 - lr: 0.00265 - Train loss: 1.44655 - Test loss: 1.48794\n",
      "Epoch 578 - lr: 0.00264 - Train loss: 1.44627 - Test loss: 1.48749\n",
      "Epoch 579 - lr: 0.00264 - Train loss: 1.44599 - Test loss: 1.48703\n",
      "Epoch 580 - lr: 0.00263 - Train loss: 1.44571 - Test loss: 1.48657\n",
      "Epoch 581 - lr: 0.00262 - Train loss: 1.44543 - Test loss: 1.48611\n",
      "Epoch 582 - lr: 0.00262 - Train loss: 1.44515 - Test loss: 1.48565\n",
      "Epoch 583 - lr: 0.00261 - Train loss: 1.44487 - Test loss: 1.48520\n",
      "Epoch 584 - lr: 0.00261 - Train loss: 1.44459 - Test loss: 1.48474\n",
      "Epoch 585 - lr: 0.00260 - Train loss: 1.44432 - Test loss: 1.48428\n",
      "Epoch 586 - lr: 0.00259 - Train loss: 1.44405 - Test loss: 1.48383\n",
      "Epoch 587 - lr: 0.00259 - Train loss: 1.44377 - Test loss: 1.48337\n",
      "Epoch 588 - lr: 0.00258 - Train loss: 1.44350 - Test loss: 1.48292\n",
      "Epoch 589 - lr: 0.00258 - Train loss: 1.44323 - Test loss: 1.48246\n",
      "Epoch 590 - lr: 0.00257 - Train loss: 1.44296 - Test loss: 1.48201\n",
      "Epoch 591 - lr: 0.00256 - Train loss: 1.44269 - Test loss: 1.48156\n",
      "Epoch 592 - lr: 0.00256 - Train loss: 1.44242 - Test loss: 1.48110\n",
      "Epoch 593 - lr: 0.00255 - Train loss: 1.44216 - Test loss: 1.48065\n",
      "Epoch 594 - lr: 0.00255 - Train loss: 1.44189 - Test loss: 1.48020\n",
      "Epoch 595 - lr: 0.00254 - Train loss: 1.44163 - Test loss: 1.47975\n",
      "Epoch 596 - lr: 0.00254 - Train loss: 1.44136 - Test loss: 1.47930\n",
      "Epoch 597 - lr: 0.00253 - Train loss: 1.44110 - Test loss: 1.47885\n",
      "Epoch 598 - lr: 0.00252 - Train loss: 1.44084 - Test loss: 1.47840\n",
      "Epoch 599 - lr: 0.00252 - Train loss: 1.44058 - Test loss: 1.47796\n",
      "Epoch 600 - lr: 0.00251 - Train loss: 1.44032 - Test loss: 1.47751\n",
      "Epoch 601 - lr: 0.00251 - Train loss: 1.44006 - Test loss: 1.47706\n",
      "Epoch 602 - lr: 0.00250 - Train loss: 1.43980 - Test loss: 1.47662\n",
      "Epoch 603 - lr: 0.00249 - Train loss: 1.43954 - Test loss: 1.47617\n",
      "Epoch 604 - lr: 0.00249 - Train loss: 1.43929 - Test loss: 1.47573\n",
      "Epoch 605 - lr: 0.00248 - Train loss: 1.43903 - Test loss: 1.47528\n",
      "Epoch 606 - lr: 0.00248 - Train loss: 1.43878 - Test loss: 1.47484\n",
      "Epoch 607 - lr: 0.00247 - Train loss: 1.43852 - Test loss: 1.47440\n",
      "Epoch 608 - lr: 0.00247 - Train loss: 1.43827 - Test loss: 1.47396\n",
      "Epoch 609 - lr: 0.00246 - Train loss: 1.43802 - Test loss: 1.47352\n",
      "Epoch 610 - lr: 0.00245 - Train loss: 1.43777 - Test loss: 1.47308\n",
      "Epoch 611 - lr: 0.00245 - Train loss: 1.43752 - Test loss: 1.47264\n",
      "Epoch 612 - lr: 0.00244 - Train loss: 1.43727 - Test loss: 1.47220\n",
      "Epoch 613 - lr: 0.00244 - Train loss: 1.43702 - Test loss: 1.47176\n",
      "Epoch 614 - lr: 0.00243 - Train loss: 1.43677 - Test loss: 1.47133\n",
      "Epoch 615 - lr: 0.00243 - Train loss: 1.43653 - Test loss: 1.47089\n",
      "Epoch 616 - lr: 0.00242 - Train loss: 1.43628 - Test loss: 1.47046\n",
      "Epoch 617 - lr: 0.00242 - Train loss: 1.43604 - Test loss: 1.47002\n",
      "Epoch 618 - lr: 0.00241 - Train loss: 1.43579 - Test loss: 1.46959\n",
      "Epoch 619 - lr: 0.00240 - Train loss: 1.43555 - Test loss: 1.46916\n",
      "Epoch 620 - lr: 0.00240 - Train loss: 1.43531 - Test loss: 1.46873\n",
      "Epoch 621 - lr: 0.00239 - Train loss: 1.43507 - Test loss: 1.46830\n",
      "Epoch 622 - lr: 0.00239 - Train loss: 1.43483 - Test loss: 1.46787\n",
      "Epoch 623 - lr: 0.00238 - Train loss: 1.43459 - Test loss: 1.46744\n",
      "Epoch 624 - lr: 0.00238 - Train loss: 1.43435 - Test loss: 1.46701\n",
      "Epoch 625 - lr: 0.00237 - Train loss: 1.43411 - Test loss: 1.46659\n",
      "Epoch 626 - lr: 0.00237 - Train loss: 1.43387 - Test loss: 1.46616\n",
      "Epoch 627 - lr: 0.00236 - Train loss: 1.43364 - Test loss: 1.46574\n",
      "Epoch 628 - lr: 0.00236 - Train loss: 1.43340 - Test loss: 1.46531\n",
      "Epoch 629 - lr: 0.00235 - Train loss: 1.43317 - Test loss: 1.46489\n",
      "Epoch 630 - lr: 0.00234 - Train loss: 1.43293 - Test loss: 1.46447\n",
      "Epoch 631 - lr: 0.00234 - Train loss: 1.43270 - Test loss: 1.46405\n",
      "Epoch 632 - lr: 0.00233 - Train loss: 1.43247 - Test loss: 1.46363\n",
      "Epoch 633 - lr: 0.00233 - Train loss: 1.43224 - Test loss: 1.46321\n",
      "Epoch 634 - lr: 0.00232 - Train loss: 1.43200 - Test loss: 1.46279\n",
      "Epoch 635 - lr: 0.00232 - Train loss: 1.43177 - Test loss: 1.46237\n",
      "Epoch 636 - lr: 0.00231 - Train loss: 1.43153 - Test loss: 1.46195\n",
      "Epoch 637 - lr: 0.00231 - Train loss: 1.43130 - Test loss: 1.46152\n",
      "Epoch 638 - lr: 0.00230 - Train loss: 1.43106 - Test loss: 1.46110\n",
      "Epoch 639 - lr: 0.00230 - Train loss: 1.43083 - Test loss: 1.46068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 640 - lr: 0.00229 - Train loss: 1.43060 - Test loss: 1.46026\n",
      "Epoch 641 - lr: 0.00229 - Train loss: 1.43036 - Test loss: 1.45984\n",
      "Epoch 642 - lr: 0.00228 - Train loss: 1.43013 - Test loss: 1.45943\n",
      "Epoch 643 - lr: 0.00228 - Train loss: 1.42990 - Test loss: 1.45901\n",
      "Epoch 644 - lr: 0.00227 - Train loss: 1.42967 - Test loss: 1.45859\n",
      "Epoch 645 - lr: 0.00226 - Train loss: 1.42944 - Test loss: 1.45818\n",
      "Epoch 646 - lr: 0.00226 - Train loss: 1.42921 - Test loss: 1.45777\n",
      "Epoch 647 - lr: 0.00225 - Train loss: 1.42898 - Test loss: 1.45735\n",
      "Epoch 648 - lr: 0.00225 - Train loss: 1.42875 - Test loss: 1.45694\n",
      "Epoch 649 - lr: 0.00224 - Train loss: 1.42853 - Test loss: 1.45653\n",
      "Epoch 650 - lr: 0.00224 - Train loss: 1.42830 - Test loss: 1.45612\n",
      "Epoch 651 - lr: 0.00223 - Train loss: 1.42807 - Test loss: 1.45572\n",
      "Epoch 652 - lr: 0.00223 - Train loss: 1.42785 - Test loss: 1.45531\n",
      "Epoch 653 - lr: 0.00222 - Train loss: 1.42762 - Test loss: 1.45490\n",
      "Epoch 654 - lr: 0.00222 - Train loss: 1.42740 - Test loss: 1.45450\n",
      "Epoch 655 - lr: 0.00221 - Train loss: 1.42718 - Test loss: 1.45409\n",
      "Epoch 656 - lr: 0.00221 - Train loss: 1.42696 - Test loss: 1.45369\n",
      "Epoch 657 - lr: 0.00220 - Train loss: 1.42673 - Test loss: 1.45329\n",
      "Epoch 658 - lr: 0.00220 - Train loss: 1.42651 - Test loss: 1.45289\n",
      "Epoch 659 - lr: 0.00219 - Train loss: 1.42629 - Test loss: 1.45249\n",
      "Epoch 660 - lr: 0.00219 - Train loss: 1.42607 - Test loss: 1.45209\n",
      "Epoch 661 - lr: 0.00218 - Train loss: 1.42585 - Test loss: 1.45170\n",
      "Epoch 662 - lr: 0.00218 - Train loss: 1.42563 - Test loss: 1.45130\n",
      "Epoch 663 - lr: 0.00217 - Train loss: 1.42542 - Test loss: 1.45091\n",
      "Epoch 664 - lr: 0.00217 - Train loss: 1.42520 - Test loss: 1.45051\n",
      "Epoch 665 - lr: 0.00216 - Train loss: 1.42498 - Test loss: 1.45012\n",
      "Epoch 666 - lr: 0.00216 - Train loss: 1.42477 - Test loss: 1.44973\n",
      "Epoch 667 - lr: 0.00215 - Train loss: 1.42455 - Test loss: 1.44934\n",
      "Epoch 668 - lr: 0.00215 - Train loss: 1.42434 - Test loss: 1.44895\n",
      "Epoch 669 - lr: 0.00214 - Train loss: 1.42412 - Test loss: 1.44856\n",
      "Epoch 670 - lr: 0.00214 - Train loss: 1.42391 - Test loss: 1.44817\n",
      "Epoch 671 - lr: 0.00213 - Train loss: 1.42370 - Test loss: 1.44779\n",
      "Epoch 672 - lr: 0.00213 - Train loss: 1.42349 - Test loss: 1.44741\n",
      "Epoch 673 - lr: 0.00212 - Train loss: 1.42327 - Test loss: 1.44702\n",
      "Epoch 674 - lr: 0.00212 - Train loss: 1.42306 - Test loss: 1.44664\n",
      "Epoch 675 - lr: 0.00211 - Train loss: 1.42285 - Test loss: 1.44626\n",
      "Epoch 676 - lr: 0.00211 - Train loss: 1.42264 - Test loss: 1.44588\n",
      "Epoch 677 - lr: 0.00210 - Train loss: 1.42243 - Test loss: 1.44550\n",
      "Epoch 678 - lr: 0.00210 - Train loss: 1.42222 - Test loss: 1.44512\n",
      "Epoch 679 - lr: 0.00209 - Train loss: 1.42202 - Test loss: 1.44475\n",
      "Epoch 680 - lr: 0.00209 - Train loss: 1.42181 - Test loss: 1.44437\n",
      "Epoch 681 - lr: 0.00208 - Train loss: 1.42160 - Test loss: 1.44400\n",
      "Epoch 682 - lr: 0.00208 - Train loss: 1.42140 - Test loss: 1.44363\n",
      "Epoch 683 - lr: 0.00207 - Train loss: 1.42119 - Test loss: 1.44326\n",
      "Epoch 684 - lr: 0.00207 - Train loss: 1.42098 - Test loss: 1.44289\n",
      "Epoch 685 - lr: 0.00207 - Train loss: 1.42078 - Test loss: 1.44252\n",
      "Epoch 686 - lr: 0.00206 - Train loss: 1.42058 - Test loss: 1.44215\n",
      "Epoch 687 - lr: 0.00206 - Train loss: 1.42037 - Test loss: 1.44179\n",
      "Epoch 688 - lr: 0.00205 - Train loss: 1.42017 - Test loss: 1.44142\n",
      "Epoch 689 - lr: 0.00205 - Train loss: 1.41997 - Test loss: 1.44106\n",
      "Epoch 690 - lr: 0.00204 - Train loss: 1.41976 - Test loss: 1.44070\n",
      "Epoch 691 - lr: 0.00204 - Train loss: 1.41956 - Test loss: 1.44033\n",
      "Epoch 692 - lr: 0.00203 - Train loss: 1.41936 - Test loss: 1.43997\n",
      "Epoch 693 - lr: 0.00203 - Train loss: 1.41916 - Test loss: 1.43962\n",
      "Epoch 694 - lr: 0.00202 - Train loss: 1.41896 - Test loss: 1.43926\n",
      "Epoch 695 - lr: 0.00202 - Train loss: 1.41876 - Test loss: 1.43890\n",
      "Epoch 696 - lr: 0.00201 - Train loss: 1.41856 - Test loss: 1.43853\n",
      "Epoch 697 - lr: 0.00201 - Train loss: 1.41837 - Test loss: 1.43817\n",
      "Epoch 698 - lr: 0.00200 - Train loss: 1.41817 - Test loss: 1.43781\n",
      "Epoch 699 - lr: 0.00200 - Train loss: 1.41797 - Test loss: 1.43744\n",
      "Epoch 700 - lr: 0.00200 - Train loss: 1.41777 - Test loss: 1.43708\n",
      "Epoch 701 - lr: 0.00199 - Train loss: 1.41758 - Test loss: 1.43672\n",
      "Epoch 702 - lr: 0.00199 - Train loss: 1.41738 - Test loss: 1.43636\n",
      "Epoch 703 - lr: 0.00198 - Train loss: 1.41718 - Test loss: 1.43600\n",
      "Epoch 704 - lr: 0.00198 - Train loss: 1.41699 - Test loss: 1.43565\n",
      "Epoch 705 - lr: 0.00197 - Train loss: 1.41679 - Test loss: 1.43529\n",
      "Epoch 706 - lr: 0.00197 - Train loss: 1.41660 - Test loss: 1.43494\n",
      "Epoch 707 - lr: 0.00196 - Train loss: 1.41640 - Test loss: 1.43458\n",
      "Epoch 708 - lr: 0.00196 - Train loss: 1.41621 - Test loss: 1.43423\n",
      "Epoch 709 - lr: 0.00195 - Train loss: 1.41602 - Test loss: 1.43388\n",
      "Epoch 710 - lr: 0.00195 - Train loss: 1.41582 - Test loss: 1.43353\n",
      "Epoch 711 - lr: 0.00195 - Train loss: 1.41563 - Test loss: 1.43318\n",
      "Epoch 712 - lr: 0.00194 - Train loss: 1.41544 - Test loss: 1.43283\n",
      "Epoch 713 - lr: 0.00194 - Train loss: 1.41525 - Test loss: 1.43249\n",
      "Epoch 714 - lr: 0.00193 - Train loss: 1.41506 - Test loss: 1.43214\n",
      "Epoch 715 - lr: 0.00193 - Train loss: 1.41487 - Test loss: 1.43180\n",
      "Epoch 716 - lr: 0.00192 - Train loss: 1.41467 - Test loss: 1.43146\n",
      "Epoch 717 - lr: 0.00192 - Train loss: 1.41448 - Test loss: 1.43112\n",
      "Epoch 718 - lr: 0.00191 - Train loss: 1.41430 - Test loss: 1.43078\n",
      "Epoch 719 - lr: 0.00191 - Train loss: 1.41411 - Test loss: 1.43044\n",
      "Epoch 720 - lr: 0.00191 - Train loss: 1.41392 - Test loss: 1.43010\n",
      "Epoch 721 - lr: 0.00190 - Train loss: 1.41373 - Test loss: 1.42976\n",
      "Epoch 722 - lr: 0.00190 - Train loss: 1.41354 - Test loss: 1.42943\n",
      "Epoch 723 - lr: 0.00189 - Train loss: 1.41335 - Test loss: 1.42910\n",
      "Epoch 724 - lr: 0.00189 - Train loss: 1.41317 - Test loss: 1.42876\n",
      "Epoch 725 - lr: 0.00188 - Train loss: 1.41298 - Test loss: 1.42843\n",
      "Epoch 726 - lr: 0.00188 - Train loss: 1.41279 - Test loss: 1.42810\n",
      "Epoch 727 - lr: 0.00187 - Train loss: 1.41261 - Test loss: 1.42778\n",
      "Epoch 728 - lr: 0.00187 - Train loss: 1.41242 - Test loss: 1.42745\n",
      "Epoch 729 - lr: 0.00187 - Train loss: 1.41224 - Test loss: 1.42712\n",
      "Epoch 730 - lr: 0.00186 - Train loss: 1.41205 - Test loss: 1.42680\n",
      "Epoch 731 - lr: 0.00186 - Train loss: 1.41187 - Test loss: 1.42647\n",
      "Epoch 732 - lr: 0.00185 - Train loss: 1.41168 - Test loss: 1.42615\n",
      "Epoch 733 - lr: 0.00185 - Train loss: 1.41150 - Test loss: 1.42583\n",
      "Epoch 734 - lr: 0.00185 - Train loss: 1.41132 - Test loss: 1.42551\n",
      "Epoch 735 - lr: 0.00184 - Train loss: 1.41114 - Test loss: 1.42519\n",
      "Epoch 736 - lr: 0.00184 - Train loss: 1.41095 - Test loss: 1.42488\n",
      "Epoch 737 - lr: 0.00183 - Train loss: 1.41077 - Test loss: 1.42456\n",
      "Epoch 738 - lr: 0.00183 - Train loss: 1.41059 - Test loss: 1.42424\n",
      "Epoch 739 - lr: 0.00182 - Train loss: 1.41041 - Test loss: 1.42393\n",
      "Epoch 740 - lr: 0.00182 - Train loss: 1.41023 - Test loss: 1.42362\n",
      "Epoch 741 - lr: 0.00182 - Train loss: 1.41005 - Test loss: 1.42331\n",
      "Epoch 742 - lr: 0.00181 - Train loss: 1.40987 - Test loss: 1.42300\n",
      "Epoch 743 - lr: 0.00181 - Train loss: 1.40969 - Test loss: 1.42269\n",
      "Epoch 744 - lr: 0.00180 - Train loss: 1.40951 - Test loss: 1.42238\n",
      "Epoch 745 - lr: 0.00180 - Train loss: 1.40933 - Test loss: 1.42208\n",
      "Epoch 746 - lr: 0.00179 - Train loss: 1.40915 - Test loss: 1.42177\n",
      "Epoch 747 - lr: 0.00179 - Train loss: 1.40897 - Test loss: 1.42147\n",
      "Epoch 748 - lr: 0.00179 - Train loss: 1.40880 - Test loss: 1.42117\n",
      "Epoch 749 - lr: 0.00178 - Train loss: 1.40862 - Test loss: 1.42087\n",
      "Epoch 750 - lr: 0.00178 - Train loss: 1.40844 - Test loss: 1.42057\n",
      "Epoch 751 - lr: 0.00177 - Train loss: 1.40826 - Test loss: 1.42027\n",
      "Epoch 752 - lr: 0.00177 - Train loss: 1.40809 - Test loss: 1.41997\n",
      "Epoch 753 - lr: 0.00177 - Train loss: 1.40791 - Test loss: 1.41967\n",
      "Epoch 754 - lr: 0.00176 - Train loss: 1.40774 - Test loss: 1.41938\n",
      "Epoch 755 - lr: 0.00176 - Train loss: 1.40756 - Test loss: 1.41909\n",
      "Epoch 756 - lr: 0.00175 - Train loss: 1.40739 - Test loss: 1.41879\n",
      "Epoch 757 - lr: 0.00175 - Train loss: 1.40721 - Test loss: 1.41850\n",
      "Epoch 758 - lr: 0.00175 - Train loss: 1.40704 - Test loss: 1.41821\n",
      "Epoch 759 - lr: 0.00174 - Train loss: 1.40686 - Test loss: 1.41792\n",
      "Epoch 760 - lr: 0.00174 - Train loss: 1.40669 - Test loss: 1.41764\n",
      "Epoch 761 - lr: 0.00173 - Train loss: 1.40651 - Test loss: 1.41735\n",
      "Epoch 762 - lr: 0.00173 - Train loss: 1.40634 - Test loss: 1.41707\n",
      "Epoch 763 - lr: 0.00173 - Train loss: 1.40617 - Test loss: 1.41678\n",
      "Epoch 764 - lr: 0.00172 - Train loss: 1.40600 - Test loss: 1.41650\n",
      "Epoch 765 - lr: 0.00172 - Train loss: 1.40582 - Test loss: 1.41622\n",
      "Epoch 766 - lr: 0.00171 - Train loss: 1.40565 - Test loss: 1.41594\n",
      "Epoch 767 - lr: 0.00171 - Train loss: 1.40548 - Test loss: 1.41566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 768 - lr: 0.00171 - Train loss: 1.40531 - Test loss: 1.41538\n",
      "Epoch 769 - lr: 0.00170 - Train loss: 1.40514 - Test loss: 1.41511\n",
      "Epoch 770 - lr: 0.00170 - Train loss: 1.40497 - Test loss: 1.41483\n",
      "Epoch 771 - lr: 0.00169 - Train loss: 1.40480 - Test loss: 1.41456\n",
      "Epoch 772 - lr: 0.00169 - Train loss: 1.40463 - Test loss: 1.41428\n",
      "Epoch 773 - lr: 0.00169 - Train loss: 1.40446 - Test loss: 1.41401\n",
      "Epoch 774 - lr: 0.00168 - Train loss: 1.40429 - Test loss: 1.41374\n",
      "Epoch 775 - lr: 0.00168 - Train loss: 1.40412 - Test loss: 1.41347\n",
      "Epoch 776 - lr: 0.00167 - Train loss: 1.40395 - Test loss: 1.41320\n",
      "Epoch 777 - lr: 0.00167 - Train loss: 1.40379 - Test loss: 1.41294\n",
      "Epoch 778 - lr: 0.00167 - Train loss: 1.40362 - Test loss: 1.41267\n",
      "Epoch 779 - lr: 0.00166 - Train loss: 1.40345 - Test loss: 1.41241\n",
      "Epoch 780 - lr: 0.00166 - Train loss: 1.40328 - Test loss: 1.41214\n",
      "Epoch 781 - lr: 0.00166 - Train loss: 1.40312 - Test loss: 1.41188\n",
      "Epoch 782 - lr: 0.00165 - Train loss: 1.40295 - Test loss: 1.41162\n",
      "Epoch 783 - lr: 0.00165 - Train loss: 1.40278 - Test loss: 1.41136\n",
      "Epoch 784 - lr: 0.00164 - Train loss: 1.40262 - Test loss: 1.41110\n",
      "Epoch 785 - lr: 0.00164 - Train loss: 1.40245 - Test loss: 1.41084\n",
      "Epoch 786 - lr: 0.00164 - Train loss: 1.40228 - Test loss: 1.41059\n",
      "Epoch 787 - lr: 0.00163 - Train loss: 1.40212 - Test loss: 1.41033\n",
      "Epoch 788 - lr: 0.00163 - Train loss: 1.40195 - Test loss: 1.41008\n",
      "Epoch 789 - lr: 0.00163 - Train loss: 1.40179 - Test loss: 1.40983\n",
      "Epoch 790 - lr: 0.00162 - Train loss: 1.40162 - Test loss: 1.40957\n",
      "Epoch 791 - lr: 0.00162 - Train loss: 1.40146 - Test loss: 1.40932\n",
      "Epoch 792 - lr: 0.00161 - Train loss: 1.40130 - Test loss: 1.40907\n",
      "Epoch 793 - lr: 0.00161 - Train loss: 1.40113 - Test loss: 1.40883\n",
      "Epoch 794 - lr: 0.00161 - Train loss: 1.40097 - Test loss: 1.40858\n",
      "Epoch 795 - lr: 0.00160 - Train loss: 1.40081 - Test loss: 1.40833\n",
      "Epoch 796 - lr: 0.00160 - Train loss: 1.40064 - Test loss: 1.40809\n",
      "Epoch 797 - lr: 0.00160 - Train loss: 1.40048 - Test loss: 1.40784\n",
      "Epoch 798 - lr: 0.00159 - Train loss: 1.40032 - Test loss: 1.40760\n",
      "Epoch 799 - lr: 0.00159 - Train loss: 1.40016 - Test loss: 1.40736\n",
      "Epoch 800 - lr: 0.00158 - Train loss: 1.40000 - Test loss: 1.40712\n",
      "Epoch 801 - lr: 0.00158 - Train loss: 1.39983 - Test loss: 1.40688\n",
      "Epoch 802 - lr: 0.00158 - Train loss: 1.39967 - Test loss: 1.40664\n",
      "Epoch 803 - lr: 0.00157 - Train loss: 1.39951 - Test loss: 1.40641\n",
      "Epoch 804 - lr: 0.00157 - Train loss: 1.39935 - Test loss: 1.40617\n",
      "Epoch 805 - lr: 0.00157 - Train loss: 1.39919 - Test loss: 1.40593\n",
      "Epoch 806 - lr: 0.00156 - Train loss: 1.39903 - Test loss: 1.40570\n",
      "Epoch 807 - lr: 0.00156 - Train loss: 1.39887 - Test loss: 1.40547\n",
      "Epoch 808 - lr: 0.00156 - Train loss: 1.39871 - Test loss: 1.40524\n",
      "Epoch 809 - lr: 0.00155 - Train loss: 1.39855 - Test loss: 1.40501\n",
      "Epoch 810 - lr: 0.00155 - Train loss: 1.39839 - Test loss: 1.40478\n",
      "Epoch 811 - lr: 0.00155 - Train loss: 1.39823 - Test loss: 1.40455\n",
      "Epoch 812 - lr: 0.00154 - Train loss: 1.39808 - Test loss: 1.40432\n",
      "Epoch 813 - lr: 0.00154 - Train loss: 1.39792 - Test loss: 1.40410\n",
      "Epoch 814 - lr: 0.00153 - Train loss: 1.39776 - Test loss: 1.40387\n",
      "Epoch 815 - lr: 0.00153 - Train loss: 1.39760 - Test loss: 1.40365\n",
      "Epoch 816 - lr: 0.00153 - Train loss: 1.39744 - Test loss: 1.40342\n",
      "Epoch 817 - lr: 0.00152 - Train loss: 1.39729 - Test loss: 1.40320\n",
      "Epoch 818 - lr: 0.00152 - Train loss: 1.39713 - Test loss: 1.40298\n",
      "Epoch 819 - lr: 0.00152 - Train loss: 1.39697 - Test loss: 1.40276\n",
      "Epoch 820 - lr: 0.00151 - Train loss: 1.39682 - Test loss: 1.40254\n",
      "Epoch 821 - lr: 0.00151 - Train loss: 1.39666 - Test loss: 1.40232\n",
      "Epoch 822 - lr: 0.00151 - Train loss: 1.39650 - Test loss: 1.40211\n",
      "Epoch 823 - lr: 0.00150 - Train loss: 1.39635 - Test loss: 1.40189\n",
      "Epoch 824 - lr: 0.00150 - Train loss: 1.39619 - Test loss: 1.40168\n",
      "Epoch 825 - lr: 0.00150 - Train loss: 1.39604 - Test loss: 1.40147\n",
      "Epoch 826 - lr: 0.00149 - Train loss: 1.39588 - Test loss: 1.40125\n",
      "Epoch 827 - lr: 0.00149 - Train loss: 1.39573 - Test loss: 1.40104\n",
      "Epoch 828 - lr: 0.00149 - Train loss: 1.39557 - Test loss: 1.40083\n",
      "Epoch 829 - lr: 0.00148 - Train loss: 1.39542 - Test loss: 1.40062\n",
      "Epoch 830 - lr: 0.00148 - Train loss: 1.39527 - Test loss: 1.40041\n",
      "Epoch 831 - lr: 0.00148 - Train loss: 1.39511 - Test loss: 1.40021\n",
      "Epoch 832 - lr: 0.00147 - Train loss: 1.39496 - Test loss: 1.40000\n",
      "Epoch 833 - lr: 0.00147 - Train loss: 1.39480 - Test loss: 1.39979\n",
      "Epoch 834 - lr: 0.00147 - Train loss: 1.39465 - Test loss: 1.39959\n",
      "Epoch 835 - lr: 0.00146 - Train loss: 1.39450 - Test loss: 1.39939\n",
      "Epoch 836 - lr: 0.00146 - Train loss: 1.39435 - Test loss: 1.39919\n",
      "Epoch 837 - lr: 0.00146 - Train loss: 1.39419 - Test loss: 1.39898\n",
      "Epoch 838 - lr: 0.00145 - Train loss: 1.39404 - Test loss: 1.39878\n",
      "Epoch 839 - lr: 0.00145 - Train loss: 1.39389 - Test loss: 1.39858\n",
      "Epoch 840 - lr: 0.00145 - Train loss: 1.39374 - Test loss: 1.39839\n",
      "Epoch 841 - lr: 0.00144 - Train loss: 1.39359 - Test loss: 1.39819\n",
      "Epoch 842 - lr: 0.00144 - Train loss: 1.39343 - Test loss: 1.39799\n",
      "Epoch 843 - lr: 0.00144 - Train loss: 1.39328 - Test loss: 1.39780\n",
      "Epoch 844 - lr: 0.00143 - Train loss: 1.39313 - Test loss: 1.39760\n",
      "Epoch 845 - lr: 0.00143 - Train loss: 1.39298 - Test loss: 1.39741\n",
      "Epoch 846 - lr: 0.00143 - Train loss: 1.39283 - Test loss: 1.39722\n",
      "Epoch 847 - lr: 0.00142 - Train loss: 1.39268 - Test loss: 1.39703\n",
      "Epoch 848 - lr: 0.00142 - Train loss: 1.39253 - Test loss: 1.39684\n",
      "Epoch 849 - lr: 0.00142 - Train loss: 1.39238 - Test loss: 1.39665\n",
      "Epoch 850 - lr: 0.00141 - Train loss: 1.39223 - Test loss: 1.39646\n",
      "Epoch 851 - lr: 0.00141 - Train loss: 1.39208 - Test loss: 1.39627\n",
      "Epoch 852 - lr: 0.00141 - Train loss: 1.39193 - Test loss: 1.39608\n",
      "Epoch 853 - lr: 0.00140 - Train loss: 1.39178 - Test loss: 1.39590\n",
      "Epoch 854 - lr: 0.00140 - Train loss: 1.39164 - Test loss: 1.39571\n",
      "Epoch 855 - lr: 0.00140 - Train loss: 1.39149 - Test loss: 1.39553\n",
      "Epoch 856 - lr: 0.00139 - Train loss: 1.39134 - Test loss: 1.39535\n",
      "Epoch 857 - lr: 0.00139 - Train loss: 1.39119 - Test loss: 1.39516\n",
      "Epoch 858 - lr: 0.00139 - Train loss: 1.39104 - Test loss: 1.39498\n",
      "Epoch 859 - lr: 0.00138 - Train loss: 1.39090 - Test loss: 1.39480\n",
      "Epoch 860 - lr: 0.00138 - Train loss: 1.39075 - Test loss: 1.39462\n",
      "Epoch 861 - lr: 0.00138 - Train loss: 1.39060 - Test loss: 1.39445\n",
      "Epoch 862 - lr: 0.00137 - Train loss: 1.39045 - Test loss: 1.39427\n",
      "Epoch 863 - lr: 0.00137 - Train loss: 1.39031 - Test loss: 1.39409\n",
      "Epoch 864 - lr: 0.00137 - Train loss: 1.39016 - Test loss: 1.39392\n",
      "Epoch 865 - lr: 0.00136 - Train loss: 1.39001 - Test loss: 1.39374\n",
      "Epoch 866 - lr: 0.00136 - Train loss: 1.38987 - Test loss: 1.39357\n",
      "Epoch 867 - lr: 0.00136 - Train loss: 1.38972 - Test loss: 1.39340\n",
      "Epoch 868 - lr: 0.00136 - Train loss: 1.38958 - Test loss: 1.39322\n",
      "Epoch 869 - lr: 0.00135 - Train loss: 1.38943 - Test loss: 1.39305\n",
      "Epoch 870 - lr: 0.00135 - Train loss: 1.38929 - Test loss: 1.39288\n",
      "Epoch 871 - lr: 0.00135 - Train loss: 1.38914 - Test loss: 1.39271\n",
      "Epoch 872 - lr: 0.00134 - Train loss: 1.38900 - Test loss: 1.39255\n",
      "Epoch 873 - lr: 0.00134 - Train loss: 1.38886 - Test loss: 1.39239\n",
      "Epoch 874 - lr: 0.00134 - Train loss: 1.38872 - Test loss: 1.39224\n",
      "Epoch 875 - lr: 0.00133 - Train loss: 1.38858 - Test loss: 1.39208\n",
      "Epoch 876 - lr: 0.00133 - Train loss: 1.38844 - Test loss: 1.39193\n",
      "Epoch 877 - lr: 0.00133 - Train loss: 1.38830 - Test loss: 1.39177\n",
      "Epoch 878 - lr: 0.00132 - Train loss: 1.38816 - Test loss: 1.39162\n",
      "Epoch 879 - lr: 0.00132 - Train loss: 1.38802 - Test loss: 1.39146\n",
      "Epoch 880 - lr: 0.00132 - Train loss: 1.38788 - Test loss: 1.39131\n",
      "Epoch 881 - lr: 0.00132 - Train loss: 1.38774 - Test loss: 1.39116\n",
      "Epoch 882 - lr: 0.00131 - Train loss: 1.38760 - Test loss: 1.39101\n",
      "Epoch 883 - lr: 0.00131 - Train loss: 1.38746 - Test loss: 1.39086\n",
      "Epoch 884 - lr: 0.00131 - Train loss: 1.38733 - Test loss: 1.39071\n",
      "Epoch 885 - lr: 0.00130 - Train loss: 1.38719 - Test loss: 1.39057\n",
      "Epoch 886 - lr: 0.00130 - Train loss: 1.38705 - Test loss: 1.39042\n",
      "Epoch 887 - lr: 0.00130 - Train loss: 1.38691 - Test loss: 1.39027\n",
      "Epoch 888 - lr: 0.00129 - Train loss: 1.38677 - Test loss: 1.39013\n",
      "Epoch 889 - lr: 0.00129 - Train loss: 1.38663 - Test loss: 1.38999\n",
      "Epoch 890 - lr: 0.00129 - Train loss: 1.38650 - Test loss: 1.38984\n",
      "Epoch 891 - lr: 0.00129 - Train loss: 1.38636 - Test loss: 1.38970\n",
      "Epoch 892 - lr: 0.00128 - Train loss: 1.38622 - Test loss: 1.38956\n",
      "Epoch 893 - lr: 0.00128 - Train loss: 1.38608 - Test loss: 1.38942\n",
      "Epoch 894 - lr: 0.00128 - Train loss: 1.38595 - Test loss: 1.38928\n",
      "Epoch 895 - lr: 0.00127 - Train loss: 1.38581 - Test loss: 1.38914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 896 - lr: 0.00127 - Train loss: 1.38567 - Test loss: 1.38900\n",
      "Epoch 897 - lr: 0.00127 - Train loss: 1.38554 - Test loss: 1.38887\n",
      "Epoch 898 - lr: 0.00126 - Train loss: 1.38540 - Test loss: 1.38873\n",
      "Epoch 899 - lr: 0.00126 - Train loss: 1.38527 - Test loss: 1.38860\n",
      "Epoch 900 - lr: 0.00126 - Train loss: 1.38513 - Test loss: 1.38846\n",
      "Epoch 901 - lr: 0.00126 - Train loss: 1.38499 - Test loss: 1.38833\n",
      "Epoch 902 - lr: 0.00125 - Train loss: 1.38486 - Test loss: 1.38820\n",
      "Epoch 903 - lr: 0.00125 - Train loss: 1.38472 - Test loss: 1.38807\n",
      "Epoch 904 - lr: 0.00125 - Train loss: 1.38459 - Test loss: 1.38794\n",
      "Epoch 905 - lr: 0.00124 - Train loss: 1.38445 - Test loss: 1.38781\n",
      "Epoch 906 - lr: 0.00124 - Train loss: 1.38432 - Test loss: 1.38768\n",
      "Epoch 907 - lr: 0.00124 - Train loss: 1.38418 - Test loss: 1.38755\n",
      "Epoch 908 - lr: 0.00124 - Train loss: 1.38405 - Test loss: 1.38742\n",
      "Epoch 909 - lr: 0.00123 - Train loss: 1.38391 - Test loss: 1.38730\n",
      "Epoch 910 - lr: 0.00123 - Train loss: 1.38378 - Test loss: 1.38717\n",
      "Epoch 911 - lr: 0.00123 - Train loss: 1.38365 - Test loss: 1.38705\n",
      "Epoch 912 - lr: 0.00122 - Train loss: 1.38351 - Test loss: 1.38692\n",
      "Epoch 913 - lr: 0.00122 - Train loss: 1.38338 - Test loss: 1.38680\n",
      "Epoch 914 - lr: 0.00122 - Train loss: 1.38325 - Test loss: 1.38668\n",
      "Epoch 915 - lr: 0.00122 - Train loss: 1.38311 - Test loss: 1.38656\n",
      "Epoch 916 - lr: 0.00121 - Train loss: 1.38298 - Test loss: 1.38644\n",
      "Epoch 917 - lr: 0.00121 - Train loss: 1.38285 - Test loss: 1.38632\n",
      "Epoch 918 - lr: 0.00121 - Train loss: 1.38271 - Test loss: 1.38620\n",
      "Epoch 919 - lr: 0.00121 - Train loss: 1.38258 - Test loss: 1.38608\n",
      "Epoch 920 - lr: 0.00120 - Train loss: 1.38245 - Test loss: 1.38596\n",
      "Epoch 921 - lr: 0.00120 - Train loss: 1.38232 - Test loss: 1.38585\n",
      "Epoch 922 - lr: 0.00120 - Train loss: 1.38218 - Test loss: 1.38573\n",
      "Epoch 923 - lr: 0.00119 - Train loss: 1.38205 - Test loss: 1.38562\n",
      "Epoch 924 - lr: 0.00119 - Train loss: 1.38192 - Test loss: 1.38550\n",
      "Epoch 925 - lr: 0.00119 - Train loss: 1.38179 - Test loss: 1.38539\n",
      "Epoch 926 - lr: 0.00119 - Train loss: 1.38166 - Test loss: 1.38528\n",
      "Epoch 927 - lr: 0.00118 - Train loss: 1.38153 - Test loss: 1.38517\n",
      "Epoch 928 - lr: 0.00118 - Train loss: 1.38139 - Test loss: 1.38505\n",
      "Epoch 929 - lr: 0.00118 - Train loss: 1.38126 - Test loss: 1.38494\n",
      "Epoch 930 - lr: 0.00117 - Train loss: 1.38113 - Test loss: 1.38482\n",
      "Epoch 931 - lr: 0.00117 - Train loss: 1.38099 - Test loss: 1.38471\n",
      "Epoch 932 - lr: 0.00117 - Train loss: 1.38086 - Test loss: 1.38459\n",
      "Epoch 933 - lr: 0.00117 - Train loss: 1.38073 - Test loss: 1.38448\n",
      "Epoch 934 - lr: 0.00116 - Train loss: 1.38059 - Test loss: 1.38437\n",
      "Epoch 935 - lr: 0.00116 - Train loss: 1.38046 - Test loss: 1.38426\n",
      "Epoch 936 - lr: 0.00116 - Train loss: 1.38033 - Test loss: 1.38415\n",
      "Epoch 937 - lr: 0.00116 - Train loss: 1.38020 - Test loss: 1.38404\n",
      "Epoch 938 - lr: 0.00115 - Train loss: 1.38007 - Test loss: 1.38394\n",
      "Epoch 939 - lr: 0.00115 - Train loss: 1.37994 - Test loss: 1.38383\n",
      "Epoch 940 - lr: 0.00115 - Train loss: 1.37981 - Test loss: 1.38372\n",
      "Epoch 941 - lr: 0.00115 - Train loss: 1.37967 - Test loss: 1.38362\n",
      "Epoch 942 - lr: 0.00114 - Train loss: 1.37954 - Test loss: 1.38351\n",
      "Epoch 943 - lr: 0.00114 - Train loss: 1.37941 - Test loss: 1.38341\n",
      "Epoch 944 - lr: 0.00114 - Train loss: 1.37928 - Test loss: 1.38330\n",
      "Epoch 945 - lr: 0.00114 - Train loss: 1.37915 - Test loss: 1.38320\n",
      "Epoch 946 - lr: 0.00113 - Train loss: 1.37903 - Test loss: 1.38310\n",
      "Epoch 947 - lr: 0.00113 - Train loss: 1.37890 - Test loss: 1.38300\n",
      "Epoch 948 - lr: 0.00113 - Train loss: 1.37877 - Test loss: 1.38290\n",
      "Epoch 949 - lr: 0.00112 - Train loss: 1.37864 - Test loss: 1.38279\n",
      "Epoch 950 - lr: 0.00112 - Train loss: 1.37851 - Test loss: 1.38270\n",
      "Epoch 951 - lr: 0.00112 - Train loss: 1.37838 - Test loss: 1.38260\n",
      "Epoch 952 - lr: 0.00112 - Train loss: 1.37825 - Test loss: 1.38250\n",
      "Epoch 953 - lr: 0.00111 - Train loss: 1.37812 - Test loss: 1.38240\n",
      "Epoch 954 - lr: 0.00111 - Train loss: 1.37800 - Test loss: 1.38230\n",
      "Epoch 955 - lr: 0.00111 - Train loss: 1.37787 - Test loss: 1.38221\n",
      "Epoch 956 - lr: 0.00111 - Train loss: 1.37774 - Test loss: 1.38211\n",
      "Epoch 957 - lr: 0.00110 - Train loss: 1.37761 - Test loss: 1.38202\n",
      "Epoch 958 - lr: 0.00110 - Train loss: 1.37749 - Test loss: 1.38192\n",
      "Epoch 959 - lr: 0.00110 - Train loss: 1.37736 - Test loss: 1.38183\n",
      "Epoch 960 - lr: 0.00110 - Train loss: 1.37723 - Test loss: 1.38174\n",
      "Epoch 961 - lr: 0.00109 - Train loss: 1.37710 - Test loss: 1.38164\n",
      "Epoch 962 - lr: 0.00109 - Train loss: 1.37698 - Test loss: 1.38155\n",
      "Epoch 963 - lr: 0.00109 - Train loss: 1.37685 - Test loss: 1.38146\n",
      "Epoch 964 - lr: 0.00109 - Train loss: 1.37672 - Test loss: 1.38137\n",
      "Epoch 965 - lr: 0.00108 - Train loss: 1.37660 - Test loss: 1.38128\n",
      "Epoch 966 - lr: 0.00108 - Train loss: 1.37647 - Test loss: 1.38119\n",
      "Epoch 967 - lr: 0.00108 - Train loss: 1.37635 - Test loss: 1.38110\n",
      "Epoch 968 - lr: 0.00108 - Train loss: 1.37622 - Test loss: 1.38101\n",
      "Epoch 969 - lr: 0.00107 - Train loss: 1.37610 - Test loss: 1.38092\n",
      "Epoch 970 - lr: 0.00107 - Train loss: 1.37597 - Test loss: 1.38084\n",
      "Epoch 971 - lr: 0.00107 - Train loss: 1.37585 - Test loss: 1.38075\n",
      "Epoch 972 - lr: 0.00107 - Train loss: 1.37572 - Test loss: 1.38066\n",
      "Epoch 973 - lr: 0.00106 - Train loss: 1.37560 - Test loss: 1.38058\n",
      "Epoch 974 - lr: 0.00106 - Train loss: 1.37547 - Test loss: 1.38049\n",
      "Epoch 975 - lr: 0.00106 - Train loss: 1.37535 - Test loss: 1.38041\n",
      "Epoch 976 - lr: 0.00106 - Train loss: 1.37522 - Test loss: 1.38033\n",
      "Epoch 977 - lr: 0.00105 - Train loss: 1.37510 - Test loss: 1.38024\n",
      "Epoch 978 - lr: 0.00105 - Train loss: 1.37497 - Test loss: 1.38016\n",
      "Epoch 979 - lr: 0.00105 - Train loss: 1.37485 - Test loss: 1.38008\n",
      "Epoch 980 - lr: 0.00105 - Train loss: 1.37473 - Test loss: 1.38000\n",
      "Epoch 981 - lr: 0.00104 - Train loss: 1.37460 - Test loss: 1.37992\n",
      "Epoch 982 - lr: 0.00104 - Train loss: 1.37448 - Test loss: 1.37984\n",
      "Epoch 983 - lr: 0.00104 - Train loss: 1.37436 - Test loss: 1.37976\n",
      "Epoch 984 - lr: 0.00104 - Train loss: 1.37423 - Test loss: 1.37968\n",
      "Epoch 985 - lr: 0.00104 - Train loss: 1.37411 - Test loss: 1.37960\n",
      "Epoch 986 - lr: 0.00103 - Train loss: 1.37399 - Test loss: 1.37952\n",
      "Epoch 987 - lr: 0.00103 - Train loss: 1.37387 - Test loss: 1.37944\n",
      "Epoch 988 - lr: 0.00103 - Train loss: 1.37374 - Test loss: 1.37934\n",
      "Epoch 989 - lr: 0.00103 - Train loss: 1.37362 - Test loss: 1.37923\n",
      "Epoch 990 - lr: 0.00102 - Train loss: 1.37350 - Test loss: 1.37912\n",
      "Epoch 991 - lr: 0.00102 - Train loss: 1.37337 - Test loss: 1.37902\n",
      "Epoch 992 - lr: 0.00102 - Train loss: 1.37325 - Test loss: 1.37891\n",
      "Epoch 993 - lr: 0.00102 - Train loss: 1.37313 - Test loss: 1.37881\n",
      "Epoch 994 - lr: 0.00101 - Train loss: 1.37302 - Test loss: 1.37870\n",
      "Epoch 995 - lr: 0.00101 - Train loss: 1.37290 - Test loss: 1.37860\n",
      "Epoch 996 - lr: 0.00101 - Train loss: 1.37278 - Test loss: 1.37850\n",
      "Epoch 997 - lr: 0.00101 - Train loss: 1.37266 - Test loss: 1.37840\n",
      "Epoch 998 - lr: 0.00100 - Train loss: 1.37255 - Test loss: 1.37830\n",
      "Epoch 999 - lr: 0.00100 - Train loss: 1.37243 - Test loss: 1.37820\n",
      "Epoch 1000 - lr: 0.00100 - Train loss: 1.37231 - Test loss: 1.37810\n"
     ]
    }
   ],
   "source": [
    "### PARAMETERS\n",
    "Ni = 1   # Number of inputs\n",
    "Nh1 = 50 # Number of hidden neurons (layer 1) \n",
    "Nh2 = 50 # Number of hidden neurons (layer 2)\n",
    "No = 1   # Number of outputs\n",
    "L1_lambda = 0.01\n",
    "\n",
    "### Initialize network\n",
    "net = Network(Ni, Nh1, Nh2, No, 'leaky_ReLU', L1_lambda)\n",
    "\n",
    "# Train the network by calling its method\n",
    "train_loss_log, test_loss_log = net.train(x_train, y_train, x_test, y_test, \n",
    "                                          num_epochs = 1000,\n",
    "                                          lr = 0.01, \n",
    "                                          en_decay = True,\n",
    "                                          lr_final = 0.001, \n",
    "                                          return_log = True,\n",
    "                                          out_log = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we print the results of the trained network. Take in consideration that the test error can be lower than the train error because for the last one we add the L1 regularization term to the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot losses\n",
    "plt.close('all')\n",
    "plt.semilogy(train_loss_log, label='Train loss')\n",
    "plt.semilogy(test_loss_log, label='Test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Plot estimated function\n",
    "x_highres = np.linspace(-4,2,6000)\n",
    "net_output = np.array([net.forward(x) for x in x_highres])\n",
    "\n",
    "plt.close('all')\n",
    "plt.plot(x_train, y_train, color='r', ls='', marker='.', label='Train data points')\n",
    "plt.plot(x_test, y_test, color='b', ls='', marker='.', label='Test data points')\n",
    "plt.plot(x_highres, net_output, color='g', ls='--', label='Network output (trained weights)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation implementation\n",
    "Now we implement a cross validation training, where we split our training dataset into 4 subset and we use each one of these as a validation set (so we train the network 4 times). The training and validation will be averaged between all the iteration of the cross validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to execute K-fold Cross Validation:\n",
    "def Kfold_Cross_Validation (x_train, y_train, Ni, Nh1, Nh2, No, act_func, L1_lambda, \n",
    "                            num_epochs, lr, en_decay, lr_final, log=False, K=4):\n",
    "\n",
    "    # initialize average validation error to 0\n",
    "    avg_val_error = 0\n",
    "    \n",
    "    # Use each fold as a validation set for the network trained with the other K-1\n",
    "    for i in range(K):\n",
    "        if log:\n",
    "            print(\"\\nFold\",i,\"out of\",K,\":\")\n",
    "\n",
    "        # Split dataset into Train and Validation (changes at each iteration) sets\n",
    "        indexes_of_val_set = np.arange( i*(len(x_train)/K), (i+1)*(len(x_train)/K), dtype=int)\n",
    "        x_val = x_train[ indexes_of_val_set ]\n",
    "        x_train_tmp = np.delete(x_train, indexes_of_val_set)\n",
    "        y_val = y_train[ indexes_of_val_set ]\n",
    "        y_train_tmp = np.delete(y_train, indexes_of_val_set)\n",
    "\n",
    "        # Re-initialize netowrk to reset its weights\n",
    "        net = Network(Ni, Nh1, Nh2, No, act_func, L1_lambda)\n",
    "\n",
    "        # Train the model and test it with the validation set\n",
    "        final_train_loss, final_val_loss = net.train(x_train_tmp, y_train_tmp, x_val, y_val, \n",
    "                                                     num_epochs, lr, en_decay, lr_final)\n",
    "        # update average validation error\n",
    "        avg_val_error += final_val_loss/K\n",
    "        \n",
    "        # Print final train/validation error of the trained network if requested\n",
    "        if log:\n",
    "            print(\"Train loss      : \", final_train_loss)\n",
    "            print(\"Validation loss : \", final_val_loss)  \n",
    "\n",
    "    return avg_val_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 out of 4 :\n",
      "Train loss      :  0.9718413075230501\n",
      "Validation loss :  0.8159908030924053\n",
      "\n",
      "Fold 1 out of 4 :\n",
      "Train loss      :  0.9534584841083129\n",
      "Validation loss :  1.1005304394004656\n",
      "\n",
      "Fold 2 out of 4 :\n",
      "Train loss      :  1.160142927930249\n",
      "Validation loss :  0.8298786353421616\n",
      "\n",
      "Fold 3 out of 4 :\n",
      "Train loss      :  0.9238537146552133\n",
      "Validation loss :  1.757704348872274\n",
      "\n",
      "Average validation error: 1.1260260566768265\n"
     ]
    }
   ],
   "source": [
    "# Shuffle dataset before splitting it\n",
    "random_order = np.arange(len(x_train))\n",
    "np.random.shuffle(random_order)\n",
    "x_train = x_train[random_order]\n",
    "y_train = y_train[random_order]\n",
    "\n",
    "# Perform Cross Validation and compute average error on validation sets\n",
    "avg_val_error = Kfold_Cross_Validation(x_train, y_train, Ni, Nh1, Nh2, No, 'ReLU', L1_lambda,\n",
    "                                       num_epochs=1000, lr=0.05, en_decay=True, lr_final=0.001, log=True)\n",
    "\n",
    "# Print Average Validation error\n",
    "print(\"\\nAverage validation error:\",avg_val_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Implementation\n",
    "With this method it is possible to find the best combination of hyperparameters of the network and its training schedule. We will train and test the network using all possible combination between:\n",
    "\n",
    " - **Network Structure**: Define the number of neurons every layers of the network, obviously the input and output layers have to contain only one neuron;\n",
    " - **Activation Function**: Activation function used for every neuron of the network (apart from the output neuron). The  activation functions that we will compare are Sigmoid, ReLU, Leaky_ReLU;\n",
    " - **L1 lambda parameter**: Lambda parameter used in L1 regularizaion;\n",
    " - **Learning rate and its decay**: Learning rate schedule is defined by a tuple of 3 variables (initial_lr, lr_decay, final_lr). If we want a constant lr, the lr_decay must be `False` and the final lr can be set to any value (for simplicity it was set to zero in these cases).\n",
    " \n",
    "First, we have to build a dictionary that contains every possible value of each hyper-parameter that we want to test. Then, we use the `itertools.product` function to compute all possible combinations of these values. Each combination is saved in a tuple and each tuple is saved in a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Old version\n",
    "In the beginning, the dictionary described above was built in this way:\n",
    "\n",
    "```python\n",
    "params_dict = {'Net_struct':[(1,10,10,1), (1,25,10,1), (1,25,25,1), (1,50,25,1), (1,50,50,1), \n",
    "                             (1,75,50,1), (1,75,75,1), (1,100,75,1), (1,100,100,1)],\n",
    "               'Act_func':  ['expit', 'ReLU', 'leaky_ReLU'],\n",
    "               'L1_par':    [0, 0.001, 0.01, 0.1],\n",
    "               'n_epochs':  [1000],\n",
    "               'lr_policy': [(0.05,False,0), \n",
    "                             (0.01,False,0), \n",
    "                             (0.1, True, 0.001), \n",
    "                             (0.05,True, 0.001), \n",
    "                             (0.01,True, 0.001), \n",
    "                             (0.01,True, 0.0001)]\n",
    "              }\n",
    "```\n",
    "Those values were not choosen with a systematic method, I choosed some values that just seemed right. So it was unexpected to find some anomalies in the Grid Search results:\n",
    "\n",
    "```\n",
    "Best combinations are:\n",
    "(1, 75,  50,  1, 'ReLU',       0,     1000, 0.05, True, 0.001)  with score  0.6013646932830741\n",
    "(1, 25,  10,  1, 'leaky_ReLU', 0,     1000, 0.05, True, 0.001)  with score  0.6087147749624167\n",
    "(1, 75,  75,  1, 'ReLU',       0,     1000, 0.05, True, 0.001)  with score  0.6090597201388589\n",
    "(1, 50,  50,  1, 'leaky_ReLU', 0,     1000, 0.05, True, 0.001)  with score  0.6102277031919986\n",
    "(1, 100, 100, 1, 'ReLU',       0,     1000, 0.05, True, 0.001)  with score  0.6111253060568337\n",
    "(1, 100, 75,  1, 'ReLU',       0,     1000, 0.05, True, 0.001)  with score  0.6117814700240817\n",
    "(1, 100, 75,  1, 'leaky_ReLU', 0,     1000, 0.05, True, 0.001)  with score  0.6147644224884894\n",
    "(1, 25,  25,  1, 'leaky_ReLU', 0,     1000, 0.05, True, 0.001)  with score  0.6176147630817669\n",
    "(1, 100, 75,  1, 'ReLU',       0.001, 1000, 0.1,  True, 0.001)  with score  0.6206305893760173\n",
    "(1, 25,  25,  1, 'ReLU',       0,     1000, 0.05, True, 0.001)  with score  0.6226920805785986\n",
    "(1, 25,  10,  1, 'ReLU',       0,     1000, 0.05, True, 0.001)  with score  0.6235306755057399\n",
    "(1, 75,  75,  1, 'leaky_ReLU', 0,     1000, 0.05, True, 0.001)  with score  0.6238183097745367\n",
    "(1, 100, 100, 1, 'ReLU',       0,     1000, 0.1,  True, 0.001)  with score  0.6261058395468648\n",
    "(1, 75,  50,  1, 'leaky_ReLU', 0,     1000, 0.05, True, 0.001)  with score  0.6261716255503684\n",
    "(1, 50,  25,  1, 'leaky_ReLU', 0,     1000, 0.05, True, 0.001)  with score  0.6278317042975219\n",
    "(1, 75,  50,  1, 'ReLU',       0.001, 1000, 0.05, True, 0.001)  with score  0.6303288347332184\n",
    "(1, 50,  25,  1, 'ReLU',       0.001, 1000, 0.05, True, 0.001)  with score  0.6306405849711662\n",
    "(1, 100, 100, 1, 'leaky_ReLU', 0,     1000, 0.05, True, 0.001)  with score  0.632037896990721\n",
    "(1, 50,  50,  1, 'ReLU',       0,     1000, 0.05, True, 0.001)  with score  0.6328486970530985\n",
    "(1, 25,  25,  1, 'ReLU',       0.001, 1000, 0.05, True, 0.001)  with score  0.6333584056356556\n",
    "```\n",
    "\n",
    "From these results, we can make some important considerations:\n",
    " - The possible values of `L1_par`are too high, so most of the best models are not implementing any kind of regularization (i.e. `L1_par` = 0) \n",
    " - Every best model uses Learning rate decay, and the best initial value seems to be near 0.05, so we should search around it to find the best value\n",
    " \n",
    "So we repeated the Grid Search process after changing the possible `L1_par` values and the learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 432 combinations of params\n",
      "\n",
      "For example, the first combination of values will be:\n",
      " (1, 10, 10, 1, 'expit', 0, 1000, 0.1, True, 0.001)\n"
     ]
    }
   ],
   "source": [
    "# Define dictionary containing all the values of the params that I want to test\n",
    "params_dict = {'Net_struct':[(1,10,10,1), (1,25,10,1), (1,25,25,1), (1,50,25,1), (1,50,50,1), \n",
    "                             (1,75,50,1), (1,75,75,1), (1,100,75,1), (1,100,100,1)],\n",
    "               'Act_func':  ['expit', 'ReLU', 'leaky_ReLU'],\n",
    "               'L1_par':    [0, 0.00001, 0.0001, 0.001],\n",
    "               'n_epochs':  [1000],\n",
    "               'lr_policy': [(0.1, True, 0.001), \n",
    "                             (0.075, True, 0.001), \n",
    "                             (0.05,True, 0.001), \n",
    "                             (0.025, True, 0.001)]\n",
    "              }\n",
    "\n",
    "# Compute all combinations of dict elements\n",
    "import itertools as it\n",
    "combinations = it.product(*(params_dict[Name] for Name in params_dict.keys()))\n",
    "\n",
    "# Since some params are defined by tuples, we have to flatten each\n",
    "# combination to use its values as argumnts for Cross_Validation func\n",
    "def flatten(T):\n",
    "    if (isinstance(T,tuple)==False): return (T,)\n",
    "    elif len(T) == 0: return ()\n",
    "    else: return flatten(T[0]) + flatten(T[1:])\n",
    "\n",
    "# Compute a list of all possible combinations flattening the elements \n",
    "# of the combination found few rows above\n",
    "params_combinations = [flatten(params) for params in list(combinations)]\n",
    "print(\"There are\", len(params_combinations), \"combinations of params\")\n",
    "print(\"\\nFor example, the first combination of values will be:\\n\", params_combinations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `Kfold_Cross_Validation` function to test each combination of parameters by measuring the network average error on validation sets when initialized and trained following these values.\n",
    "\n",
    "During the Grid Search process, we want to keep track of the average validation error (that we will call also as 'score' for simplcity) of each combination of parameters. After finishing this process, the scores will be sorted from best to worst, and the combinations of parameters will be ordered accordingly.\n",
    "\n",
    "The function `Grid_Search` defined in the next cell computes the entire process and returns the two ordered sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grid_Search(x_train, y_train, params_combinations):\n",
    "\n",
    "    # Initialize array to save all scores\n",
    "    scores = np.zeros(len(params_combinations))\n",
    "    \n",
    "    # Execute Cross Validation for each combination of parameters\n",
    "    for comb_index in range(len(params_combinations)):\n",
    "        \n",
    "        print(\"\\nPerforming training\", comb_index, \"out of\", len(params_combinations))\n",
    "        params = params_combinations[comb_index]\n",
    "        \n",
    "        # Perform compute the average validation error using these params\n",
    "        avg_valid_loss = Kfold_Cross_Validation(x_train, y_train, *params)\n",
    "        \n",
    "        # Save score of the current params\n",
    "        scores[comb_index] = avg_valid_loss\n",
    "        \n",
    "        # print simple log\n",
    "        print(\"\\nConfiguration:\",params)\n",
    "        print(\"Gives a score of\", avg_valid_loss)\n",
    "\n",
    "    # Sort scores from best to worst and the combinations accordingly\n",
    "    sorted_scores, sorted_combs = zip(*sorted(zip(scores, params_combinations)))\n",
    "        \n",
    "    # return the best 'n_best_combs' combinations (and their scores)\n",
    "    return sorted_combs, sorted_scores                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start process we simply have to call the function, which takes as arguments the traning set and the list of all possible combinations:\n",
    "\n",
    "```python\n",
    "best_combs, best_scores = Grid_Search(x_train, y_train, params_combinations)\n",
    "```\n",
    "Since we have a lot of combinations, initializing and training sequentially one network at a time for the Grid Search process can take around a day to finish. One way to speed up the task is to parallelize it by splitting the work between all the cores available in the machine (in my case there are 4 cores). The easiest way that I found to make use of all of my cores is through the `multiprocessing` library. Its `Pool` function creates an object representing the four threads that I want to use, and the method `starmap` of these objects allows to execute for each element of a list a function that takes it as argument. The `itertools.repeat` defines constant arguments that will be passed at each call of the function.\n",
    "\n",
    "The `Grid_Search_single_iteration` defined below can be seen as a wrapper of the `Kfold_Cross_Validation` function that takes a combination of parameters and pass them in the correct format.\n",
    "\n",
    "We then sort all scores and combinations like before and we save them in to separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Grid Search process\n",
      "\n",
      "Training combination number 0 out of 432\n",
      "(1, 10, 10, 1, 'expit', 0, 1000, 0.1, True, 0.001)  scored  1.1083920995873426\n",
      "\n",
      "Training combination number 54 out of 432\n",
      "(1, 25, 10, 1, 'expit', 1e-05, 1000, 0.05, True, 0.001)  scored  0.6992427687566944\n",
      "\n",
      "Training combination number 27 out of 432\n",
      "(1, 10, 10, 1, 'ReLU', 0.0001, 1000, 0.025, True, 0.001)  scored  0.6604154235067495\n",
      "\n",
      "Training combination number 81 out of 432\n",
      "(1, 25, 10, 1, 'leaky_ReLU', 0, 1000, 0.075, True, 0.001)  scored  0.6550039006725958\n",
      "\n",
      "Training combination number 1 out of 432\n",
      "(1, 10, 10, 1, 'expit', 0, 1000, 0.075, True, 0.001)  scored  0.7023095832773661\n",
      "\n",
      "Training combination number 55 out of 432\n",
      "(1, 25, 10, 1, 'expit', 1e-05, 1000, 0.025, True, 0.001)  scored  0.7480253452568987\n",
      "\n",
      "Training combination number 28 out of 432\n",
      "(1, 10, 10, 1, 'ReLU', 0.001, 1000, 0.1, True, 0.001)  scored  0.7278425856879315\n",
      "\n",
      "Training combination number 82 out of 432\n",
      "(1, 25, 10, 1, 'leaky_ReLU', 0, 1000, 0.05, True, 0.001)  scored  0.6200042806400925\n",
      "\n",
      "Training combination number 2 out of 432\n",
      "(1, 10, 10, 1, 'expit', 0, 1000, 0.05, True, 0.001)  scored  0.7055742139055684\n",
      "\n",
      "Training combination number 56 out of 432\n",
      "(1, 25, 10, 1, 'expit', 0.0001, 1000, 0.1, True, 0.001)  scored  0.6966671071233077\n",
      "\n",
      "Training combination number 29 out of 432\n",
      "(1, 10, 10, 1, 'ReLU', 0.001, 1000, 0.075, True, 0.001)  scored  0.7233480592874886\n",
      "\n",
      "Training combination number 3 out of 432\n",
      "(1, 10, 10, 1, 'expit', 0, 1000, 0.025, True, 0.001)  scored  0.7388645627538953\n",
      "\n",
      "Training combination number 57 out of 432\n",
      "(1, 25, 10, 1, 'expit', 0.0001, 1000, 0.075, True, 0.001)  scored  0.7079563670582406\n",
      "\n",
      "Training combination number 83 out of 432\n",
      "(1, 25, 10, 1, 'leaky_ReLU', 0, 1000, 0.025, True, 0.001)  scored  0.645133424199507\n",
      "\n",
      "Training combination number 30 out of 432\n",
      "(1, 10, 10, 1, 'ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.6176123945162094\n",
      "\n",
      "Training combination number 4 out of 432\n",
      "(1, 10, 10, 1, 'expit', 1e-05, 1000, 0.1, True, 0.001)  scored  1.3777891242238354\n",
      "\n",
      "Training combination number 58 out of 432\n",
      "(1, 25, 10, 1, 'expit', 0.0001, 1000, 0.05, True, 0.001)  scored  0.7419925055643052\n",
      "\n",
      "Training combination number 84 out of 432\n",
      "(1, 25, 10, 1, 'leaky_ReLU', 1e-05, 1000, 0.1, True, 0.001)  scored  0.6889490839626936\n",
      "\n",
      "Training combination number 5 out of 432\n",
      "(1, 10, 10, 1, 'expit', 1e-05, 1000, 0.075, True, 0.001)  scored  0.6914346775081199\n",
      "\n",
      "Training combination number 31 out of 432\n",
      "(1, 10, 10, 1, 'ReLU', 0.001, 1000, 0.025, True, 0.001)  scored  0.6261433098977658\n",
      "\n",
      "Training combination number 59 out of 432\n",
      "(1, 25, 10, 1, 'expit', 0.0001, 1000, 0.025, True, 0.001)  scored  0.7275821941850327\n",
      "\n",
      "Training combination number 85 out of 432\n",
      "(1, 25, 10, 1, 'leaky_ReLU', 1e-05, 1000, 0.075, True, 0.001)  scored  0.6643831798451408\n",
      "\n",
      "Training combination number 6 out of 432\n",
      "(1, 10, 10, 1, 'expit', 1e-05, 1000, 0.05, True, 0.001)  scored  0.7740609429342845\n",
      "\n",
      "Training combination number 60 out of 432\n",
      "(1, 25, 10, 1, 'expit', 0.001, 1000, 0.1, True, 0.001)  scored  0.7064406286451996\n",
      "\n",
      "Training combination number 32 out of 432\n",
      "(1, 10, 10, 1, 'leaky_ReLU', 0, 1000, 0.1, True, 0.001)  scored  0.6983494755178754\n",
      "\n",
      "Training combination number 7 out of 432\n",
      "(1, 10, 10, 1, 'expit', 1e-05, 1000, 0.025, True, 0.001)  scored  0.7221584202168703\n",
      "\n",
      "Training combination number 61 out of 432\n",
      "(1, 25, 10, 1, 'expit', 0.001, 1000, 0.075, True, 0.001)  scored  0.7141459566841345\n",
      "\n",
      "Training combination number 86 out of 432\n",
      "(1, 25, 10, 1, 'leaky_ReLU', 1e-05, 1000, 0.05, True, 0.001)  scored  0.624524064453469\n",
      "\n",
      "Training combination number 33 out of 432\n",
      "(1, 10, 10, 1, 'leaky_ReLU', 0, 1000, 0.075, True, 0.001)  scored  0.6729761450165712\n",
      "\n",
      "Training combination number 8 out of 432\n",
      "(1, 10, 10, 1, 'expit', 0.0001, 1000, 0.1, True, 0.001)  scored  1.4389712602395295\n",
      "\n",
      "Training combination number 62 out of 432\n",
      "(1, 25, 10, 1, 'expit', 0.001, 1000, 0.05, True, 0.001)  scored  0.6980198712694523\n",
      "\n",
      "Training combination number 87 out of 432\n",
      "(1, 25, 10, 1, 'leaky_ReLU', 1e-05, 1000, 0.025, True, 0.001)  scored  0.6537777890362813\n",
      "\n",
      "Training combination number 9 out of 432\n",
      "(1, 10, 10, 1, 'expit', 0.0001, 1000, 0.075, True, 0.001)  scored  0.727821954090494\n",
      "\n",
      "Training combination number 34 out of 432\n",
      "(1, 10, 10, 1, 'leaky_ReLU', 0, 1000, 0.05, True, 0.001)  scored  0.6320302029840953\n",
      "\n",
      "Training combination number 63 out of 432\n",
      "(1, 25, 10, 1, 'expit', 0.001, 1000, 0.025, True, 0.001)  scored  0.7983117350256049\n",
      "\n",
      "Training combination number 10 out of 432\n",
      "(1, 10, 10, 1, 'expit', 0.0001, 1000, 0.05, True, 0.001)  scored  0.7258312797426845\n",
      "\n",
      "Training combination number 88 out of 432\n",
      "(1, 25, 10, 1, 'leaky_ReLU', 0.0001, 1000, 0.1, True, 0.001)  scored  0.6331535662269637\n",
      "\n",
      "Training combination number 35 out of 432\n",
      "(1, 10, 10, 1, 'leaky_ReLU', 0, 1000, 0.025, True, 0.001)  scored  0.649846066088227\n",
      "\n",
      "Training combination number 64 out of 432\n",
      "(1, 25, 10, 1, 'ReLU', 0, 1000, 0.1, True, 0.001)  scored  0.7491661985231612\n",
      "\n",
      "Training combination number 11 out of 432\n",
      "(1, 10, 10, 1, 'expit', 0.0001, 1000, 0.025, True, 0.001)  scored  0.7251526352728185\n",
      "\n",
      "Training combination number 89 out of 432\n",
      "(1, 25, 10, 1, 'leaky_ReLU', 0.0001, 1000, 0.075, True, 0.001)  scored  0.6628724383362657\n",
      "\n",
      "Training combination number 36 out of 432\n",
      "(1, 10, 10, 1, 'leaky_ReLU', 1e-05, 1000, 0.1, True, 0.001)  scored  0.6629097177610387\n",
      "\n",
      "Training combination number 12 out of 432\n",
      "(1, 10, 10, 1, 'expit', 0.001, 1000, 0.1, True, 0.001)  scored  0.7029713646589145\n",
      "\n",
      "Training combination number 65 out of 432\n",
      "(1, 25, 10, 1, 'ReLU', 0, 1000, 0.075, True, 0.001)  scored  0.9585970943632458\n",
      "\n",
      "Training combination number 37 out of 432\n",
      "(1, 10, 10, 1, 'leaky_ReLU', 1e-05, 1000, 0.075, True, 0.001)  scored  0.6610286316898196\n",
      "\n",
      "Training combination number 13 out of 432\n",
      "(1, 10, 10, 1, 'expit', 0.001, 1000, 0.075, True, 0.001)  scored  0.709864197003975\n",
      "\n",
      "Training combination number 90 out of 432\n",
      "(1, 25, 10, 1, 'leaky_ReLU', 0.0001, 1000, 0.05, True, 0.001)  scored  0.6460135284199232\n",
      "\n",
      "Training combination number 66 out of 432\n",
      "(1, 25, 10, 1, 'ReLU', 0, 1000, 0.05, True, 0.001)  scored  0.7921258776877913\n",
      "\n",
      "Training combination number 14 out of 432\n",
      "(1, 10, 10, 1, 'expit', 0.001, 1000, 0.05, True, 0.001)  scored  0.8412412232603989\n",
      "\n",
      "Training combination number 38 out of 432\n",
      "(1, 10, 10, 1, 'leaky_ReLU', 1e-05, 1000, 0.05, True, 0.001)  scored  0.6269886714057131\n",
      "\n",
      "Training combination number 91 out of 432\n",
      "(1, 25, 10, 1, 'leaky_ReLU', 0.0001, 1000, 0.025, True, 0.001)  scored  0.6551919430777104\n",
      "\n",
      "Training combination number 15 out of 432\n",
      "(1, 10, 10, 1, 'expit', 0.001, 1000, 0.025, True, 0.001)  scored  0.7525013084623711\n",
      "\n",
      "Training combination number 67 out of 432\n",
      "(1, 25, 10, 1, 'ReLU', 0, 1000, 0.025, True, 0.001)  scored  0.821332827089515\n",
      "\n",
      "Training combination number 39 out of 432\n",
      "(1, 10, 10, 1, 'leaky_ReLU', 1e-05, 1000, 0.025, True, 0.001)  scored  0.6953059769405656\n",
      "\n",
      "Training combination number 92 out of 432\n",
      "(1, 25, 10, 1, 'leaky_ReLU', 0.001, 1000, 0.1, True, 0.001)  scored  0.6541083707946495\n",
      "\n",
      "Training combination number 16 out of 432\n",
      "(1, 10, 10, 1, 'ReLU', 0, 1000, 0.1, True, 0.001)  scored  6.2868434878721375\n",
      "\n",
      "Training combination number 68 out of 432\n",
      "(1, 25, 10, 1, 'ReLU', 1e-05, 1000, 0.1, True, 0.001)  scored  0.6520870816702484\n",
      "\n",
      "Training combination number 40 out of 432\n",
      "(1, 10, 10, 1, 'leaky_ReLU', 0.0001, 1000, 0.1, True, 0.001)  scored  0.6754016126591179\n",
      "\n",
      "Training combination number 93 out of 432\n",
      "(1, 25, 10, 1, 'leaky_ReLU', 0.001, 1000, 0.075, True, 0.001)  scored  0.6446361178782611\n",
      "\n",
      "Training combination number 17 out of 432\n",
      "(1, 10, 10, 1, 'ReLU', 0, 1000, 0.075, True, 0.001)  scored  0.6898196508077576\n",
      "\n",
      "Training combination number 41 out of 432\n",
      "(1, 10, 10, 1, 'leaky_ReLU', 0.0001, 1000, 0.075, True, 0.001)  scored  0.6825881793523494\n",
      "\n",
      "Training combination number 69 out of 432\n",
      "(1, 25, 10, 1, 'ReLU', 1e-05, 1000, 0.075, True, 0.001)  scored  0.7913990100029002\n",
      "\n",
      "Training combination number 94 out of 432\n",
      "(1, 25, 10, 1, 'leaky_ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.680004738349078\n",
      "\n",
      "Training combination number 18 out of 432\n",
      "(1, 10, 10, 1, 'ReLU', 0, 1000, 0.05, True, 0.001)  scored  0.6675698447951853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training combination number 42 out of 432\n",
      "(1, 10, 10, 1, 'leaky_ReLU', 0.0001, 1000, 0.05, True, 0.001)  scored  0.6701363702790577\n",
      "\n",
      "Training combination number 70 out of 432\n",
      "(1, 25, 10, 1, 'ReLU', 1e-05, 1000, 0.05, True, 0.001)  scored  0.658764591217029\n",
      "\n",
      "Training combination number 95 out of 432\n",
      "(1, 25, 10, 1, 'leaky_ReLU', 0.001, 1000, 0.025, True, 0.001)  scored  0.8165114938909603\n",
      "\n",
      "Training combination number 19 out of 432\n",
      "(1, 10, 10, 1, 'ReLU', 0, 1000, 0.025, True, 0.001)  scored  0.6686880990526132\n",
      "\n",
      "Training combination number 43 out of 432\n",
      "(1, 10, 10, 1, 'leaky_ReLU', 0.0001, 1000, 0.025, True, 0.001)  scored  0.7362808405704643\n",
      "\n",
      "Training combination number 71 out of 432\n",
      "(1, 25, 10, 1, 'ReLU', 1e-05, 1000, 0.025, True, 0.001)  scored  0.6354991309291584\n",
      "\n",
      "Training combination number 96 out of 432\n",
      "(1, 25, 25, 1, 'expit', 0, 1000, 0.1, True, 0.001)  scored  0.7093426236784005\n",
      "\n",
      "Training combination number 20 out of 432\n",
      "(1, 10, 10, 1, 'ReLU', 1e-05, 1000, 0.1, True, 0.001)  scored  0.8294348074060114\n",
      "\n",
      "Training combination number 44 out of 432\n",
      "(1, 10, 10, 1, 'leaky_ReLU', 0.001, 1000, 0.1, True, 0.001)  scored  0.6560601837644506\n",
      "\n",
      "Training combination number 72 out of 432\n",
      "(1, 25, 10, 1, 'ReLU', 0.0001, 1000, 0.1, True, 0.001)  scored  0.681318456636073\n",
      "\n",
      "Training combination number 97 out of 432\n",
      "(1, 25, 25, 1, 'expit', 0, 1000, 0.075, True, 0.001)  scored  0.6914598248989381\n",
      "\n",
      "Training combination number 21 out of 432\n",
      "(1, 10, 10, 1, 'ReLU', 1e-05, 1000, 0.075, True, 0.001)  scored  3.8784239114636807\n",
      "\n",
      "Training combination number 45 out of 432\n",
      "(1, 10, 10, 1, 'leaky_ReLU', 0.001, 1000, 0.075, True, 0.001)  scored  0.6807217492439288\n",
      "\n",
      "Training combination number 98 out of 432\n",
      "(1, 25, 25, 1, 'expit', 0, 1000, 0.05, True, 0.001)  scored  0.6803592716330484\n",
      "\n",
      "Training combination number 73 out of 432\n",
      "(1, 25, 10, 1, 'ReLU', 0.0001, 1000, 0.075, True, 0.001)  scored  0.688341914703625\n",
      "\n",
      "Training combination number 22 out of 432\n",
      "(1, 10, 10, 1, 'ReLU', 1e-05, 1000, 0.05, True, 0.001)  scored  0.8062747646694002\n",
      "\n",
      "Training combination number 46 out of 432\n",
      "(1, 10, 10, 1, 'leaky_ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.6596092258132058\n",
      "\n",
      "Training combination number 99 out of 432\n",
      "(1, 25, 25, 1, 'expit', 0, 1000, 0.025, True, 0.001)  scored  0.7147190512085109\n",
      "\n",
      "Training combination number 74 out of 432\n",
      "(1, 25, 10, 1, 'ReLU', 0.0001, 1000, 0.05, True, 0.001)  scored  0.6126823233502273\n",
      "\n",
      "Training combination number 23 out of 432\n",
      "(1, 10, 10, 1, 'ReLU', 1e-05, 1000, 0.025, True, 0.001)  scored  0.6312509125452739\n",
      "\n",
      "Training combination number 47 out of 432\n",
      "(1, 10, 10, 1, 'leaky_ReLU', 0.001, 1000, 0.025, True, 0.001)  scored  0.7941330911994839\n",
      "\n",
      "Training combination number 100 out of 432\n",
      "(1, 25, 25, 1, 'expit', 1e-05, 1000, 0.1, True, 0.001)  scored  0.7450948536176024\n",
      "\n",
      "Training combination number 24 out of 432\n",
      "(1, 10, 10, 1, 'ReLU', 0.0001, 1000, 0.1, True, 0.001)  scored  0.7224058582389621\n",
      "\n",
      "Training combination number 75 out of 432\n",
      "(1, 25, 10, 1, 'ReLU', 0.0001, 1000, 0.025, True, 0.001)  scored  0.6615822178658889\n",
      "\n",
      "Training combination number 48 out of 432\n",
      "(1, 25, 10, 1, 'expit', 0, 1000, 0.1, True, 0.001)  scored  0.6731239262240184\n",
      "\n",
      "Training combination number 101 out of 432\n",
      "(1, 25, 25, 1, 'expit', 1e-05, 1000, 0.075, True, 0.001)  scored  0.6849995536390183\n",
      "\n",
      "Training combination number 25 out of 432\n",
      "(1, 10, 10, 1, 'ReLU', 0.0001, 1000, 0.075, True, 0.001)  scored  0.6215762300099299\n",
      "\n",
      "Training combination number 49 out of 432\n",
      "(1, 25, 10, 1, 'expit', 0, 1000, 0.075, True, 0.001)  scored  0.7304681338058221\n",
      "\n",
      "Training combination number 102 out of 432\n",
      "(1, 25, 25, 1, 'expit', 1e-05, 1000, 0.05, True, 0.001)  scored  0.7012719035771611\n",
      "\n",
      "Training combination number 76 out of 432\n",
      "(1, 25, 10, 1, 'ReLU', 0.001, 1000, 0.1, True, 0.001)  scored  0.7591922459913234\n",
      "\n",
      "Training combination number 50 out of 432\n",
      "(1, 25, 10, 1, 'expit', 0, 1000, 0.05, True, 0.001)  scored  0.8082055539601634\n",
      "\n",
      "Training combination number\n",
      "Training combination number 103 out of 432\n",
      "(1, 25, 25, 1, 'expit', 1e-05, 1000, 0.025, True, 0.001)  scored  0.7325882413001972\n",
      " 26 out of 432\n",
      "(1, 10, 10, 1, 'ReLU', 0.0001, 1000, 0.05, True, 0.001)  scored  0.6326848712473524\n",
      "\n",
      "Training combination number 77 out of 432\n",
      "(1, 25, 10, 1, 'ReLU', 0.001, 1000, 0.075, True, 0.001)  scored  0.7463001817749579\n",
      "\n",
      "Training combination number 51 out of 432\n",
      "(1, 25, 10, 1, 'expit', 0, 1000, 0.025, True, 0.001)  scored  0.7449381155824979\n",
      "\n",
      "Training combination number 108 out of 432\n",
      "(1, 25, 25, 1, 'expit', 0.001, 1000, 0.1, True, 0.001)  scored  0.6988336137533295\n",
      "\n",
      "Training combination number 104 out of 432\n",
      "(1, 25, 25, 1, 'expit', 0.0001, 1000, 0.1, True, 0.001)  scored  0.6780036887676821\n",
      "\n",
      "Training combination number 78 out of 432\n",
      "(1, 25, 10, 1, 'ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.63080888915415\n",
      "\n",
      "Training combination number 52 out of 432\n",
      "(1, 25, 10, 1, 'expit', 1e-05, 1000, 0.1, True, 0.001)  scored  1.629593407813689\n",
      "\n",
      "Training combination number 109 out of 432\n",
      "(1, 25, 25, 1, 'expit', 0.001, 1000, 0.075, True, 0.001)  scored  0.7215513028891738\n",
      "\n",
      "Training combination number 105 out of 432\n",
      "(1, 25, 25, 1, 'expit', 0.0001, 1000, 0.075, True, 0.001)  scored  0.6907355096826531\n",
      "\n",
      "Training combination number 53 out of 432\n",
      "(1, 25, 10, 1, 'expit', 1e-05, 1000, 0.075, True, 0.001)  scored  0.7334194932881137\n",
      "\n",
      "Training combination number 79 out of 432\n",
      "(1, 25, 10, 1, 'ReLU', 0.001, 1000, 0.025, True, 0.001)  scored  0.6626711574725369\n",
      "\n",
      "Training combination number 110 out of 432\n",
      "(1, 25, 25, 1, 'expit', 0.001, 1000, 0.05, True, 0.001)  scored  0.7024202326139007\n",
      "\n",
      "Training combination number 106 out of 432\n",
      "(1, 25, 25, 1, 'expit', 0.0001, 1000, 0.05, True, 0.001)  scored  0.6816861021359056\n",
      "\n",
      "Training combination number 111 out of 432\n",
      "(1, 25, 25, 1, 'expit', 0.001, 1000, 0.025, True, 0.001)  scored  0.8212331459106386\n",
      "\n",
      "Training combination number 107 out of 432\n",
      "(1, 25, 25, 1, 'expit', 0.0001, 1000, 0.025, True, 0.001)  scored  0.7116986337507181\n",
      "\n",
      "Training combination number 80 out of 432\n",
      "(1, 25, 10, 1, 'leaky_ReLU', 0, 1000, 0.1, True, 0.001)  scored  0.7633803773031482\n",
      "\n",
      "Training combination number 135 out of 432\n",
      "(1, 25, 25, 1, 'leaky_ReLU', 1e-05, 1000, 0.025, True, 0.001)  scored  0.6646871772262628\n",
      "\n",
      "Training combination number 112 out of 432\n",
      "(1, 25, 25, 1, 'ReLU', 0, 1000, 0.1, True, 0.001)  scored  0.6516182227735793\n",
      "\n",
      "Training combination number 136 out of 432\n",
      "(1, 25, 25, 1, 'leaky_ReLU', 0.0001, 1000, 0.1, True, 0.001)  scored  0.6388643532419971\n",
      "\n",
      "Training combination number 162 out of 432\n",
      "(1, 50, 25, 1, 'ReLU', 0, 1000, 0.05, True, 0.001)  scored  0.6426200576009412\n",
      "\n",
      "Training combination number 189 out of 432\n",
      "(1, 50, 25, 1, 'leaky_ReLU', 0.001, 1000, 0.075, True, 0.001)  scored  0.6411625987760634\n",
      "\n",
      "Training combination number 113 out of 432\n",
      "(1, 25, 25, 1, 'ReLU', 0, 1000, 0.075, True, 0.001)  scored  0.7308858798027024\n",
      "\n",
      "Training combination number 137 out of 432\n",
      "(1, 25, 25, 1, 'leaky_ReLU', 0.0001, 1000, 0.075, True, 0.001)  scored  0.6624538146140755\n",
      "\n",
      "Training combination number 163 out of 432\n",
      "(1, 50, 25, 1, 'ReLU', 0, 1000, 0.025, True, 0.001)  scored  0.6570392506817492\n",
      "\n",
      "Training combination number 190 out of 432\n",
      "(1, 50, 25, 1, 'leaky_ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.6126143570422118\n",
      "\n",
      "Training combination number 114 out of 432\n",
      "(1, 25, 25, 1, 'ReLU', 0, 1000, 0.05, True, 0.001)  scored  0.6337400787052901\n",
      "\n",
      "Training combination number 138 out of 432\n",
      "(1, 25, 25, 1, 'leaky_ReLU', 0.0001, 1000, 0.05, True, 0.001)  scored  0.6460479179080696\n",
      "\n",
      "Training combination number 164 out of 432\n",
      "(1, 50, 25, 1, 'ReLU', 1e-05, 1000, 0.1, True, 0.001)  scored  0.6560920323080552\n",
      "\n",
      "Training combination number 191 out of 432\n",
      "(1, 50, 25, 1, 'leaky_ReLU', 0.001, 1000, 0.025, True, 0.001)  scored  0.8191539546569944\n",
      "\n",
      "Training combination number 115 out of 432\n",
      "(1, 25, 25, 1, 'ReLU', 0, 1000, 0.025, True, 0.001)  scored  0.6866134637056067\n",
      "\n",
      "Training combination number 139 out of 432\n",
      "(1, 25, 25, 1, 'leaky_ReLU', 0.0001, 1000, 0.025, True, 0.001)  scored  0.6273579120444022\n",
      "\n",
      "Training combination number 192 out of 432\n",
      "(1, 50, 50, 1, 'expit', 0, 1000, 0.1, True, 0.001)  scored  1.3939315144836804\n",
      "\n",
      "Training combination number 165 out of 432\n",
      "(1, 50, 25, 1, 'ReLU', 1e-05, 1000, 0.075, True, 0.001)  scored  0.6383351393171126\n",
      "\n",
      "Training combination number 116 out of 432\n",
      "(1, 25, 25, 1, 'ReLU', 1e-05, 1000, 0.1, True, 0.001)  scored  0.6780585089909567\n",
      "\n",
      "Training combination number 140 out of 432\n",
      "(1, 25, 25, 1, 'leaky_ReLU', 0.001, 1000, 0.1, True, 0.001)  scored  0.6472404809780734\n",
      "\n",
      "Training combination number 193 out of 432\n",
      "(1, 50, 50, 1, 'expit', 0, 1000, 0.075, True, 0.001)  scored  1.0355021049820974\n",
      "\n",
      "Training combination number 166 out of 432\n",
      "(1, 50, 25, 1, 'ReLU', 1e-05, 1000, 0.05, True, 0.001)  scored  0.6452082987611198\n",
      "\n",
      "Training combination number 117 out of 432\n",
      "(1, 25, 25, 1, 'ReLU', 1e-05, 1000, 0.075, True, 0.001)  scored  0.7099772720637977\n",
      "\n",
      "Training combination number 194 out of 432\n",
      "(1, 50, 50, 1, 'expit', 0, 1000, 0.05, True, 0.001)  scored  0.7142011129802681\n",
      "\n",
      "Training combination number 141 out of 432\n",
      "(1, 25, 25, 1, 'leaky_ReLU', 0.001, 1000, 0.075, True, 0.001)  scored  0.6462904590432744\n",
      "\n",
      "Training combination number 167 out of 432\n",
      "(1, 50, 25, 1, 'ReLU', 1e-05, 1000, 0.025, True, 0.001)  scored  0.6636888573646034\n",
      "\n",
      "Training combination number 195 out of 432\n",
      "(1, 50, 50, 1, 'expit', 0, 1000, 0.025, True, 0.001)  scored  0.7058434870897703\n",
      "\n",
      "Training combination number 118 out of 432\n",
      "(1, 25, 25, 1, 'ReLU', 1e-05, 1000, 0.05, True, 0.001)  scored  0.6383635455042634\n",
      "\n",
      "Training combination number 142 out of 432\n",
      "(1, 25, 25, 1, 'leaky_ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.6391968005374162\n",
      "\n",
      "Training combination number 196 out of 432\n",
      "(1, 50, 50, 1, 'expit', 1e-05, 1000, 0.1, True, 0.001)  scored  0.875859084630566\n",
      "\n",
      "Training combination number 168 out of 432\n",
      "(1, 50, 25, 1, 'ReLU', 0.0001, 1000, 0.1, True, 0.001)  scored  0.7043229068564119\n",
      "\n",
      "Training combination number 119 out of 432\n",
      "(1, 25, 25, 1, 'ReLU', 1e-05, 1000, 0.025, True, 0.001)  scored  0.666065210049936\n",
      "\n",
      "Training combination number 143 out of 432\n",
      "(1, 25, 25, 1, 'leaky_ReLU', 0.001, 1000, 0.025, True, 0.001)  scored  0.7920237659113272\n",
      "\n",
      "Training combination number 197 out of 432\n",
      "(1, 50, 50, 1, 'expit', 1e-05, 1000, 0.075, True, 0.001)  scored  0.6883279504551207\n",
      "\n",
      "Training combination number 144 out of 432\n",
      "(1, 50, 25, 1, 'expit', 0, 1000, 0.1, True, 0.001)  scored  0.6394367238264311\n",
      "\n",
      "Training combination number 120 out of 432\n",
      "(1, 25, 25, 1, 'ReLU', 0.0001, 1000, 0.1, True, 0.001)  scored  0.6176316303731982\n",
      "\n",
      "Training combination number 169 out of 432\n",
      "(1, 50, 25, 1, 'ReLU', 0.0001, 1000, 0.075, True, 0.001)  scored  0.6613248619140577\n",
      "\n",
      "Training combination number 198 out of 432\n",
      "(1, 50, 50, 1, 'expit', 1e-05, 1000, 0.05, True, 0.001)  scored  0.691586717929529\n",
      "\n",
      "Training combination number 145 out of 432\n",
      "(1, 50, 25, 1, 'expit', 0, 1000, 0.075, True, 0.001)  scored  0.6846320525925121\n",
      "\n",
      "Training combination number 121 out of 432\n",
      "(1, 25, 25, 1, 'ReLU', 0.0001, 1000, 0.075, True, 0.001)  scored  0.6816935962042275\n",
      "\n",
      "Training combination number 199 out of 432\n",
      "(1, 50, 50, 1, 'expit', 1e-05, 1000, 0.025, True, 0.001)  scored  0.7326421698101115\n",
      "\n",
      "Training combination number 170 out of 432\n",
      "(1, 50, 25, 1, 'ReLU', 0.0001, 1000, 0.05, True, 0.001)  scored  0.6620633277935254\n",
      "\n",
      "Training combination number 146 out of 432\n",
      "(1, 50, 25, 1, 'expit', 0, 1000, 0.05, True, 0.001)  scored  0.7161258396027694\n",
      "\n",
      "Training combination number 200 out of 432\n",
      "(1, 50, 50, 1, 'expit', 0.0001, 1000, 0.1, True, 0.001)  scored  1.4941334847448193\n",
      "\n",
      "Training combination number 122 out of 432\n",
      "(1, 25, 25, 1, 'ReLU', 0.0001, 1000, 0.05, True, 0.001)  scored  0.6558461981793937\n",
      "\n",
      "Training combination number 147 out of 432\n",
      "(1, 50, 25, 1, 'expit', 0, 1000, 0.025, True, 0.001)  scored  0.7258292437972659\n",
      "\n",
      "Training combination number 171 out of 432\n",
      "(1, 50, 25, 1, 'ReLU', 0.0001, 1000, 0.025, True, 0.001)  scored  0.6773084169723225\n",
      "\n",
      "Training combination number 201 out of 432\n",
      "(1, 50, 50, 1, 'expit', 0.0001, 1000, 0.075, True, 0.001)  scored  0.6747587345505299\n",
      "\n",
      "Training combination number 148 out of 432\n",
      "(1, 50, 25, 1, 'expit', 1e-05, 1000, 0.1, True, 0.001)  scored  0.647543130068181\n",
      "\n",
      "Training combination number 123 out of 432\n",
      "(1, 25, 25, 1, 'ReLU', 0.0001, 1000, 0.025, True, 0.001)  scored  0.658301956724209\n",
      "\n",
      "Training combination number 172 out of 432\n",
      "(1, 50, 25, 1, 'ReLU', 0.001, 1000, 0.1, True, 0.001)  scored  0.6637591224371213\n",
      "\n",
      "Training combination number 202 out of 432\n",
      "(1, 50, 50, 1, 'expit', 0.0001, 1000, 0.05, True, 0.001)  scored  0.6709641818213447\n",
      "\n",
      "Training combination number 149 out of 432\n",
      "(1, 50, 25, 1, 'expit', 1e-05, 1000, 0.075, True, 0.001)  scored  0.7030239995921089\n",
      "\n",
      "Training combination number 124 out of 432\n",
      "(1, 25, 25, 1, 'ReLU', 0.001, 1000, 0.1, True, 0.001)  scored  0.6721251651644841\n",
      "\n",
      "Training combination number 150 out of 432\n",
      "(1, 50, 25, 1, 'expit', 1e-05, 1000, 0.05, True, 0.001)  scored  0.7202908711646767\n",
      "\n",
      "Training combination number 203 out of 432\n",
      "(1, 50, 50, 1, 'expit', 0.0001, 1000, 0.025, True, 0.001)  scored  0.7603466817879948\n",
      "\n",
      "Training combination number 173 out of 432\n",
      "(1, 50, 25, 1, 'ReLU', 0.001, 1000, 0.075, True, 0.001)  scored  0.6664360979639424\n",
      "\n",
      "Training combination number 125 out of 432\n",
      "(1, 25, 25, 1, 'ReLU', 0.001, 1000, 0.075, True, 0.001)  scored  0.631887528965659\n",
      "\n",
      "Training combination number 151 out of 432\n",
      "(1, 50, 25, 1, 'expit', 1e-05, 1000, 0.025, True, 0.001)  scored  0.70870395604187\n",
      "\n",
      "Training combination number 204 out of 432\n",
      "(1, 50, 50, 1, 'expit', 0.001, 1000, 0.1, True, 0.001)  scored  0.7058075891590192\n",
      "\n",
      "Training combination number 174 out of 432\n",
      "(1, 50, 25, 1, 'ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.6212308220058159\n",
      "\n",
      "Training combination number 152 out of 432\n",
      "(1, 50, 25, 1, 'expit', 0.0001, 1000, 0.1, True, 0.001)  scored  0.6613202866548111\n",
      "\n",
      "Training combination number 126 out of 432\n",
      "(1, 25, 25, 1, 'ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.6177488520084532\n",
      "\n",
      "Training combination number 205 out of 432\n",
      "(1, 50, 50, 1, 'expit', 0.001, 1000, 0.075, True, 0.001)  scored  0.7048982854774317\n",
      "\n",
      "Training combination number 153 out of 432\n",
      "(1, 50, 25, 1, 'expit', 0.0001, 1000, 0.075, True, 0.001)  scored  0.6755387891446997\n",
      "\n",
      "Training combination number 206 out of 432\n",
      "(1, 50, 50, 1, 'expit', 0.001, 1000, 0.05, True, 0.001)  scored  0.790748437502724\n",
      "\n",
      "Training combination number 175 out of 432\n",
      "(1, 50, 25, 1, 'ReLU', 0.001, 1000, 0.025, True, 0.001)  scored  0.7505863484732076\n",
      "\n",
      "Training combination number 127 out of 432\n",
      "(1, 25, 25, 1, 'ReLU', 0.001, 1000, 0.025, True, 0.001)  scored  0.7011168886678141\n",
      "\n",
      "Training combination number 154 out of 432\n",
      "(1, 50, 25, 1, 'expit', 0.0001, 1000, 0.05, True, 0.001)  scored  0.7013457646183443\n",
      "\n",
      "Training combination number 207 out of 432\n",
      "(1, 50, 50, 1, 'expit', 0.001, 1000, 0.025, True, 0.001)  scored  0.8582857791838729\n",
      "\n",
      "Training combination number 128 out of 432\n",
      "(1, 25, 25, 1, 'leaky_ReLU', 0, 1000, 0.1, True, 0.001)  scored  0.6508311977082025\n",
      "\n",
      "Training combination number 176 out of 432\n",
      "(1, 50, 25, 1, 'leaky_ReLU', 0, 1000, 0.1, True, 0.001)  scored  0.6844219638905888\n",
      "\n",
      "Training combination number 155 out of 432\n",
      "(1, 50, 25, 1, 'expit', 0.0001, 1000, 0.025, True, 0.001)  scored  0.7076476382335357\n",
      "\n",
      "Training combination number 156 out of 432\n",
      "(1, 50, 25, 1, 'expit', 0.001, 1000, 0.1, True, 0.001)  scored  0.6941045854817585\n",
      "\n",
      "Training combination number 129 out of 432\n",
      "(1, 25, 25, 1, 'leaky_ReLU', 0, 1000, 0.075, True, 0.001)  scored  0.6478395113785682\n",
      "\n",
      "Training combination number 208 out of 432\n",
      "(1, 50, 50, 1, 'ReLU', 0, 1000, 0.1, True, 0.001)  scored  0.6900201178361317\n",
      "\n",
      "Training combination number 177 out of 432\n",
      "(1, 50, 25, 1, 'leaky_ReLU', 0, 1000, 0.075, True, 0.001)  scored  0.6306787760808412\n",
      "\n",
      "Training combination number 157 out of 432\n",
      "(1, 50, 25, 1, 'expit', 0.001, 1000, 0.075, True, 0.001)  scored  0.7303847963696403\n",
      "\n",
      "Training combination number 130 out of 432\n",
      "(1, 25, 25, 1, 'leaky_ReLU', 0, 1000, 0.05, True, 0.001)  scored  0.6217698289022504\n",
      "\n",
      "Training combination number 178 out of 432\n",
      "(1, 50, 25, 1, 'leaky_ReLU', 0, 1000, 0.05, True, 0.001)  scored  0.6298575275011042\n",
      "\n",
      "Training combination number 209 out of 432\n",
      "(1, 50, 50, 1, 'ReLU', 0, 1000, 0.075, True, 0.001)  scored  0.6309127066283965\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training combination number 158 out of 432\n",
      "(1, 50, 25, 1, 'expit', 0.001, 1000, 0.05, True, 0.001)  scored  0.7804488046821371\n",
      "\n",
      "Training combination number 131 out of 432\n",
      "(1, 25, 25, 1, 'leaky_ReLU', 0, 1000, 0.025, True, 0.001)  scored  0.6536112403691817\n",
      "\n",
      "Training combination number 159 out of 432\n",
      "(1, 50, 25, 1, 'expit', 0.001, 1000, 0.025, True, 0.001)  scored  0.8460669586793792\n",
      "\n",
      "Training combination number 179 out of 432\n",
      "(1, 50, 25, 1, 'leaky_ReLU', 0, 1000, 0.025, True, 0.001)  scored  0.6519215663318318\n",
      "\n",
      "Training combination number 210 out of 432\n",
      "(1, 50, 50, 1, 'ReLU', 0, 1000, 0.05, True, 0.001)  scored  0.6402448172591736\n",
      "\n",
      "Training combination number 132 out of 432\n",
      "(1, 25, 25, 1, 'leaky_ReLU', 1e-05, 1000, 0.1, True, 0.001)  scored  0.6562051551325097\n",
      "\n",
      "Training combination number 160 out of 432\n",
      "(1, 50, 25, 1, 'ReLU', 0, 1000, 0.1, True, 0.001)  scored  0.7652485398763323\n",
      "\n",
      "Training combination number 180 out of 432\n",
      "(1, 50, 25, 1, 'leaky_ReLU', 1e-05, 1000, 0.1, True, 0.001)  scored  0.6595604782265085\n",
      "\n",
      "Training combination number 133 out of 432\n",
      "(1, 25, 25, 1, 'leaky_ReLU', 1e-05, 1000, 0.075, True, 0.001)  scored  0.6509720233856345\n",
      "\n",
      "Training combination number 211 out of 432\n",
      "(1, 50, 50, 1, 'ReLU', 0, 1000, 0.025, True, 0.001)  scored  0.6653230080722874\n",
      "\n",
      "Training combination number 161 out of 432\n",
      "(1, 50, 25, 1, 'ReLU', 0, 1000, 0.075, True, 0.001)  scored  0.6313525705812991\n",
      "\n",
      "Training combination number 181 out of 432\n",
      "(1, 50, 25, 1, 'leaky_ReLU', 1e-05, 1000, 0.075, True, 0.001)  scored  0.6587436910975827\n",
      "\n",
      "Training combination number 134 out of 432\n",
      "(1, 25, 25, 1, 'leaky_ReLU', 1e-05, 1000, 0.05, True, 0.001)  scored  0.622705250465023\n",
      "\n",
      "Training combination number 212 out of 432\n",
      "(1, 50, 50, 1, 'ReLU', 1e-05, 1000, 0.1, True, 0.001)  scored  0.6854070487082522\n",
      "\n",
      "Training combination number 216 out of 432\n",
      "(1, 50, 50, 1, 'ReLU', 0.0001, 1000, 0.1, True, 0.001)  scored  0.6364691592679399\n",
      "\n",
      "Training combination number 243 out of 432\n",
      "(1, 75, 50, 1, 'expit', 0, 1000, 0.025, True, 0.001)  scored  0.7095150564359141\n",
      "\n",
      "Training combination number 182 out of 432\n",
      "(1, 50, 25, 1, 'leaky_ReLU', 1e-05, 1000, 0.05, True, 0.001)  scored  0.6423106569099225\n",
      "\n",
      "Training combination number 213 out of 432\n",
      "(1, 50, 50, 1, 'ReLU', 1e-05, 1000, 0.075, True, 0.001)  scored  0.629316180601264\n",
      "\n",
      "Training combination number 244 out of 432\n",
      "(1, 75, 50, 1, 'expit', 1e-05, 1000, 0.1, True, 0.001)  scored  1.845049634396925\n",
      "\n",
      "Training combination number 217 out of 432\n",
      "(1, 50, 50, 1, 'ReLU', 0.0001, 1000, 0.075, True, 0.001)  scored  0.6302357948480233\n",
      "\n",
      "Training combination number 183 out of 432\n",
      "(1, 50, 25, 1, 'leaky_ReLU', 1e-05, 1000, 0.025, True, 0.001)  scored  0.6771464081901155\n",
      "\n",
      "Training combination number 245 out of 432\n",
      "(1, 75, 50, 1, 'expit', 1e-05, 1000, 0.075, True, 0.001)  scored  0.7788707310954561\n",
      "\n",
      "Training combination number 214 out of 432\n",
      "(1, 50, 50, 1, 'ReLU', 1e-05, 1000, 0.05, True, 0.001)  scored  0.6346012636934601\n",
      "\n",
      "Training combination number 184 out of 432\n",
      "(1, 50, 25, 1, 'leaky_ReLU', 0.0001, 1000, 0.1, True, 0.001)  scored  0.6546539179910495\n",
      "\n",
      "Training combination number 218 out of 432\n",
      "(1, 50, 50, 1, 'ReLU', 0.0001, 1000, 0.05, True, 0.001)  scored  0.6593764375056058\n",
      "\n",
      "Training combination number 246 out of 432\n",
      "(1, 75, 50, 1, 'expit', 1e-05, 1000, 0.05, True, 0.001)  scored  0.734806174348696\n",
      "\n",
      "Training combination number 185 out of 432\n",
      "(1, 50, 25, 1, 'leaky_ReLU', 0.0001, 1000, 0.075, True, 0.001)  scored  0.6474687035473844\n",
      "\n",
      "Training combination number 215 out of 432\n",
      "(1, 50, 50, 1, 'ReLU', 1e-05, 1000, 0.025, True, 0.001)  scored  0.6494762522334646\n",
      "\n",
      "Training combination number 247 out of 432\n",
      "(1, 75, 50, 1, 'expit', 1e-05, 1000, 0.025, True, 0.001)  scored  0.7056488747349624\n",
      "\n",
      "Training combination number 219 out of 432\n",
      "(1, 50, 50, 1, 'ReLU', 0.0001, 1000, 0.025, True, 0.001)  scored  0.6763385107313846\n",
      "\n",
      "Training combination number 248 out of 432\n",
      "(1, 75, 50, 1, 'expit', 0.0001, 1000, 0.1, True, 0.001)  scored  1.307671305928543\n",
      "\n",
      "Training combination number 186 out of 432\n",
      "(1, 50, 25, 1, 'leaky_ReLU', 0.0001, 1000, 0.05, True, 0.001)  scored  0.6595534442903763\n",
      "\n",
      "Training combination number 270 out of 432\n",
      "(1, 75, 50, 1, 'ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.6206035466193035\n",
      "\n",
      "Training combination number 220 out of 432\n",
      "(1, 50, 50, 1, 'ReLU', 0.001, 1000, 0.1, True, 0.001)  scored  0.6511075206716365\n",
      "\n",
      "Training combination number 249 out of 432\n",
      "(1, 75, 50, 1, 'expit', 0.0001, 1000, 0.075, True, 0.001)  scored  0.6664624456095124\n",
      "\n",
      "Training combination number 187 out of 432\n",
      "(1, 50, 25, 1, 'leaky_ReLU', 0.0001, 1000, 0.025, True, 0.001)  scored  0.6624412630899101\n",
      "\n",
      "Training combination number 250 out of 432\n",
      "(1, 75, 50, 1, 'expit', 0.0001, 1000, 0.05, True, 0.001)  scored  0.7187324688654724\n",
      "\n",
      "Training combination number 221 out of 432\n",
      "(1, 50, 50, 1, 'ReLU', 0.001, 1000, 0.075, True, 0.001)  scored  0.6817115194246807\n",
      "\n",
      "Training combination number 271 out of 432\n",
      "(1, 75, 50, 1, 'ReLU', 0.001, 1000, 0.025, True, 0.001)  scored  0.8243819730596637\n",
      "\n",
      "Training combination number 188 out of 432\n",
      "(1, 50, 25, 1, 'leaky_ReLU', 0.001, 1000, 0.1, True, 0.001)  scored  0.6816653481586565\n",
      "\n",
      "Training combination number 251 out of 432\n",
      "(1, 75, 50, 1, 'expit', 0.0001, 1000, 0.025, True, 0.001)  scored  0.713343753573211\n",
      "\n",
      "Training combination number 222 out of 432\n",
      "(1, 50, 50, 1, 'ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.7161211714939864\n",
      "\n",
      "Training combination number 252 out of 432\n",
      "(1, 75, 50, 1, 'expit', 0.001, 1000, 0.1, True, 0.001)  scored  0.7296184246053403\n",
      "\n",
      "Training combination number 272 out of 432\n",
      "(1, 75, 50, 1, 'leaky_ReLU', 0, 1000, 0.1, True, 0.001)  scored  0.6693853948922122\n",
      "\n",
      "Training combination number 297 out of 432\n",
      "(1, 75, 75, 1, 'expit', 0.0001, 1000, 0.075, True, 0.001)  scored  0.7268142267030666\n",
      "\n",
      "Training combination number 253 out of 432\n",
      "(1, 75, 50, 1, 'expit', 0.001, 1000, 0.075, True, 0.001)  scored  0.745023189048838\n",
      "\n",
      "Training combination number 223 out of 432\n",
      "(1, 50, 50, 1, 'ReLU', 0.001, 1000, 0.025, True, 0.001)  scored  0.6505237630863409\n",
      "\n",
      "Training combination number 273 out of 432\n",
      "(1, 75, 50, 1, 'leaky_ReLU', 0, 1000, 0.075, True, 0.001)  scored  0.6531855398630957\n",
      "\n",
      "Training combination number 254 out of 432\n",
      "(1, 75, 50, 1, 'expit', 0.001, 1000, 0.05, True, 0.001)  scored  0.7928826397773894\n",
      "\n",
      "Training combination number 298 out of 432\n",
      "(1, 75, 75, 1, 'expit', 0.0001, 1000, 0.05, True, 0.001)  scored  0.7078974745680673\n",
      "\n",
      "Training combination number 224 out of 432\n",
      "(1, 50, 50, 1, 'leaky_ReLU', 0, 1000, 0.1, True, 0.001)  scored  0.6666182860288031\n",
      "\n",
      "Training combination number 255 out of 432\n",
      "(1, 75, 50, 1, 'expit', 0.001, 1000, 0.025, True, 0.001)  scored  0.8892393593554582\n",
      "\n",
      "Training combination number 274 out of 432\n",
      "(1, 75, 50, 1, 'leaky_ReLU', 0, 1000, 0.05, True, 0.001)  scored  0.6498673621907163\n",
      "\n",
      "Training combination number 225 out of 432\n",
      "(1, 50, 50, 1, 'leaky_ReLU', 0, 1000, 0.075, True, 0.001)  scored  0.6659157526775541\n",
      "\n",
      "Training combination number 299 out of 432\n",
      "(1, 75, 75, 1, 'expit', 0.0001, 1000, 0.025, True, 0.001)  scored  0.7388981042430124\n",
      "\n",
      "Training combination number 256 out of 432\n",
      "(1, 75, 50, 1, 'ReLU', 0, 1000, 0.1, True, 0.001)  scored  0.6322575970696336\n",
      "\n",
      "Training combination number 275 out of 432\n",
      "(1, 75, 50, 1, 'leaky_ReLU', 0, 1000, 0.025, True, 0.001)  scored  0.6710479141064418\n",
      "\n",
      "Training combination number 226 out of 432\n",
      "(1, 50, 50, 1, 'leaky_ReLU', 0, 1000, 0.05, True, 0.001)  scored  0.6211126268551489\n",
      "\n",
      "Training combination number 300 out of 432\n",
      "(1, 75, 75, 1, 'expit', 0.001, 1000, 0.1, True, 0.001)  scored  0.6664470550671683\n",
      "\n",
      "Training combination number 257 out of 432\n",
      "(1, 75, 50, 1, 'ReLU', 0, 1000, 0.075, True, 0.001)  scored  0.6217479080512986\n",
      "\n",
      "Training combination number 227 out of 432\n",
      "(1, 50, 50, 1, 'leaky_ReLU', 0, 1000, 0.025, True, 0.001)  scored  0.6660193419749249\n",
      "\n",
      "Training combination number 276 out of 432\n",
      "(1, 75, 50, 1, 'leaky_ReLU', 1e-05, 1000, 0.1, True, 0.001)  scored  0.6631571537822907\n",
      "\n",
      "Training combination number 258 out of 432\n",
      "(1, 75, 50, 1, 'ReLU', 0, 1000, 0.05, True, 0.001)  scored  0.6414761706479366\n",
      "\n",
      "Training combination number 228 out of 432\n",
      "(1, 50, 50, 1, 'leaky_ReLU', 1e-05, 1000, 0.1, True, 0.001)  scored  0.6573675754062116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training combination number 301 out of 432\n",
      "(1, 75, 75, 1, 'expit', 0.001, 1000, 0.075, True, 0.001)  scored  0.7110612716859548\n",
      "\n",
      "Training combination number 277 out of 432\n",
      "(1, 75, 50, 1, 'leaky_ReLU', 1e-05, 1000, 0.075, True, 0.001)  scored  0.6435510712819368\n",
      "\n",
      "Training combination number 229 out of 432\n",
      "(1, 50, 50, 1, 'leaky_ReLU', 1e-05, 1000, 0.075, True, 0.001)  scored  0.6599042273469764\n",
      "\n",
      "Training combination number 259 out of 432\n",
      "(1, 75, 50, 1, 'ReLU', 0, 1000, 0.025, True, 0.001)  scored  0.6624686154374333\n",
      "\n",
      "Training combination number 302 out of 432\n",
      "(1, 75, 75, 1, 'expit', 0.001, 1000, 0.05, True, 0.001)  scored  0.7456244747983936\n",
      "\n",
      "Training combination number 278 out of 432\n",
      "(1, 75, 50, 1, 'leaky_ReLU', 1e-05, 1000, 0.05, True, 0.001)  scored  0.6225606611350187\n",
      "\n",
      "Training combination number 230 out of 432\n",
      "(1, 50, 50, 1, 'leaky_ReLU', 1e-05, 1000, 0.05, True, 0.001)  scored  0.6195408027647915\n",
      "\n",
      "Training combination number 260 out of 432\n",
      "(1, 75, 50, 1, 'ReLU', 1e-05, 1000, 0.1, True, 0.001)  scored  0.6645648889198392\n",
      "\n",
      "Training combination number 303 out of 432\n",
      "(1, 75, 75, 1, 'expit', 0.001, 1000, 0.025, True, 0.001)  scored  0.8722570538982416\n",
      "\n",
      "Training combination number 279 out of 432\n",
      "(1, 75, 50, 1, 'leaky_ReLU', 1e-05, 1000, 0.025, True, 0.001)  scored  0.6567800102895296\n",
      "\n",
      "Training combination number 231 out of 432\n",
      "(1, 50, 50, 1, 'leaky_ReLU', 1e-05, 1000, 0.025, True, 0.001)  scored  0.6513258230579058\n",
      "\n",
      "Training combination number 261 out of 432\n",
      "(1, 75, 50, 1, 'ReLU', 1e-05, 1000, 0.075, True, 0.001)  scored  0.6210436694137336\n",
      "\n",
      "Training combination number 280 out of 432\n",
      "(1, 75, 50, 1, 'leaky_ReLU', 0.0001, 1000, 0.1, True, 0.001)  scored  0.6439084233605153\n",
      "\n",
      "Training combination number 304 out of 432\n",
      "(1, 75, 75, 1, 'ReLU', 0, 1000, 0.1, True, 0.001)  scored  0.6280666174407277\n",
      "\n",
      "Training combination number 232 out of 432\n",
      "(1, 50, 50, 1, 'leaky_ReLU', 0.0001, 1000, 0.1, True, 0.001)  scored  0.6754029861584433\n",
      "\n",
      "Training combination number 262 out of 432\n",
      "(1, 75, 50, 1, 'ReLU', 1e-05, 1000, 0.05, True, 0.001)  scored  0.6462032818662009\n",
      "\n",
      "Training combination number 233 out of 432\n",
      "(1, 50, 50, 1, 'leaky_ReLU', 0.0001, 1000, 0.075, True, 0.001)  scored  0.6658482534381394\n",
      "\n",
      "Training combination number 281 out of 432\n",
      "(1, 75, 50, 1, 'leaky_ReLU', 0.0001, 1000, 0.075, True, 0.001)  scored  0.680957001040105\n",
      "\n",
      "Training combination number 263 out of 432\n",
      "(1, 75, 50, 1, 'ReLU', 1e-05, 1000, 0.025, True, 0.001)  scored  0.6657897959338404\n",
      "\n",
      "Training combination number 305 out of 432\n",
      "(1, 75, 75, 1, 'ReLU', 0, 1000, 0.075, True, 0.001)  scored  0.6331693540068679\n",
      "\n",
      "Training combination number 234 out of 432\n",
      "(1, 50, 50, 1, 'leaky_ReLU', 0.0001, 1000, 0.05, True, 0.001)  scored  0.6558756003108961\n",
      "\n",
      "Training combination number 282 out of 432\n",
      "(1, 75, 50, 1, 'leaky_ReLU', 0.0001, 1000, 0.05, True, 0.001)  scored  0.6702117373115939\n",
      "\n",
      "Training combination number 264 out of 432\n",
      "(1, 75, 50, 1, 'ReLU', 0.0001, 1000, 0.1, True, 0.001)  scored  0.6793934377652442\n",
      "\n",
      "Training combination number 235 out of 432\n",
      "(1, 50, 50, 1, 'leaky_ReLU', 0.0001, 1000, 0.025, True, 0.001)  scored  0.6615713412230918\n",
      "\n",
      "Training combination number 283 out of 432\n",
      "(1, 75, 50, 1, 'leaky_ReLU', 0.0001, 1000, 0.025, True, 0.001)  scored  0.662866446478843\n",
      "\n",
      "Training combination number 306 out of 432\n",
      "(1, 75, 75, 1, 'ReLU', 0, 1000, 0.05, True, 0.001)  scored  0.6444376835738547\n",
      "\n",
      "Training combination number 265 out of 432\n",
      "(1, 75, 50, 1, 'ReLU', 0.0001, 1000, 0.075, True, 0.001)  scored  0.6432093238502024\n",
      "\n",
      "Training combination number 236 out of 432\n",
      "(1, 50, 50, 1, 'leaky_ReLU', 0.001, 1000, 0.1, True, 0.001)  scored  0.6475608839401261\n",
      "\n",
      "Training combination number 284 out of 432\n",
      "(1, 75, 50, 1, 'leaky_ReLU', 0.001, 1000, 0.1, True, 0.001)  scored  0.6555016130638113\n",
      "\n",
      "Training combination number 237 out of 432\n",
      "(1, 50, 50, 1, 'leaky_ReLU', 0.001, 1000, 0.075, True, 0.001)  scored  0.6509639847072277\n",
      "\n",
      "Training combination number 266 out of 432\n",
      "(1, 75, 50, 1, 'ReLU', 0.0001, 1000, 0.05, True, 0.001)  scored  0.6587673495502813\n",
      "\n",
      "Training combination number 307 out of 432\n",
      "(1, 75, 75, 1, 'ReLU', 0, 1000, 0.025, True, 0.001)  scored  0.6495632079071025\n",
      "\n",
      "Training combination number 285 out of 432\n",
      "(1, 75, 50, 1, 'leaky_ReLU', 0.001, 1000, 0.075, True, 0.001)  scored  0.6203696896039549\n",
      "\n",
      "Training combination number 238 out of 432\n",
      "(1, 50, 50, 1, 'leaky_ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.6130377008857983\n",
      "\n",
      "Training combination number 267 out of 432\n",
      "(1, 75, 50, 1, 'ReLU', 0.0001, 1000, 0.025, True, 0.001)  scored  0.6636348552954612\n",
      "\n",
      "Training combination number 286 out of 432\n",
      "(1, 75, 50, 1, 'leaky_ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.6120534856025651\n",
      "\n",
      "Training combination number 239 out of 432\n",
      "(1, 50, 50, 1, 'leaky_ReLU', 0.001, 1000, 0.025, True, 0.001)  scored  0.8064727286072708\n",
      "\n",
      "Training combination number 268 out of 432\n",
      "(1, 75, 50, 1, 'ReLU', 0.001, 1000, 0.1, True, 0.001)  scored  0.678323032101387\n",
      "\n",
      "Training combination number 308 out of 432\n",
      "(1, 75, 75, 1, 'ReLU', 1e-05, 1000, 0.1, True, 0.001)  scored  0.6404542997780778\n",
      "\n",
      "Training combination number 240 out of 432\n",
      "(1, 75, 50, 1, 'expit', 0, 1000, 0.1, True, 0.001)  scored  1.8354564156087863\n",
      "\n",
      "Training combination number 287 out of 432\n",
      "(1, 75, 50, 1, 'leaky_ReLU', 0.001, 1000, 0.025, True, 0.001)  scored  0.8095277761041922\n",
      "\n",
      "Training combination number 241 out of 432\n",
      "(1, 75, 50, 1, 'expit', 0, 1000, 0.075, True, 0.001)  scored  0.7450389747286634\n",
      "\n",
      "Training combination number 269 out of 432\n",
      "(1, 75, 50, 1, 'ReLU', 0.001, 1000, 0.075, True, 0.001)  scored  0.6221562719136783\n",
      "\n",
      "Training combination number 288 out of 432\n",
      "(1, 75, 75, 1, 'expit', 0, 1000, 0.1, True, 0.001)  scored  1.8333425344848995\n",
      "\n",
      "Training combination number 242 out of 432\n",
      "(1, 75, 50, 1, 'expit', 0, 1000, 0.05, True, 0.001)  scored  0.7415828376638856\n",
      "\n",
      "Training combination number 309 out of 432\n",
      "(1, 75, 75, 1, 'ReLU', 1e-05, 1000, 0.075, True, 0.001)  scored  0.6186552400572267\n",
      "\n",
      "Training combination number 289 out of 432\n",
      "(1, 75, 75, 1, 'expit', 0, 1000, 0.075, True, 0.001)  scored  0.7866680923622595\n",
      "\n",
      "Training combination number 324 out of 432\n",
      "(1, 75, 75, 1, 'leaky_ReLU', 1e-05, 1000, 0.1, True, 0.001)  scored  0.6688475833447395\n",
      "\n",
      "Training combination number 351 out of 432\n",
      "(1, 100, 75, 1, 'expit', 0.001, 1000, 0.025, True, 0.001)  scored  1.0009874619050685\n",
      "\n",
      "Training combination number 310 out of 432\n",
      "(1, 75, 75, 1, 'ReLU', 1e-05, 1000, 0.05, True, 0.001)  scored  0.6305370429744794\n",
      "\n",
      "Training combination number 290 out of 432\n",
      "(1, 75, 75, 1, 'expit', 0, 1000, 0.05, True, 0.001)  scored  0.7115467084859599\n",
      "\n",
      "Training combination number 325 out of 432\n",
      "(1, 75, 75, 1, 'leaky_ReLU', 1e-05, 1000, 0.075, True, 0.001)  scored  0.6695492371029957\n",
      "\n",
      "Training combination number 352 out of 432\n",
      "(1, 100, 75, 1, 'ReLU', 0, 1000, 0.1, True, 0.001)  scored  0.616641987897224\n",
      "\n",
      "Training combination number 291 out of 432\n",
      "(1, 75, 75, 1, 'expit', 0, 1000, 0.025, True, 0.001)  scored  0.74653651921993\n",
      "\n",
      "Training combination number 311 out of 432\n",
      "(1, 75, 75, 1, 'ReLU', 1e-05, 1000, 0.025, True, 0.001)  scored  0.6551280494141675\n",
      "\n",
      "Training combination number 292 out of 432\n",
      "(1, 75, 75, 1, 'expit', 1e-05, 1000, 0.1, True, 0.001)  scored  1.7332360972330192\n",
      "\n",
      "Training combination number 326 out of 432\n",
      "(1, 75, 75, 1, 'leaky_ReLU', 1e-05, 1000, 0.05, True, 0.001)  scored  0.6205076545157234\n",
      "\n",
      "Training combination number 353 out of 432\n",
      "(1, 100, 75, 1, 'ReLU', 0, 1000, 0.075, True, 0.001)  scored  0.6279507193998803\n",
      "\n",
      "Training combination number 293 out of 432\n",
      "(1, 75, 75, 1, 'expit', 1e-05, 1000, 0.075, True, 0.001)  scored  1.2904331850771653\n",
      "\n",
      "Training combination number 312 out of 432\n",
      "(1, 75, 75, 1, 'ReLU', 0.0001, 1000, 0.1, True, 0.001)  scored  0.6653295895159551\n",
      "\n",
      "Training combination number 294 out of 432\n",
      "(1, 75, 75, 1, 'expit', 1e-05, 1000, 0.05, True, 0.001)  scored  0.6895486259302419\n",
      "\n",
      "Training combination number 327 out of 432\n",
      "(1, 75, 75, 1, 'leaky_ReLU', 1e-05, 1000, 0.025, True, 0.001)  scored  0.6670634218732127\n",
      "\n",
      "Training combination number 354 out of 432\n",
      "(1, 100, 75, 1, 'ReLU', 0, 1000, 0.05, True, 0.001)  scored  0.6440707835984527\n",
      "\n",
      "Training combination number 313 out of 432\n",
      "(1, 75, 75, 1, 'ReLU', 0.0001, 1000, 0.075, True, 0.001)  scored  0.6749716215135921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training combination number 295 out of 432\n",
      "(1, 75, 75, 1, 'expit', 1e-05, 1000, 0.025, True, 0.001)  scored  0.780854942098776\n",
      "\n",
      "Training combination number 328 out of 432\n",
      "(1, 75, 75, 1, 'leaky_ReLU', 0.0001, 1000, 0.1, True, 0.001)  scored  0.6513867989000842\n",
      "\n",
      "Training combination number 296 out of 432\n",
      "(1, 75, 75, 1, 'expit', 0.0001, 1000, 0.1, True, 0.001)  scored  1.3225627228061057\n",
      "\n",
      "Training combination number 314 out of 432\n",
      "(1, 75, 75, 1, 'ReLU', 0.0001, 1000, 0.05, True, 0.001)  scored  0.6593973019879555\n",
      "\n",
      "Training combination number 355 out of 432\n",
      "(1, 100, 75, 1, 'ReLU', 0, 1000, 0.025, True, 0.001)  scored  0.671140356977846\n",
      "\n",
      "Training combination number 329 out of 432\n",
      "(1, 75, 75, 1, 'leaky_ReLU', 0.0001, 1000, 0.075, True, 0.001)  scored  0.6412036271158453\n",
      "\n",
      "Training combination number 378 out of 432\n",
      "(1, 100, 75, 1, 'leaky_ReLU', 0.0001, 1000, 0.05, True, 0.001)  scored  0.6582535539807187\n",
      "\n",
      "Training combination number 315 out of 432\n",
      "(1, 75, 75, 1, 'ReLU', 0.0001, 1000, 0.025, True, 0.001)  scored  0.6744160079011323\n",
      "\n",
      "Training combination number 356 out of 432\n",
      "(1, 100, 75, 1, 'ReLU', 1e-05, 1000, 0.1, True, 0.001)  scored  0.6218102794363511\n",
      "\n",
      "Training combination number 330 out of 432\n",
      "(1, 75, 75, 1, 'leaky_ReLU', 0.0001, 1000, 0.05, True, 0.001)  scored  0.6587683460844272\n",
      "\n",
      "Training combination number 316 out of 432\n",
      "(1, 75, 75, 1, 'ReLU', 0.001, 1000, 0.1, True, 0.001)  scored  0.6306896951825982\n",
      "\n",
      "Training combination number 379 out of 432\n",
      "(1, 100, 75, 1, 'leaky_ReLU', 0.0001, 1000, 0.025, True, 0.001)  scored  0.6488445355860151\n",
      "\n",
      "Training combination number 357 out of 432\n",
      "(1, 100, 75, 1, 'ReLU', 1e-05, 1000, 0.075, True, 0.001)  scored  0.6255208382159285\n",
      "\n",
      "Training combination number 331 out of 432\n",
      "(1, 75, 75, 1, 'leaky_ReLU', 0.0001, 1000, 0.025, True, 0.001)  scored  0.6559021853791261\n",
      "\n",
      "Training combination number 317 out of 432\n",
      "(1, 75, 75, 1, 'ReLU', 0.001, 1000, 0.075, True, 0.001)  scored  0.6561149438971102\n",
      "\n",
      "Training combination number 380 out of 432\n",
      "(1, 100, 75, 1, 'leaky_ReLU', 0.001, 1000, 0.1, True, 0.001)  scored  0.6518289125544933\n",
      "\n",
      "Training combination number 358 out of 432\n",
      "(1, 100, 75, 1, 'ReLU', 1e-05, 1000, 0.05, True, 0.001)  scored  0.6558093611355218\n",
      "\n",
      "Training combination number 332 out of 432\n",
      "(1, 75, 75, 1, 'leaky_ReLU', 0.001, 1000, 0.1, True, 0.001)  scored  0.661343415377709\n",
      "\n",
      "Training combination number 318 out of 432\n",
      "(1, 75, 75, 1, 'ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.6230783681842991\n",
      "\n",
      "Training combination number 381 out of 432\n",
      "(1, 100, 75, 1, 'leaky_ReLU', 0.001, 1000, 0.075, True, 0.001)  scored  0.6537914022821935\n",
      "\n",
      "Training combination number 359 out of 432\n",
      "(1, 100, 75, 1, 'ReLU', 1e-05, 1000, 0.025, True, 0.001)  scored  0.6632424570402072\n",
      "\n",
      "Training combination number 333 out of 432\n",
      "(1, 75, 75, 1, 'leaky_ReLU', 0.001, 1000, 0.075, True, 0.001)  scored  0.6418693785495604\n",
      "\n",
      "Training combination number 319 out of 432\n",
      "(1, 75, 75, 1, 'ReLU', 0.001, 1000, 0.025, True, 0.001)  scored  0.6795688565409752\n",
      "\n",
      "Training combination number 382 out of 432\n",
      "(1, 100, 75, 1, 'leaky_ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.6173870870955898\n",
      "\n",
      "Training combination number 334 out of 432\n",
      "(1, 75, 75, 1, 'leaky_ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.6264703376866921\n",
      "\n",
      "Training combination number 360 out of 432\n",
      "(1, 100, 75, 1, 'ReLU', 0.0001, 1000, 0.1, True, 0.001)  scored  0.6405949123861306\n",
      "\n",
      "Training combination number 320 out of 432\n",
      "(1, 75, 75, 1, 'leaky_ReLU', 0, 1000, 0.1, True, 0.001)  scored  0.6647418846912793\n",
      "\n",
      "Training combination number 335 out of 432\n",
      "(1, 75, 75, 1, 'leaky_ReLU', 0.001, 1000, 0.025, True, 0.001)  scored  0.8197490597775519\n",
      "\n",
      "Training combination number 383 out of 432\n",
      "(1, 100, 75, 1, 'leaky_ReLU', 0.001, 1000, 0.025, True, 0.001)  scored  0.824320525292039\n",
      "\n",
      "Training combination number 361 out of 432\n",
      "(1, 100, 75, 1, 'ReLU', 0.0001, 1000, 0.075, True, 0.001)  scored  0.6374257040279306\n",
      "\n",
      "Training combination number 321 out of 432\n",
      "(1, 75, 75, 1, 'leaky_ReLU', 0, 1000, 0.075, True, 0.001)  scored  0.6478101093189579\n",
      "\n",
      "Training combination number 336 out of 432\n",
      "(1, 100, 75, 1, 'expit', 0, 1000, 0.1, True, 0.001)  scored  1.8350248514283605\n",
      "\n",
      "Training combination number 384 out of 432\n",
      "(1, 100, 100, 1, 'expit', 0, 1000, 0.1, True, 0.001)  scored  1.8341682109500046\n",
      "\n",
      "Training combination number 362 out of 432\n",
      "(1, 100, 75, 1, 'ReLU', 0.0001, 1000, 0.05, True, 0.001)  scored  0.661784788451937\n",
      "\n",
      "Training combination number 337 out of 432\n",
      "(1, 100, 75, 1, 'expit', 0, 1000, 0.075, True, 0.001)  scored  0.8632416722335297\n",
      "\n",
      "Training combination number 385 out of 432\n",
      "(1, 100, 100, 1, 'expit', 0, 1000, 0.075, True, 0.001)  scored  1.3181525822598474\n",
      "\n",
      "Training combination number 322 out of 432\n",
      "(1, 75, 75, 1, 'leaky_ReLU', 0, 1000, 0.05, True, 0.001)  scored  0.6215781351032388\n",
      "\n",
      "Training combination number 338 out of 432\n",
      "(1, 100, 75, 1, 'expit', 0, 1000, 0.05, True, 0.001)  scored  0.7191650750574364\n",
      "\n",
      "Training combination number 386 out of 432\n",
      "(1, 100, 100, 1, 'expit', 0, 1000, 0.05, True, 0.001)  scored  0.7019236868513004\n",
      "\n",
      "Training combination number 363 out of 432\n",
      "(1, 100, 75, 1, 'ReLU', 0.0001, 1000, 0.025, True, 0.001)  scored  0.6555299226644815\n",
      "\n",
      "Training combination number 323 out of 432\n",
      "(1, 75, 75, 1, 'leaky_ReLU', 0, 1000, 0.025, True, 0.001)  scored  0.6653619404581415\n",
      "\n",
      "Training combination number 339 out of 432\n",
      "(1, 100, 75, 1, 'expit', 0, 1000, 0.025, True, 0.001)  scored  0.7323690640592995\n",
      "\n",
      "Training combination number 387 out of 432\n",
      "(1, 100, 100, 1, 'expit', 0, 1000, 0.025, True, 0.001)  scored  0.8186721610966476\n",
      "\n",
      "Training combination number 340 out of 432\n",
      "(1, 100, 75, 1, 'expit', 1e-05, 1000, 0.1, True, 0.001)  scored  1.8356557984377266\n",
      "\n",
      "Training combination number 388 out of 432\n",
      "(1, 100, 100, 1, 'expit', 1e-05, 1000, 0.1, True, 0.001)  scored  1.8353077103484767\n",
      "\n",
      "Training combination number 364 out of 432\n",
      "(1, 100, 75, 1, 'ReLU', 0.001, 1000, 0.1, True, 0.001)  scored  0.6497599573208592\n",
      "\n",
      "Training combination number 405 out of 432\n",
      "(1, 100, 100, 1, 'ReLU', 1e-05, 1000, 0.075, True, 0.001)  scored  0.6433300519527281\n",
      "\n",
      "Training combination number 341 out of 432\n",
      "(1, 100, 75, 1, 'expit', 1e-05, 1000, 0.075, True, 0.001)  scored  0.7162929561965602\n",
      "\n",
      "Training combination number 389 out of 432\n",
      "(1, 100, 100, 1, 'expit', 1e-05, 1000, 0.075, True, 0.001)  scored  1.7400525385864012\n",
      "\n",
      "Training combination number 342 out of 432\n",
      "(1, 100, 75, 1, 'expit', 1e-05, 1000, 0.05, True, 0.001)  scored  0.6682371375508127\n",
      "\n",
      "Training combination number 365 out of 432\n",
      "(1, 100, 75, 1, 'ReLU', 0.001, 1000, 0.075, True, 0.001)  scored  0.7097306908868983\n",
      "\n",
      "Training combination number 406 out of 432\n",
      "(1, 100, 100, 1, 'ReLU', 1e-05, 1000, 0.05, True, 0.001)  scored  0.6354211728553503\n",
      "\n",
      "Training combination number 390 out of 432\n",
      "(1, 100, 100, 1, 'expit', 1e-05, 1000, 0.05, True, 0.001)  scored  0.723455159641932\n",
      "\n",
      "Training combination number 343 out of 432\n",
      "(1, 100, 75, 1, 'expit', 1e-05, 1000, 0.025, True, 0.001)  scored  0.7637216594422758\n",
      "\n",
      "Training combination number 391 out of 432\n",
      "(1, 100, 100, 1, 'expit', 1e-05, 1000, 0.025, True, 0.001)  scored  0.7667730195440139\n",
      "\n",
      "Training combination number 366 out of 432\n",
      "(1, 100, 75, 1, 'ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.6627575760101677\n",
      "\n",
      "Training combination number 344 out of 432\n",
      "(1, 100, 75, 1, 'expit', 0.0001, 1000, 0.1, True, 0.001)  scored  1.8440096450344006\n",
      "\n",
      "Training combination number 407 out of 432\n",
      "(1, 100, 100, 1, 'ReLU', 1e-05, 1000, 0.025, True, 0.001)  scored  0.6697675682581007\n",
      "\n",
      "Training combination number 392 out of 432\n",
      "(1, 100, 100, 1, 'expit', 0.0001, 1000, 0.1, True, 0.001)  scored  1.3843738564977677\n",
      "\n",
      "Training combination number 345 out of 432\n",
      "(1, 100, 75, 1, 'expit', 0.0001, 1000, 0.075, True, 0.001)  scored  0.6735848777378256\n",
      "\n",
      "Training combination number 367 out of 432\n",
      "(1, 100, 75, 1, 'ReLU', 0.001, 1000, 0.025, True, 0.001)  scored  0.6628638203904107\n",
      "\n",
      "Training combination number 393 out of 432\n",
      "(1, 100, 100, 1, 'expit', 0.0001, 1000, 0.075, True, 0.001)  scored  0.6832390995454874\n",
      "\n",
      "Training combination number 408 out of 432\n",
      "(1, 100, 100, 1, 'ReLU', 0.0001, 1000, 0.1, True, 0.001)  scored  0.6881364483762076\n",
      "\n",
      "Training combination number 346 out of 432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 75, 1, 'expit', 0.0001, 1000, 0.05, True, 0.001)  scored  0.6782634208203151\n",
      "\n",
      "Training combination number 394 out of 432\n",
      "(1, 100, 100, 1, 'expit', 0.0001, 1000, 0.05, True, 0.001)  scored  0.6845036366662718\n",
      "\n",
      "Training combination number 347 out of 432\n",
      "(1, 100, 75, 1, 'expit', 0.0001, 1000, 0.025, True, 0.001)  scored  0.7471982060793113\n",
      "\n",
      "Training combination number 368 out of 432\n",
      "(1, 100, 75, 1, 'leaky_ReLU', 0, 1000, 0.1, True, 0.001)  scored  0.6759028109184797\n",
      "\n",
      "Training combination number 395 out of 432\n",
      "(1, 100, 100, 1, 'expit', 0.0001, 1000, 0.025, True, 0.001)  scored  0.7549153141546953\n",
      "\n",
      "Training combination number 409 out of 432\n",
      "(1, 100, 100, 1, 'ReLU', 0.0001, 1000, 0.075, True, 0.001)  scored  0.6708064514327768\n",
      "\n",
      "Training combination number 348 out of 432\n",
      "(1, 100, 75, 1, 'expit', 0.001, 1000, 0.1, True, 0.001)  scored  0.7086523406690046\n",
      "\n",
      "Training combination number 396 out of 432\n",
      "(1, 100, 100, 1, 'expit', 0.001, 1000, 0.1, True, 0.001)  scored  0.7211844668970588\n",
      "\n",
      "Training combination number 349 out of 432\n",
      "(1, 100, 75, 1, 'expit', 0.001, 1000, 0.075, True, 0.001)  scored  0.7296108574320417\n",
      "\n",
      "Training combination number 369 out of 432\n",
      "(1, 100, 75, 1, 'leaky_ReLU', 0, 1000, 0.075, True, 0.001)  scored  0.6650654361490596\n",
      "\n",
      "Training combination number 410 out of 432\n",
      "(1, 100, 100, 1, 'ReLU', 0.0001, 1000, 0.05, True, 0.001)  scored  0.660353642082216\n",
      "\n",
      "Training combination number 397 out of 432\n",
      "(1, 100, 100, 1, 'expit', 0.001, 1000, 0.075, True, 0.001)  scored  0.7435010971103905\n",
      "\n",
      "Training combination number 350 out of 432\n",
      "(1, 100, 75, 1, 'expit', 0.001, 1000, 0.05, True, 0.001)  scored  0.7715449070768006\n",
      "\n",
      "Training combination number 370 out of 432\n",
      "(1, 100, 75, 1, 'leaky_ReLU', 0, 1000, 0.05, True, 0.001)  scored  0.6300320091408197\n",
      "\n",
      "Training combination number 398 out of 432\n",
      "(1, 100, 100, 1, 'expit', 0.001, 1000, 0.05, True, 0.001)  scored  0.7629643187788744\n",
      "\n",
      "Training combination number 411 out of 432\n",
      "(1, 100, 100, 1, 'ReLU', 0.0001, 1000, 0.025, True, 0.001)  scored  0.6635074240429469\n",
      "\n",
      "Training combination number 399 out of 432\n",
      "(1, 100, 100, 1, 'expit', 0.001, 1000, 0.025, True, 0.001)  scored  0.9317483791512997\n",
      "\n",
      "Training combination number 371 out of 432\n",
      "(1, 100, 75, 1, 'leaky_ReLU', 0, 1000, 0.025, True, 0.001)  scored  0.6590523232158253\n",
      "\n",
      "Training combination number 412 out of 432\n",
      "(1, 100, 100, 1, 'ReLU', 0.001, 1000, 0.1, True, 0.001)  scored  0.6476501744482381\n",
      "\n",
      "Training combination number 400 out of 432\n",
      "(1, 100, 100, 1, 'ReLU', 0, 1000, 0.1, True, 0.001)  scored  0.6942578652879521\n",
      "\n",
      "Training combination number 372 out of 432\n",
      "(1, 100, 75, 1, 'leaky_ReLU', 1e-05, 1000, 0.1, True, 0.001)  scored  0.6625230944604003\n",
      "\n",
      "Training combination number 413 out of 432\n",
      "(1, 100, 100, 1, 'ReLU', 0.001, 1000, 0.075, True, 0.001)  scored  0.7694463823958402\n",
      "\n",
      "Training combination number 401 out of 432\n",
      "(1, 100, 100, 1, 'ReLU', 0, 1000, 0.075, True, 0.001)  scored  0.6208688159730402\n",
      "\n",
      "Training combination number 373 out of 432\n",
      "(1, 100, 75, 1, 'leaky_ReLU', 1e-05, 1000, 0.075, True, 0.001)  scored  0.6323708871604274\n",
      "\n",
      "Training combination number 414 out of 432\n",
      "(1, 100, 100, 1, 'ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.66906543239732\n",
      "\n",
      "Training combination number 402 out of 432\n",
      "(1, 100, 100, 1, 'ReLU', 0, 1000, 0.05, True, 0.001)  scored  0.6376822255752608\n",
      "\n",
      "Training combination number 374 out of 432\n",
      "(1, 100, 75, 1, 'leaky_ReLU', 1e-05, 1000, 0.05, True, 0.001)  scored  0.6152260111639933\n",
      "\n",
      "Training combination number 415 out of 432\n",
      "(1, 100, 100, 1, 'ReLU', 0.001, 1000, 0.025, True, 0.001)  scored  0.8311086670413801\n",
      "\n",
      "Training combination number 403 out of 432\n",
      "(1, 100, 100, 1, 'ReLU', 0, 1000, 0.025, True, 0.001)  scored  0.6678809316646949\n",
      "\n",
      "Training combination number 375 out of 432\n",
      "(1, 100, 75, 1, 'leaky_ReLU', 1e-05, 1000, 0.025, True, 0.001)  scored  0.6631645040177376\n",
      "\n",
      "Training combination number 416 out of 432\n",
      "(1, 100, 100, 1, 'leaky_ReLU', 0, 1000, 0.1, True, 0.001)  scored  0.6473927631568152\n",
      "\n",
      "Training combination number 404 out of 432\n",
      "(1, 100, 100, 1, 'ReLU', 1e-05, 1000, 0.1, True, 0.001)  scored  0.6385839990674927\n",
      "\n",
      "Training combination number 376 out of 432\n",
      "(1, 100, 75, 1, 'leaky_ReLU', 0.0001, 1000, 0.1, True, 0.001)  scored  0.6184122009141904\n",
      "\n",
      "Training combination number 417 out of 432\n",
      "(1, 100, 100, 1, 'leaky_ReLU', 0, 1000, 0.075, True, 0.001)  scored  0.6443449839897217\n",
      "\n",
      "Training combination number 377 out of 432\n",
      "(1, 100, 75, 1, 'leaky_ReLU', 0.0001, 1000, 0.075, True, 0.001)  scored  0.6577480156956916\n",
      "\n",
      "Training combination number 418 out of 432\n",
      "(1, 100, 100, 1, 'leaky_ReLU', 0, 1000, 0.05, True, 0.001)  scored  0.6360560954450303\n",
      "\n",
      "Training combination number 419 out of 432\n",
      "(1, 100, 100, 1, 'leaky_ReLU', 0, 1000, 0.025, True, 0.001)  scored  0.6585839236702984\n",
      "\n",
      "Training combination number 420 out of 432\n",
      "(1, 100, 100, 1, 'leaky_ReLU', 1e-05, 1000, 0.1, True, 0.001)  scored  0.6405436412105552\n",
      "\n",
      "Training combination number 421 out of 432\n",
      "(1, 100, 100, 1, 'leaky_ReLU', 1e-05, 1000, 0.075, True, 0.001)  scored  0.6486550714972799\n",
      "\n",
      "Training combination number 422 out of 432\n",
      "(1, 100, 100, 1, 'leaky_ReLU', 1e-05, 1000, 0.05, True, 0.001)  scored  0.6200530370349193\n",
      "\n",
      "Training combination number 423 out of 432\n",
      "(1, 100, 100, 1, 'leaky_ReLU', 1e-05, 1000, 0.025, True, 0.001)  scored  0.6517355806362244\n",
      "\n",
      "Training combination number 424 out of 432\n",
      "(1, 100, 100, 1, 'leaky_ReLU', 0.0001, 1000, 0.1, True, 0.001)  scored  0.7525561204237338\n",
      "\n",
      "Training combination number 425 out of 432\n",
      "(1, 100, 100, 1, 'leaky_ReLU', 0.0001, 1000, 0.075, True, 0.001)  scored  0.6262975234730341\n",
      "\n",
      "Training combination number 426 out of 432\n",
      "(1, 100, 100, 1, 'leaky_ReLU', 0.0001, 1000, 0.05, True, 0.001)  scored  0.6612283641444584\n",
      "\n",
      "Training combination number 427 out of 432\n",
      "(1, 100, 100, 1, 'leaky_ReLU', 0.0001, 1000, 0.025, True, 0.001)  scored  0.6582653832510659\n",
      "\n",
      "Training combination number 428 out of 432\n",
      "(1, 100, 100, 1, 'leaky_ReLU', 0.001, 1000, 0.1, True, 0.001)  scored  0.6463798379727762\n",
      "\n",
      "Training combination number 429 out of 432\n",
      "(1, 100, 100, 1, 'leaky_ReLU', 0.001, 1000, 0.075, True, 0.001)  scored  0.6736121294971172\n",
      "\n",
      "Training combination number 430 out of 432\n",
      "(1, 100, 100, 1, 'leaky_ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.614359719203359\n",
      "\n",
      "Training combination number 431 out of 432\n",
      "(1, 100, 100, 1, 'leaky_ReLU', 0.001, 1000, 0.025, True, 0.001)  scored  0.8206644061779185\n",
      "\n",
      "\n",
      " Time needed to complete Grid Search process:  35153.69070935249\n"
     ]
    }
   ],
   "source": [
    "# ----------- VECTORIZED ALTERNATIVE FOR GRID SEARCH ----------\n",
    "\n",
    "# Check time needed to perform the grid search\n",
    "import time\n",
    "\n",
    "# Define function to vectorize\n",
    "def Grid_Search_single_iteration(params, combin_index, tot_combins, x_train, y_train):\n",
    "    score = Kfold_Cross_Validation(x_train, y_train, *params)\n",
    "    print('\\nTraining combination number', combin_index, 'out of', tot_combins)\n",
    "    print(params, ' scored ', score)\n",
    "    return score\n",
    "\n",
    "\n",
    "# Import multi-threading libraries and define a pool of 4 threads\n",
    "from multiprocessing import Pool\n",
    "threads_pool = Pool(4)\n",
    "\n",
    "# Execute Vectorized Grid Search and save all scores in output\n",
    "n_combins = len(params_combinations)\n",
    "print('Starting Grid Search process')\n",
    "\n",
    "start = time.time()\n",
    "scores = threads_pool.starmap(Grid_Search_single_iteration, \n",
    "                              zip(params_combinations[:n_combins], np.arange(n_combins),\n",
    "                                  it.repeat(n_combins), it.repeat(x_train), it.repeat(y_train)))\n",
    "finish = time.time()\n",
    "print(\"\\n\\n Time needed to complete Grid Search process: \", finish-start)\n",
    "\n",
    "# close multiprocessing pool\n",
    "threads_pool.close()\n",
    "threads_pool.join()\n",
    "\n",
    "# Sort scores from best to worst and the combinations accordingly\n",
    "#sorted_scores, sorted_params = zip(*sorted(zip(scores, params_combinations[:n_combins])))\n",
    "sorted_scores, sorted_comb_indexes = zip(*sorted(zip(scores, np.arange(n_combins))))\n",
    "\n",
    "# Save to files the 2 ordered sequences\n",
    "with open('sorted_scores.dat', 'w') as f:\n",
    "    for item in sorted_scores:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open('sorted_combin_indexes.dat', 'w') as f:\n",
    "    for item in sorted_comb_indexes:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_comb_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelizing the work, the Grid Search process takes about 10 hours. So, if the kernel is interrupted or the machine is restarted, we still can execute the final training by reading the results directly from the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve scores of grid search from files\n",
    "sorted_scores = np.loadtxt('sorted_scores.dat', delimiter='\\n')\n",
    "sorted_comb_indexes = np.loadtxt('sorted_combin_indexes.dat', dtype=int, delimiter='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last training\n",
    "After ranking all the combinations of hyper-parameters based on their score during the Cross Validation, we can take the 10 best networks and re-train them over the entire training set. We then evaluate their performance on the test set (that we never used in the Grid Serarch process) to understand which is the best one.\n",
    "\n",
    "This can be done by storing a `winning_net` when its score measured on the test set is better then the previous one. After esting each network, the winning one will be saved to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best combinations are:\n",
      "(1, 75, 50, 1, 'leaky_ReLU', 0.001, 1000, 0.05, True, 0.001)  with score  0.6120534856025651\n",
      "(1, 50, 25, 1, 'leaky_ReLU', 0.001, 1000, 0.05, True, 0.001)  with score  0.6126143570422118\n",
      "(1, 25, 10, 1, 'ReLU', 0.0001, 1000, 0.05, True, 0.001)  with score  0.6126823233502273\n",
      "(1, 50, 50, 1, 'leaky_ReLU', 0.001, 1000, 0.05, True, 0.001)  with score  0.6130377008857983\n",
      "(1, 100, 100, 1, 'leaky_ReLU', 0.001, 1000, 0.05, True, 0.001)  with score  0.614359719203359\n",
      "(1, 100, 75, 1, 'leaky_ReLU', 1e-05, 1000, 0.05, True, 0.001)  with score  0.6152260111639933\n",
      "(1, 100, 75, 1, 'ReLU', 0, 1000, 0.1, True, 0.001)  with score  0.616641987897224\n",
      "(1, 100, 75, 1, 'leaky_ReLU', 0.001, 1000, 0.05, True, 0.001)  with score  0.6173870870955898\n",
      "(1, 10, 10, 1, 'ReLU', 0.001, 1000, 0.05, True, 0.001)  with score  0.6176123945162094\n",
      "(1, 25, 25, 1, 'ReLU', 0.0001, 1000, 0.1, True, 0.001)  with score  0.6176316303731982\n",
      "(1, 25, 25, 1, 'ReLU', 0.001, 1000, 0.05, True, 0.001)  with score  0.6177488520084532\n",
      "(1, 100, 75, 1, 'leaky_ReLU', 0.0001, 1000, 0.1, True, 0.001)  with score  0.6184122009141904\n",
      "(1, 75, 75, 1, 'ReLU', 1e-05, 1000, 0.075, True, 0.001)  with score  0.6186552400572267\n",
      "(1, 50, 50, 1, 'leaky_ReLU', 1e-05, 1000, 0.05, True, 0.001)  with score  0.6195408027647915\n",
      "(1, 25, 10, 1, 'leaky_ReLU', 0, 1000, 0.05, True, 0.001)  with score  0.6200042806400925\n",
      "(1, 100, 100, 1, 'leaky_ReLU', 1e-05, 1000, 0.05, True, 0.001)  with score  0.6200530370349193\n",
      "(1, 75, 50, 1, 'leaky_ReLU', 0.001, 1000, 0.075, True, 0.001)  with score  0.6203696896039549\n",
      "(1, 75, 75, 1, 'leaky_ReLU', 1e-05, 1000, 0.05, True, 0.001)  with score  0.6205076545157234\n",
      "(1, 75, 50, 1, 'ReLU', 0.001, 1000, 0.05, True, 0.001)  with score  0.6206035466193035\n",
      "(1, 100, 100, 1, 'ReLU', 0, 1000, 0.075, True, 0.001)  with score  0.6208688159730402\n"
     ]
    }
   ],
   "source": [
    "# First, let's see which are the best 20 configurations\n",
    "take_best = 20\n",
    "\n",
    "print(\"Best combinations are:\")\n",
    "for i in range(take_best):\n",
    "    print(params_combinations[sorted_comb_indexes[i]], \n",
    "          \" with score \", sorted_scores[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 75, 50, 1, 'leaky_ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.6006765952993338\n",
      "(1, 50, 25, 1, 'leaky_ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.6004071690227736\n",
      "(1, 25, 10, 1, 'ReLU', 0.0001, 1000, 0.05, True, 0.001)  scored  0.6152465018021325\n",
      "(1, 50, 50, 1, 'leaky_ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.6022631475296059\n",
      "(1, 100, 100, 1, 'leaky_ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.5946389250776093\n",
      "(1, 100, 75, 1, 'leaky_ReLU', 1e-05, 1000, 0.05, True, 0.001)  scored  0.6090152023266789\n",
      "(1, 100, 75, 1, 'ReLU', 0, 1000, 0.1, True, 0.001)  scored  0.7308170317110588\n",
      "(1, 100, 75, 1, 'leaky_ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.6036983793336554\n",
      "(1, 10, 10, 1, 'ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.6331324103516827\n",
      "(1, 25, 25, 1, 'ReLU', 0.0001, 1000, 0.1, True, 0.001)  scored  0.6112978083104891\n",
      "(1, 25, 25, 1, 'ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.6034518865308345\n",
      "(1, 100, 75, 1, 'leaky_ReLU', 0.0001, 1000, 0.1, True, 0.001)  scored  0.6674391014795582\n",
      "(1, 75, 75, 1, 'ReLU', 1e-05, 1000, 0.075, True, 0.001)  scored  0.6427483492524305\n",
      "(1, 50, 50, 1, 'leaky_ReLU', 1e-05, 1000, 0.05, True, 0.001)  scored  0.6528727987477778\n",
      "(1, 25, 10, 1, 'leaky_ReLU', 0, 1000, 0.05, True, 0.001)  scored  0.5792411838732837\n",
      "(1, 100, 100, 1, 'leaky_ReLU', 1e-05, 1000, 0.05, True, 0.001)  scored  0.6110749203381269\n",
      "(1, 75, 50, 1, 'leaky_ReLU', 0.001, 1000, 0.075, True, 0.001)  scored  0.5678242013361091\n",
      "(1, 75, 75, 1, 'leaky_ReLU', 1e-05, 1000, 0.05, True, 0.001)  scored  0.6223574310801716\n",
      "(1, 75, 50, 1, 'ReLU', 0.001, 1000, 0.05, True, 0.001)  scored  0.6478941397041197\n",
      "(1, 100, 100, 1, 'ReLU', 0, 1000, 0.075, True, 0.001)  scored  0.6294378518713014\n",
      "\n",
      "The best Network is the one defined by the params:\n",
      " (1, 75, 50, 1, 'leaky_ReLU', 0.001, 1000, 0.075, True, 0.001)\n"
     ]
    }
   ],
   "source": [
    "# For each network, re-initialize and re-train it with the whole train dataset\n",
    "for i in range(take_best):\n",
    "    \n",
    "    # Consider tuple parameters corresponding to the wanted index\n",
    "    good_params = params_combinations[sorted_comb_indexes[i]]\n",
    "    \n",
    "    # Initialize network with params (Ni, Nh1, Nh2, No, act, L1_lambda)\n",
    "    net = Network(*good_params[:6])\n",
    "\n",
    "    # Train the network by calling its method\n",
    "    train_loss_log, test_loss_log = net.train(x_train, y_train, x_test, y_test, \n",
    "                                              1000, #train each one for more epochs\n",
    "                                              *good_params[-3:],\n",
    "                                              return_log = True)\n",
    "    \n",
    "    print(good_params, \" scored \", test_loss_log[-1])\n",
    "    \n",
    "    # If we achieve the best score, save everything\n",
    "    if (i==0) or (test_loss_log[-1] < best_score):\n",
    "        best_score = test_loss_log[-1]\n",
    "        best_params = good_params\n",
    "        winning_net = net\n",
    "        winning_train_log = train_loss_log\n",
    "        winning_test_log = test_loss_log\n",
    "        \n",
    "# Save the winning net (and its weights) to a file    \n",
    "import dill\n",
    "with open('winning_net.dat', 'wb') as output:  # Overwrites any existing file. \n",
    "    dill.dump(winning_net, output)\n",
    "    \n",
    "print('\\nThe best Network is the one defined by the params:\\n', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot losses - DEBUG\n",
    "plt.close('all')\n",
    "plt.semilogy(winning_train_log, label='Train loss')\n",
    "plt.semilogy(winning_test_log, label='Test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that the best network has been stored, try reloading it from the file and using its `forward` method to plot the best estimation of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the best trained network\n",
    "import dill\n",
    "with open('winning_net.dat', 'rb') as input: \n",
    "    loaded_net = dill.load(input) \n",
    "\n",
    "# Plot estimated function\n",
    "x_highres = np.linspace(-4,2,6000)\n",
    "net_output = np.array([loaded_net.forward(x) for x in x_highres])\n",
    "\n",
    "plt.close('all')\n",
    "plt.plot(x_train, y_train, color='r', ls='', marker='.', label='Train data points')\n",
    "plt.plot(x_test, y_test, color='b', ls='', marker='.', label='Test data points')\n",
    "plt.plot(x_highres, net_output, color='g', ls='--', label='Network output (trained weights)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
